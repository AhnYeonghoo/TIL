# 신경망의 학습

- 학습
  - train data로부터 가중치 매개변수의 최적값을 자동으로 흭득하는 것
- 지표
  - 신경망이 학습할 수 있도록 하는 지표로 손실 함수를 사용한다.
  - 이 손실 함수의 결괏값을 가장 적게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다.
- 퍼셉트론 수렴정리
  - 퍼셉트론도 선형 분리가 가능한 문제라면, 데이터로부터 자동으로 학습할 수 있다. 이 사실은 퍼셉트론 수렴정리로 증명되어 있다. 하지만 비선형 분리 문제는 자동으로 학습할 수 없다.

<hr />
<hr />

# 기계학습 패러다임의 전환

- 사람이 직접 규칙을 만드는 것에서, 기계가 데이터로부터 배우는 방식으로의 패러다임의 전환

  - <img src="./images/fig 4-2.png">
    - 딥러닝은 데이터를 있는 그대로 학습하다. 
    - 2버째 접근 방식에서는 특징은 사람이 설계했지만, 신경망은 데이터에 포함된 중요한 특징까지도 기계가 스스로 학습한다.

    - 이러한 딥러닝을 종단간 기계학습이라고도 한다. '처음부터 끝까지'라는 의미로, 데이터의 입력에서부터 목표한 출력까지를 얻는다는 뜻이다.

- 훈련 데이터와 시험 데이터
  - 기계학습 문제는 훈련 데이터와 시험 데이터로 나눠 학습과 실험을 수행한다.
  - 이렇게 나누는 이유는 범용적인 모델을 설계하기 위해서다.
    - 여기서 말하는 범용 능력이란 예를 들어, 손글씨 숫자 인식 문제를 학습할 때, 특정인이 쓴 손글자만을 잘 인식하는 것이 아닌 임의의 사람의 임의의 글자인 것을 잘 판별해서 글씨를 인식해야 범용적이다. 라고 할 수 있다.
  - 한 데이터셋에만 지나치게 최적화된 상태를 오버피팅이라고 한다.

<hr />
<hr />

# 손실함수(Loss Function)

- 신경망 학습에서는 현재의 상태를 하나의 지표로 표현하며, 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것이 최종 목적이다.
- 그 지표로써 손실 함수를 사용한다.

<hr />
<hr />

# 손실 함수에서 쓰이는 함수

- 평균제곱오차(Mean Squared Error(MSE))

  - 평균제곱오차는 손실 함수에서 가장 많이 쓰인다.
  - 수식은 다음과 같다
    - $$E=\frac{1}{2}\sum_k(y_k-t_k)^2$$
    - $$y_k는 신경망의 출력, t_k는 정답레이블$$
    - k는 데이터의 차원 수를 나타낸다.

  ```python
  y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

  t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
  ```

  - 여기서 신경망의 출력 y는 softmax 함수의 출력이다.
  - softmax 함수의 출력은 확률로 해석할 수 있으므로, 위 예에서 예측한 결과는 '2'일 확률이 0.6이라는 뜻이다.
  - 정답 레이블도 2번째 인덱스에 1이 존재하므로 정답도 1인 것을 알 수 있다.
  - 이처럼 한 원소만 1로하고, 나머지는 0으로 하는 표기법을 '원-핫-인코딩'이라고 한다.

  - 평균제곱오차를 코드로 구현하면 다음과 같다.

  ```python
  def mean_squared_error(y, t):
      return 0.5 * np.sum((y-t)**2)
  ```

- 교차 엔트로피 오차(cross entropy error(CEE))

  - 손실 함수로 교차 엔트로피 오차도 자주 이용된다.
  - 수식은 다음과 같다.

    - $$E=-\sum_k t_k\log_ey_k$$
    - $$y_k는 신경망의 출력$$
    - $$t_k는 정답 레이블이다$$
    - 또한 $$t_k$$는 원-핫 인코딩 방식이다.
    - 코드로 구현하면 다음과 같다.

    ```python
    def cross_entropy_error(y, t):
        delta = 1e-7
        return -np.sum(t * np.log(y + delta))
    ```

    - delta라는 아주 작은 값을 더하는 이유는 log 함수에 0을 넣으면 -inf 값이 나오기 때문에 이를 방지하기 위함이다.

<hr />
<hr />

# 미니배치 학습

기계 학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구한 후, 그 값을 최대로 줄여주는 매개변수를 찾는 것이 목적이다. 즉, 만약 훈련 데이터가 100개가 존재하면, 계산한 100개의 손실 함수 값들의 합을 알아야 한다. 이를 수식을 표현하면 다음과 같다.

- $$E=-\frac{1}{N}\sum_n\sum_kt_{nk}\log y_{nk}$$

  - 데이터가 N개라면, t_nk는 n번째 데이터의 k차원 째의 값을 의미한다.
  - 앞서 말했듯 y는 신경망의 출력, t는 정답 레이블이다.
  - 위 수식은 데이터 하나에 대한 손실 함수 값을 N개의 데이터를 확장하고, 마지막에 N으로 나누어 정규화하는 과정을 거친다.
  - 이렇게 정규화함으로써 평균 손실 함수를 구할 수 있다.

- 하지만 훈련데이터가 방대해지면 그 모든 데이터를 대상으로 손실 함수의 값을 구하는 것은 시간이 오래걸린다.
- 더 나아가 빅데이터 수준이 된다면, 그 수는 수백만, 수천만이 넘는 큰 값이 되기도 한다.
- 따라서 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행하는데, 이 일부를 '미니배치(mini-batch)'라고 한다.
- 예를 들어 60,000개 데이터 중 100장을 무작위로 선별하여 100개의 데이터를 학습하는 것이다. 이러한 학습 방법을 미니배치 학습이라고 한다.

<hr />
<hr />

# 손실 함수를 설정하는 이유- 미분

- 손실함수를 설정하는 이유는 높은 정확도를 끌어내는 매개변수 값을 찾는 것이다.
- 신경망 학습에서 최적의 매개변수를 탐색할 때, 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾게 되는데 이때 매개변수의 미분(기울기)을 계산하고, 그 미분 값을 서서히 갱신하는 과정을 반복함으로써 최적의 매개변수 값을 찾게된다.
- 손실함수에서 미분에 초점을 두는 이유
  - 가중치 매개변수의 손실 함수의 미분은 가중치 매개변수의 값을 아주 조금만 변화시켰을 때, 손실함수 값이 어떻게 변하냐에 관심을 갖는다.
  - 만약 미분값(기울기)이 음수면 그 가중치를 양의 방향으로 변화시켜, 값을 갱신할 수 있고, 양수라면 음의 방향으로 변화시켜, 손실 함수의 값을 갱신할 수 있다.
  - 여기서 주의해야 할 점은 정확도를 지표로 삼아서는 안된다는 것이다.
    - 정확도를 지표로 삼으면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.
    - 정확도를 지표로 삼게 된다면, 매개변수 값을 아주 조금 변화시켜도 미분값은 연속적으로 변하기 보다는, 띄엄띄엄 변화한다.
    - 이는 활성화 함수로 계단 함수를 사용하지 않는 이유와 일맥상통한다.
    - 매개변수의 갱신에서 손실 함수를 지표로 삼는 이유는 아주 약간의 매개변수의 갱신에서 얻는 손실 함수의 미세한 연속적인 변화 때문이다.
    - 즉, 정확도를 지표로 삼으면 계단 함수와 같은 특성을 띄게 되고, 대부분의 장소에서 기울기가 0이 된다.
    - 반면에 손실함수를 지표로 삼게 되면, 시그모이드 함수처럼 연속적으로 변화한다. 또한 어느 장소에서도 0이 되지는 않는다.
    - 기울기가 0이 되지 않는 덕분에 신경망 학습이 올바르게 진행될 수 있다.

<hr />
<hr />

# 수치 미분

경사법에서는 기울기 값을 기준으로 나아갈 방향을 정한다(갱신)

- 일반적인 미분은 어느 한 시점에서의 변화률을 뜻하고, 수식은 다음과 같다.

  - $$\frac{df(x)}{dx}=\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}$$
  - <img src="./images/fig 4-5.png">

    - 수치 미분에는 오차가 포함되며, 이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수 f의 차분을 계산하는 방법을 사용하기도 한다.
    - x를 중심으로 그 전후의 차분을 계산한다는 의미에서
      **중심차분** 또는 **중앙차분**이라고도 한다.
    - **코드로 구현한 수치 미분**

    ```python
    def numerical_diff(f, x):
        h = 1e-4
        return (f(x+h) - f(x-h)) / (2*h)
    ```

        - 이 처럼 아주 작은 차분으로 미분을 구하는 것을 수치 미분이라고 하는데, 수식을 전개해 미분을 구하는 것을
        해석적 미분이라고도 한다. 이는 오차를 포함하지 않는 진정한 미분 값을 구하는 행위이다.

- 기울기(gradient)
  $$(\frac{\alpha f}{\alpha x_0}, \frac{\alpha f}{\alpha x_1})$$
  와 같이 모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 한다.
  코드로 구현하면 다음과 같다.

  ```python
  def numerical_gradient(f, x):
      h = 1e-4
      grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성한다.

      for idx in range(x.size):
          temp_val = x[idx]
          x[idx] = temp_val + h
          fxh1 = f(x)

          x[idx] = temp_val - h
          fxh2 = f(x)

          grad[idx] = (fxh1 - fxh2) / (2*h)
          x[idx] = temp_val
       return grad
  ```

  - 기울기는 방향을 가진 벡터로 그려질 수 있다.
    - <img src="./images/fig 4-9.png">
    - 각 화살표들은 한 점을 향해있고, 가장 낮은 값에서 멀어질수록 화살표의 크기가 커짐을 알 수 있다.
    - 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 줄일 수 있는 방향을 뜻한다.

<hr />
<hr />

# 경사 하강법

기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾는 것이 목적이다. 최적의 매개변수는 손실 함수가 최솟값을 가질 때를 뜻하는데, 일반적인 문제의 손실 함수는 매우 복잡하다. 따라서 기울기를 잘 이용하여, 함수의 최솟값을 찾으려는 방법이 경사하강법이다.

- 안장점

  - 함수가 극솟값이 되는 장소에서는 기울기가 0이다.
  - 극솟값은 국소적인 최솟값, 즉 한정된 범위에서의 최솟값이다.
  - 따라서 어느 방향에서는 극댓값이 되기도 하고, 극솟값이 되기도 한다.
  - 또한 경사법은 기울기가 0인 지점을 찾지만, 그것이 반드시 최솟값이라고 보장할 수 없다.
  - 또, 복잡하고 찌그러운 모양의 함수라면 평평한 곳으로 파고들면서 고원이라고 하는 학습이 진행되지 않는 정체기에 빠질 수 있다.

- 경사법의 도입

  - 이러한 문제들 때문에 경사법이 등장했다.

  경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서 또 기울기를 구하고, 기울어진 방향으로 나아가는 일을 반복한다.
  이렇게 함수의 값을 점차 줄여나가는 것을 **경사법(gradient method)이라고 한다**

  - 경사법은 최댓값을 찾냐, 최솟값을 찾냐에 따라 경사 상승법, 경사 하강법이라고 하지만, 손실함수의 부호를 반전시키면 최솟값을 찾는 문제나 최댓값을 찾는 문제나 본질적으로는 동일한 문제이기 때문에, 보편적으로는 경사 하강법으로 불린다.

- 경사법을 수식으로 나타내면 다음과 같다.
  - $$x_0=x_0-\eta\frac{\alpha f}{\alpha x_0}$$
  - $$x_1=x1-\eta\frac{\alpha f}{\alpha x_1}$$
    - 이 수식은 갱신하는 양을 나타내는데 이를 신경망 학습에서는 **학습률(learning rate)** 이라고 한다.
    - 한 번의 학습으로 얼마만큼 학습해야 할 지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다.
    - 학습률 값은 미리 특정 값으로 정해두어야 하는데, 이 값이 너무 크거나 작으면 최적의 값으로 학습할 수 없다.
- 코드로 구현한 경사법

  ```python
  def gradient_descent(f, init_x, lr=0.01, step_num=100):
      x = init_x

      for i in range(step_num):
          grad = numerical_gradient(f, x)
          x -= lr * grad
      return x
  ```

  - lr은 학습률, init_x는 초깃값, f는 최적화 타겟 함수, step_num은 반복 횟수이다.
  - 이 과정에서 학습률을 너무 크게 설정하면, 큰 값으로 발산해버리고, 반대로 너무 작게 설정하면, 매개변수의 갱신이 거의 이루어지지 않고 학습이 끝나버리므로, 적절한 값을 설정하는 것이 중요하다.

- 하이퍼파라미터(hyper parameter)
  - 초매개변수라고 하는데, 학습률과 같은 매개변수를 의미한다.
  - 가중치와 편향과 같은 신경망 매개변수와는 다른 매개변수인데, 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해 **자동** 으로 흭득되는 매개변수이다.
  - 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수이다.
- 신경망에서의 기울기
  - $$W=\begin{pmatrix}w_{11} & w_{21} & w_{31} \\ w_{12} & w_{22} & w_{32} \end{pmatrix}$$
  - $$\frac{\alpha L}{\alpha W}=\begin{pmatrix}\frac{\alpha L}{\alpha_{w_{11}}} & \frac{\alpha L}{\alpha_{w_{21}}} & \frac{\alpha L}{\alpha_{w_{31}}} \\ \frac{\alpha L}{\alpha_{w_{12}}} & \frac{\alpha L}{\alpha_{w_{22}}} & \frac{\alpha L}{\alpha_{w_{32}}}\end{pmatrix}$$
  - $$\frac{\alpha L}{\alpha W}$$
    의 각 원소는 각각의 원소에 관한 편미분이다.
  - 형상은 2x3, 가중치는 W, 손실 함수는 L을 뜻한다.
    - 예를 들어 1행 1번째 원소인 $$\frac{\alpha L}{\alpha_{w_{11}}}$$은 $$w_{11}$$을 조금 변경했을 때 손실 함수 L이 얼마나 변화느냐를 나타낸다.

<hr />
<hr />
<hr />

# 최종 학습 알고리즘 정리

- 전체

  - 신경망에는 적응 가능한 가중치 및 편항이 있고, 이 가중치와 편향을 훈련 데이터에 적응할 수 있도록 조정하는 과정을 학습이라고 한다.
  - 1단계: 미니배치
    - 훈련 데이터 중 일부를 무작위로 가져온다. 이 데이터를 미니배치라고 하며, 미니배치의 손실 함수 값을 줄이는 것이 목적이다.
  - 2단계: 기울기 산출
    - 미니배치의 손실 함수 값을 줄이기 위해 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게하는 방향을 제시한다.
  - 3단계: 매개변수의 갱신
    - 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다(경사법)
  - 4단계: 반복
    - 1부터 3단계까지 반복한다.

- 이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 이때 데이터는 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(stochastic gradient descent(SGD))** 라고도 부른다.

```python
# SGD를 pytorch로 구현하기
# 필요한 모듈 import 해오기
import torch
import torch.nn as nn # 뉴럴 네트웤
import torch.nn.functional as F
import torch.optim as optim
# 훈련 데이터 셋팅
x_train = torch.FloatTensor([[1,],[2],[3]])
y_train = torch.FloatTensor([[2],[4],[6]])
# 가중치와 편향의 초기화
W = torch.zeros(1, requires_grad=True)
# requires_grad=True는 학습을 통해 계속 값이 변경되는 변수임을 뜻한다.
b = torch.zeros(1, requires_grad=True)
# 가설 세우기
hypothesis = x_train * W + b
# 비용 함수 선언하기
cost = torch.mean((hypotehsis - y_train) ** 2)
# 비용함수로 평균 제곱오차 사용

# 경사하강법 구현
optimizer = optim.SGD([w,b], lr=0.01)
optimizer.zero_grad() # 미분을 통해 얻은 기울기를 0으로 초기화
cost.backward() # 기울기의 계산
optimizer.step() # 기울기에 학습률을 곱하여 빼줌으로써 매개변수의 갱신

epochs = 1999 # 전체 훈련 데이터가 학습에 한 번 사용된 주기
for epoch in range(epochs+1):
    hypothesis = x_train * W + b

```

- 에폭(epoch)
  에폭은 하나의 단위로, 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 예를 들어, 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, SGD를 100회 반복하면 모든 훈련 데이터를 소진한 것이 된다. 이 경우 100회가 1epoch이다.
