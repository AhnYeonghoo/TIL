# RNN(Recurrent Neural Network)

- word2vec와 같은 신경망들은 feed forward 유형의 신경망인데 이는 단뱡항 신경망이다.
- layer들의 신호가 한 방향으로만 전달된다는 뜻이다.
- 따라서 이러한 feed forward 신경망은 시계열 데이터의 패턴을 학습할 수 없다.
- 그래서 등장한 신경망이 바로 RNN이다.

### 언어 모델(Language Model)

- 언어 모델은 특정한 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느정도 인지를 확률로써 평가한다. 즉, 단어 나열에 확률을 부여한다.
- 언어 모델을 수식으로 알아보자.

  - $$w_1,\dots,w_m$$
  - 이라는 m개 단어로 된 문장을 생각해보자.
  - 이때 단어가 $$w_1,\dots,w_m$$ 이라는 순서로 출현할 확률은 $$P(w_1,\dots,w_m)$$로 표현할 수 있다.
  - 이 확률은 여러 사건이 동시에 일어날 확률이어서 동시 확률이라고 한다.
  - 해당 동시 확률을 사후 확률을 사용하여 전개하면 다음과 같이 분해할 수 있다.

    - $$P(w_1,\dots,w_m)=P(w_m|w_1,\dots,w_{m-1})P(w_3|w_1,w_2)P(w_2|w_1)P(w_1)=\prod_{t=1}^mP(w_t|w_1,\dots,w_{t-1})$$
    - 식에서 알 수 있듯이, **동시 확률은 사후 확률의 총곱으로 나타낼 수 있다**

    - 해당 수식은 확률의 곱셈정리로부터 유도할 수 있다.
    - $$P(A, B)=P(A|B)P(B)$$
    - 혹은
    - $$P(A, B)=P(B|A)P(A)$$

- 해당 곱셉정리를 이용하면 m개 단어의 동시 확률 P를 사후 확률로써 나타낼 수 있다.

  - $$P(w_1,\dots,w_{m-1},w_m)=P(A,w_m)=P(w_m|A)P(A)$$
  - 여기서 $$w_1,\dots,w_{m-1}$$을 하나로 모아 $$A^\prime$$으로 나타내면 다음과 같이 전개가 가능하다.
  - $$P(A)=P(w_1,\dots,w_{m-2},w_{m-1})=P(A^\prime,w_{m-1})=P(w_{m-1}|A^\prime)P(A)$$
  - 이처럼 단어 시퀀스를 하나씩 줄여가며, 매번 사후 확률로 분해해 갈 수 있다.

- 이 사후 확률은 target 단어보다 왼쪽에 있는 모든 단어를 context로 했을 때의 확률이다.

- CBOW 모델을 context의 크기를 특정 값을 한정하면 언어 모델에 근사적으로 적용할 수 있다.
  - $$P(w_1,\dots,w_m)=\prod_{t=1}^mP(w_t|w_1,\dots,w_{t-1}\approx\prod_{t=1}^mP(w_t|w_{t-2},w_{t-1})$$
  - 이 수식에서는 context를 왼쪽 2개의 단어로 한정한다.
- 마르코프 연쇄

  - 위 수식에서는 **2층 마르코프 연쇄** 라고 할 수 있다.
  - 마르코프 연쇄는 미래의 상태라 현재 상태에만 의존해 결정되는 것을 말한다.
  - 이 사상의 확률이 그 직전 N개의 사건에만 의존할 때, 이를 N개 마르코프 연쇄 혹은 모델이라고 한다.
  - 따라서 위 수식은 2개의 단어에만 의존해 다음 단어가 정해지는 수식(모델)이므로 2층 마르코프 연쇄라고 할 수 있다.

- 그렇다면, 왜 RNN이 고안되었을까
  - CBOW 모델에서는 context의 크기를 임의로 설정할 수 있다.
  - 하지만 결국 특정 길이로 고정되는 현상이 일어나는데, 이는 아주 긴 context가 필요한 문제에서 문제가 발생한다.
  - <img src="../images/fig 5-4.png">
  - 위 그림에서 context 크기가 10개였다고 가정하면, "TOM이 방에서 tv를 보고 있고, mary가 그 방에 들어왔다" 라고 한다.
  - 그럼 context를 고려하면, "인사를 건넸다"가 정답이 되어야 한다. 하지만 context 크기가 10개이므로, 앞에 나오는 "TOM"을 기억하지 못한다.
- 그럼 context 크기를 무한정 키우면 되지 않나?
  - context 크기는 얼마든지 키울 수 있지만, CBOW 모델에선 context 내 단어 순서가 무시된다는 한계점이 존재한다.
  - CBOW는 Continuous bag-of-words의 약어이듯이, 벡터로 표현된 단어들을 순서로 표현하지 않고, 분포를 이용한다.
  - 그래서 context 크기가 아무리 크더라도 앞선 context 정보를 기억하는 순환신경망, RNN이 필요하다.

<hr />
<hr />
<hr />

## RNN

- Recurrent_Neural_Network의 약자이다. 몇 번이나 순환해서 일어나는 신경망?으로 해석된다.
- RNN의 특징으로는 닫힌 경로가 있다는 것이다. 마치 닫힌 혈관 속에서 무한히 순환하는 혈관처럼 닫힌 경로 속에서 순환하는 경로가 존재한다.
- 이 덕분에 과거의 정보를 기억하는 동시에 최신 데이터도 갱신할 수 있는 것이다.

- RNN 게층의 순환 구조는 다음과 같다.

  - <img src="../images/fig 5-8.png">
  - $$x_t$$
  - 를 입력받는데, t는 시각을 뜻한다.
  - $$(x_0,x_1,\dots,x_t)$$
  - 가 입력됨에 따라 $$(h_0,h_1,\dots,h_t)$$를 출력한다.
  - 얼핏보면 기존의 feed_forward 신경망 구조와 다를 바 없지만, RNN은 다수의 layer들이 실제로는 동일한 layer인 것이 다르다.

- 시계열 데이터는 시간 방향으로 데이터가 나열된다.

  - 시계열 데이터의 인덱스를 가리킬 때는 '시각'이라는 용어를 사용한다.
  - 자연어에서도 **t번재 단어**나, **t번째 RNN 계층**이라는 표현도 사용하고, **시각 t의 단어**나 **시각 t의 RNN 계층**이라고도 표현하기도 한다.

- 각 시각의 RNN 계층은 그 계층으로의 입력과 1개의 **이전 RNN 계층**으로부터의 출력을 받는다.
- 그리고 이 두 정보를 바탕으로 현 시각의 출력을 계산한다.
- 수식으로 표현하면 다음과 같다.
  - $$h_t=\tanh(h_{t-1}W_h+x_tW_x+b)$$
  - 두 개의 weight가 존재하는데, 하나는 입력 x를 출력 h로 변환하기 위한 가중치 $$W_x$$이고, 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 $$W_h$$이다. 또한 bias도 있다.
  - $$h_{t-1}$$
  - 과 $$x_t$$는 행벡터이다.
  - 이 식에서는 행렬의 내적을 계산하고, 그 합을 tanh 함수를 이용하여 변환한다.
  - 그 결과는 시각 t의 출력 $$h_t$$가 된다. 이 $$h_t$$는 **다른 계층을 향해 위쪽으로 출력됨**과 동시에, **다음 시각의 RNN 계층을 향해 오른쪽으로도 출력된다(자기 자신)**
  - 식에서 알 수 있듯, $$h_t$$는 $$h_{t-1}$$에 기초해 계산된다. 즉, h는 state를 가지고 있으며, 해당 식에 의해 갱신된다고 할 수 있다.
  - 그래서 RNN 계층을 **메모리가 있는 계층**이라고도 한다.
  - 보통 많은 문헌에서는 $$h_t$$를 hidden state 혹은 hidden state vector라고 한다고 한다.

<hr />
<hr />
<hr />

## BPTT

- RNN에도 오차역전파법을 통해 학습할 수 있다.
- <img src="../images/fig 5-10.png">

  - RNN 계층의 순환 구조를 펼친 후에는 오차역전파법을 적용할 수 있다.
  - 먼저 forward를 수행 후, 이어서 backward를 수행하여 원하는 gradient를 구할 수 있다.
  - RNN에서의 오차역전파법은 **시간 방향으로 펼친 신경망의 오차역전파법**이라는 의미로 **Backpropagation Through Time(BPTT)** 이라고 한다.

- 하지만 BPTT는 매 시각의 RNN 계층의 중간 데이터를 모두 메모리에 유지해두어야 한다. 즉, 시계열 데이터가 길어질 수록 메모리에 부하가 생기고, backward시 gradient가 불안정해지는 것도 문제이다.

<hr />
<hr />
<hr />

## Truncated BPTT

- 큰 시계열 데이터를 다룰 때는 신경망 연결을 적당한 길이로 끊어, 작은 신경망 여러개로 나눈다.
- 이 작은 신경망 단위로 오차역전파법을 적용하는데 이를 **Truncated BPTT**라고 한다.

  - Truncated BPTT에선 신경망의 연결을 끊지만, 순전파의 연결은 보존하고, 역전파의 연결만 끊어야 한다.
  - 애초에 이 기법을 고안한 이유가 역전파 시 gradient의 불안정 문제와 메모리 점유 문제를 해결하기 위함이기 때문이다.

- 구체적인 예시

  - 예를 들어 길이가 1,000개인 시계열 데이터가 있다면, 이는 단어 1,000개 짜리 corpus에 해당한다.
  - 1,000개의 시계열 데이터를 RNN 계층으로 펼치면 계층이 RNN 계층 특성상 가로로 1,000개 늘어선 신경망이 된다.
  - 물론, 이 상태로 오차역전파법을 적용해, 기울기를 계산할 수 있다.
  - 하지만, 기울기를 계산하는 과정에서 미분을 하기 때문에 계층 하나를 통과할 때 마다 기울기 값이 조금씩 작아져, 이전 시각 t까지 역전파되기 전에 0이 되어버려 소멸할 수도 있다.(더이상 학습 불능)
  - 그래서, Truncated BPTT가 고안된 것이다.

  - <img src="../images/fig 5-11.png">

    - 여기선 RNN 계층을 길이 10개 단위로 학습할 수 있도록, **역전파**의 연결을 끊었다.
    - 여기서 중요한 점은 **순전파** 연결은 반드시 보존해야 한다는 것이다.
    - 또한, 기존 word2vec와 같은 신경망에서 **미니배치 학습** 을 수행할 때 데이터를 무작위로 선택해 입력에 넣었다.
    - 하지만, RNN에서 Truncated BPTT 기법을 사용할 때는 데이터를 **순서대로** 입력해야 한다.

- <img src="../images/fig 5-12.png">
- <img src="../images/fig 5-13.png">

  - 기존 신경망과 다를 바가 없다.
  - 하지만 두 번째 블록부터 순전파 계산에 이전 블록의 마지막 hidden state vector가 필요하다. (여기서는 $$h_9$$)
  - 이것으로 순전파는 보존될 수 있다.
  - 마찬가지로, 3번째 블록에서는 두 번째 블록의 마지막 hidden state vector를($$h_{19}$$)를 이용하여 RNN의 **순전파**는 계속 보존하며 **역전파**만 끊을 수 있게 된다.

  - <img src="../images/fig 5-14.png">

### Truncated BPTT의 미니배치 학습

- 시계열 데이터를 다루기 때문에 미니배치 단위 학습을 수행하기 위해서는 데이터를 순서대로 입력해주어야 한다.
- Truncated BPTT에서는 데이터를 주는 시작 위치를 각 미니배치의 시작 위치로 옮겨주어야 한다.
  - 예를 들어서 길이가 1,000개인 시계열 데이터에 대해서, 시각의 길이를 10개 단위로 잘라 Turncated BPTT로 학습하는 경우는 **RNN 계층의 입력 데이터로, 첫 번째 미니배치 때는 처음부터 순서대로 데이터를 제공한다.**
  - 두 번째 미니배치 때는 500번 째의 데이터를 시작 위치로 정하고, 그 위치부터 다시 순서대로 데이터를 제공한다. -<img src="../images/fig 5-15.png">

  - 첫 번째 미니배치의 원소는 $$x_0,\dots,w_9$$가 되고, 두 번째 미니배치 원소는 $$x_{500},\dots,x_{509}$$가 된다.

  - 이후로도 순서대로 시계열 데이터의 10~19번째 데이터와 510~~519번째의 데이터를 입력하면 된다.
  - 또한, 데이터를 순서대로 입력하다가 끝에 도달하면, 다시 처음부터 입력하도록 한다.
