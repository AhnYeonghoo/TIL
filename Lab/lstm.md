# (기본)RNN의 문제점

- 단순한 구조여서 시계열 데이터의 장기 의존 관계를 학습하기 어렵다.
- 그 원인으로는 BPTT에서 기울기 소실 혹은 폭팔이 일어나기 때문이다.

- 기본 RNN 계층의 계산 그래프

  - <img src="../images/fig 6-2.png">

<br >
<br >

## 기울기의 소실과 폭팔

- 기울기 소실은 역전파의 기울기 값이 점점 작아지다가 소멸되는 것을 뜻하고, 폭팔은 반대로 매우 큰 값이 된다는 것을 의미한다. 즉 학습이 더이상 불가능한 상태가 되는 것이다.

### 원인

- 예를 들어 다음과 같은 장기 기억이 필요한 문제를 보면

    <img src="../images/fig 6-3.png">

  - RNN이 이 문제에 정확한 답을 내릴려면 현재의 context에서 **Tom이 방에서 TV를 보고 있음** 과 **그 방에 Mary가 들어옴** 이라는 정보를 RNN 계층의 은닉 상태에 인코딩해 보관해두어야 한다.

  - 즉, RNN 계층이 과거 방향으로 학습에 의미가 있는 기울기를 전달해야만 시간 뱡향의 의존 관계를 학습할 수 있는 것이다.

  - 하지만 이 기울기가 중간에 사그라들면 가중치 매개변수는 전혀 갱신되지 않고, 학습할 수 없다.

<br>  
<br> 
<img src="../images/fig 6-5.png">

     그림처럼 길이가 T인 시계열 데이터를 가정하고, 앞의 문제에 대입하면, T번째 정답 레이블이 Tom인 경우에 해당한다. 이때 시간 뱡향 기울기에 주목하면 역전파로 전해지는 기울기는 차례대로 tanh, +, MatMul(행렬 곱) 연산을 통과하는 것을 알 수 있다.

- +의 역전파는 받은 기울기를 그냥 그대로 흘러보내기만 할 뿐이어서, 기울기의 변화는 없다.
- 다음으로 tanh 함수의 미분인데 y=tanh(x)일 때의 미분은 $$\frac{\partial z}{\partial x}=1-y^2$$ 이다.

- 이때의 y=tanh(x)의 값과 미분값을 그래프로 그리면 다음과 같다.

  - <img src="../images/fig 6-6.png">
      
      - 미분의 그래프는 x가 0으로부터 멀어질수록 작아진다.
      - 즉, 역전파 때 기울기가 tanh 노드를 지날 때마다 값이 계속 작아진다는 의미이다.
      - 따라서 tanh 함수를 T번 통과하면 기울기도 T번 반복하여 작아진다.

<br >

- **Improving performance of recurrent neural network with relu nonlinearity** 라는 논문에서는 RNN 계층의 활성화 함수를 tanh 함수 대신에 ReLU 함수를 사용해 기울기의 소실을 줄여 성능을 개선했다고 한다.

- ReLU 함수는 함수 특성상 입력 x가 0 이상이면 이전 노드의 기울기를 그대로 흘려보내기 때문에 기울기가 줄어들지는 않는다.

<br >
<br >

- 마지막으로 MatMul 노드에서는 기울기가 어떻게 변할까?

```python
import numpy as np
import matplotlib.pyplot as plt

N = 2 # 미니배치의 크기
H = 3 # 은닉 상태 벡터의 차원 수
T = 20  # 시계열 데이터 길이

dh = np.ones((N, H)) # 모든 원소가 1인 행렬 반환
np.random.seed(3) # 난수의 시드를 고정
Wh = np.random.randn(H, H)

norm_list = []  # L2 노름이란 각 각의 원소를 제곱해 모두 더하고 제곱근을 취한 값이다.
for t in range(T):
    dh = np.matmul(dh, Wh.T)
    norm = np.sqrt(np.sum(dh**2)) / N
    norm_list.append(norm)
```

해당 코드로 matmul 노드를 구현해 그래프를 그려보면 다음과 같이 그래프가 형성된다.

<br >

- <img src="../images/fig 6-8.png">

  - 이처럼, 기울기가 시간에 따라 지수적으로 증가한다.
  - 결국 기울기 폭팔로 이어지고, 오버플로가 발생해 NaN같은 값을 발생시켜, 학습을 이어나갈 수 없다.

```python
import numpy as np
import matplotlib.pyplot as plt

N = 2 # 미니배치의 크기
H = 3 # 은닉 상태 벡터의 차원 수
T = 20  # 시계열 데이터 길이

dh = np.ones((N, H)) # 모든 원소가 1인 행렬 반환
np.random.seed(3) # 난수의 시드를 고정
Wh = np.random.randn(H, H) * 0.5 # 가중치 초깃값 변경

norm_list = []  # L2 노름이란 각 각의 원소를 제곱해 모두 더하고 제곱근을 취한 값이다.
for t in range(T):
    dh = np.matmul(dh, Wh.T)
    norm = np.sqrt(np.sum(dh**2)) / N
    norm_list.append(norm)
```

- 가중치의 초깃값을 변경하면 그래프는 다음과 같이 형성된다.

  - <img src="../images/fig 6-9.png">

    - 시간에 따라 기울기가 지수적으로 감소한다.
    - 결국 기울기 소실이 일어나 가중치 매개변수를 갱신할 수 없고, 학습을 이어나갈 수 없게 된다.

## 기울기 폭팔 혹은 소실 대책

<br >

- 기울기 폭팔의 전통적인 대책으로 기울기 클리핑(gradients clipping)이라는 기법이 있다.

- 이 알고리즘을 의사 코드로 쓰면 다음과 같다.

  - $$if ||\hat{g}||\geq threshould:$$
  - $$\hat{g}=\frac{threshould}{||\hat{g}||}\hat{g}$$

  - threshould는 문턱값이고, 기울기의 L2 norm이 문턱값을 초과하면 두 번째 줄의 수식으로 기울기를 수정한다.

  - $$\hat{g}$$
  - 은 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것이다.

<br >

코드로 구현하면 다음과 같다.

```python
def clip_grads(grads, max_norm):
    total_norm = 0
    for grad in grads:
        total_norm += np.sum(grad ** 2)
    total_norm = np.sqrt(total_norm)

    rate = max_norm / (total_norm + 1e-6)
    if rate < 1:
        for grad in grads:
            grad *= rate
```

## 고찰

RNN이 그냥 RNN인줄 알고 있었는데 이런 문제점이 있는지 잘 몰랐고, LSTM과 GRU가 이러한 RNN의 문제점을 해결하고자 나왔다는 배경을 알게 되었다.

<br>
<br>

# LSTM(게이트가 추가된 RNN)

<br>

기존 RNN은 기울기 소실이 큰 문제이다. 그래서 고안된 두 개의 게이트가 추가된 RNN이 있는데 하나는 LSTM이고, 하나는 GRU이다.

- LSTM과 RNN 계층 비교
  - <img src="../images/fig 6-11.png">
  - LSTM에는 c라는 경로가 하나 추가되었는데, 이 c를 기억 셀(memory cell)이라고 하며, LSTM의 기억 메커니즘이다.
  - 기억 셀의 특징은 데이터를 LSTM 계층 내에서만 주고 받는 것과, 다른 계층으로는 출력하지 않는다는 것이다.
  - 반면 은닉상태 h는 RNN 계층과 같이 다른 계층으로 출력한다.

<br>

- LSTM 계층구조
  - LSTM에는 c라는 기억 셀이 존재한다.
  - 이 c에는 시각 t에서의 LSTM의 기억이 저장되어 있다.
  - 그리고 필요한 정보를 모두 가지고 있는 기억을 바탕으로 다음 계층에 은닉 상태 h를 출력한다.
    - 이때 출력하는 h에는 기억 셀의 값을 행렬 곱, 편향의 합, thanh 함수에 의해 변환을 거친 값이다.
  - 여기서 핵심은 갱신된 c를 이용하여 은닉 상태 h를 계산한다는 것이다.

<br>

- 게이트의 기능
  - 게이트는 데이터의 흐름을 제어한다.
  - LSTM에서 사용되는 게이트는 단순히 열기/닫기 뿐 아니라 열림 상태를 조절할 수 있다.
  - 여기서 중요한 것은 게이트의 열림 상태 또한 데이터로부터 자동으로 학습한다는 점이다.
  - 게이트의 열림 상태는 전용 가중치 매개변수를 이용하며, 값을 계산할 때는 시그모이드 함수를 이용한다.
    - 시그모이드 함수
      - $$\frac{1}{1+\exp^x}$$
      - 값이 0.0~1.0의 실수인 것을 이용하는 것이다.

<br>

- output 게이트

  - 은닉 상태 h는 기억 셀 c의 각 원소에 대해 행렬곱, 편향의 합, tanh 함수를 거친 후 나온 값이다.
  - 이는 다음 시각의 은닉 상태에 얼마나 중요한가를 갱신하는 일이다.
  - 따라서 이 게이트를 은닉 상태 h의 출력을 담당하는 게이트라 하여 output 게이트라고 한다.
  - output 게이트의 열림 상태는 입력 x와 이전 상태 $$h_{t-1}$$로부터 구한다.
  - 이 때의 계산은 다음과 같다.
    - $$o=\sigma{x_tW_x^{(o)}+h_{t-1}W_h^{(o)}+b^{(o)}}$$
      - o는 output의 첫 글자를 의미하며, sigma는 시그모이드 함수를 의미한다.
      - 입력 $$x_t$$에는 가중치 $$W_x^{(o)}$$가, 이전 시각의 은닉 상태 $$h_{t-1}$$에는 가중치 $$W_h^{(o)}$$가 붙는다.
      - 그리고 이 행렬들의 곱과 편향 $$b^{(o)}$$를 모둔 더한 다음 시그모이드 함수를 거쳐 출력 o를 구한다.

- output 게이트 구조
  - <img src="../images/fig 6-15.png">
  - output 게이트에서 수행하는 계산을 $$\sigma$$라고 표현하면 $$\sigma$$의 출력을 o라고 하면 $$h_t$$는 o와 $$\tanh{(c_t)}$$의 곱으로 계산된다.
  - 여기서 곱은 원소별 곱을 말하며, 아다마르 곱(Hadamard product)라고도 한다.
  - 아다마르 곱은 기호로 $$\odot$$로 표기한다.
  - 식은 다음과 같다.
    - $$h_t=o\odot\tanh{(c_t)}$$
- tanh 함수의 출력은 -1.0~1.0 사이의 실수이며, 이 수치를 그 안에 인코딩된 정보의 강약(정도)을 표시한다고 할 수 있다.
- 한편 시그모이드 함수의 출력은 0.0~1.0의 실수이며, 데이터를 얼마만큼 통과시킬지를 정하는 비율이다.
- 따라서 게이트에서는 시그모이드 함수가, 실질적인 정보를 지니는 데이터에는 tanh 함수가 활성화 함수로 사용된다.

<br >

- forget 게이트

  - 기억 셀에 저장된 $$c_{t-1}$$의 기억 중 불필요한 기억을 지우는 게이트를 forget 게이트라고 한다.

  - 게이트의 구조
    - <img src="../images/fig 6-16.png">
    - 시그마 안에는 forget 게이트 전용의 가중치 매개변수가 존재하며, 식은 다음과 같다.
      - $$f=\sigma(x_tW_x^{(f)}+h_{t-1}W_h^{(f)}+b^{(f)})$$
      - 식을 실행하면 forget 게이트의 출력인 f가 구해지며, 이 f와 이전 기억 셀인 $$c_{t-1}$$과의 원소별 곱인 $$c_t=f\odot c_{t-1}$$을 계산하여 c를 구한다.

<br >

- 새로운 기억 셀

  - 새로 기억해야 할 정보를 기억 셀에 추가해야 하는데, 그러기 위해 tanh 노드를 추가한다.
  - tanh는 행렬곱, 편향의 합, tanh 함수의 출력을 모두 포함하는 노드이다.
  - <img src="../images/fig 6-17.png">

    - 그림과 같이 tanh 노드가 계산한 결과가 이전 시각의 기억 셀인 $$c_{t-1}$$에 더해진다.
    - 이 tanh 노드는 게이트가 아닌, 새로운 정보를 셀에 저장하는 게 목적이므로 활성화함수로 시그모이드 함수가 아닌, tanh 함수를 사용한다.
    - 이 tanh 노드에서 수행하는 계산식은 다음과 같다.
      - $$g=\tanh{(x_tW_x^{(g)}+h_{t-1}W_h^{(g)}+b^{(g)})}$$
      - g는 새로운 기억이다.
