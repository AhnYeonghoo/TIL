{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a75507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.735989814121131e-13\n",
      "b1:7.380542374089625e-13\n",
      "W2:9.570435819707124e-13\n",
      "b2:1.207922664669958e-10\n",
      "train acc, test acc | 0.11851666666666667, 0.1129\n",
      "train acc, test acc | 0.9030666666666667, 0.9084\n",
      "train acc, test acc | 0.9245333333333333, 0.9261\n",
      "train acc, test acc | 0.9347833333333333, 0.9324\n",
      "train acc, test acc | 0.9462, 0.9413\n",
      "train acc, test acc | 0.9520833333333333, 0.9492\n",
      "train acc, test acc | 0.95675, 0.9541\n",
      "train acc, test acc | 0.9613166666666667, 0.9592\n",
      "train acc, test acc | 0.9637333333333333, 0.9607\n",
      "train acc, test acc | 0.9682333333333333, 0.9629\n",
      "train acc, test acc | 0.9693166666666667, 0.9636\n",
      "train acc, test acc | 0.9706166666666667, 0.9639\n",
      "train acc, test acc | 0.9739666666666666, 0.9664\n",
      "train acc, test acc | 0.9737666666666667, 0.9651\n",
      "train acc, test acc | 0.9756333333333334, 0.9662\n",
      "train acc, test acc | 0.9777666666666667, 0.9666\n",
      "train acc, test acc | 0.97855, 0.9676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n계산 과정을 시각적으로 보여주는 방법인 계산 그래프에 대해 학습했다.\\n계산 그래프를 통해 신경망의 동작과 오차역전파법을 설명하고, 계층이라는 단위로 구현했다.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "# 5.7.1 신경망 학습의 전체 그림\n",
    "\"\"\"\n",
    "(4.5와 동일)\n",
    "전제\n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n",
    "신경망 학습은 다음과 같이 4단계로 수행한다.\n",
    "\n",
    "1단계 - 미니배치\n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n",
    "그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n",
    "\n",
    "2단계 - 기울기 산출\n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n",
    "기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "3단계 - 매개변수 갱신\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "4단계 - 반복\n",
    "1~3단계를 반복한다.\n",
    "\n",
    "수치 미분과 오차역전파법은 2단계에서 사용\n",
    "수치 미분은 구현은 쉽지만 계산이 오래걸림\n",
    "오차역전파법을 통해 기울기를 효율적이고 빠르게 구할 수 있음\n",
    "\"\"\"\n",
    "\n",
    "# 5.7.2 오차역전파법을 이용한 신경망 구현하기\n",
    "\"\"\"\n",
    "TwoLayerNet 클래스로 구현\n",
    " * 클래스의 인스턴스 변수\n",
    "params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n",
    "        params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n",
    "        params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n",
    "layers : 신경망의 계층을 보관하는 순서가 있는 딕셔너리 변수\n",
    "        layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이\n",
    "        각 계층을 순서대로 유지\n",
    "lastLayer : 신경망의 마지막 계층(여기서는 SoftmaxWithLoss)\n",
    "\n",
    " * 클래스의 메서드\n",
    "__init__(...) : 초기화 수행\n",
    "predict(x) : 예측(추론)을 수행한다. x는 이미지 데이터\n",
    "loss(x, t) : 손실함수의 값을 구한다. x는 이미지 데이터, t는 정답 레이블\n",
    "accuracy(x, t) : 정확도를 구한다.\n",
    "numerical_gradient(x, t) : 가중치 매개변수의 기울기를 수치 미분으로 구함(앞 장과 같음)\n",
    "gradient(x, t) : 가중치 매개변수의 기울기를 오차역전파법으로 구함\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "        weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼 파라메터\n",
    "iters_num = 10000  # 반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # print(i)\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 오차역전파법으로 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭 당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "\"\"\"\n",
    "train acc, test acc | 0.0992833333333, 0.1032\n",
    "train acc, test acc | 0.898, 0.9026\n",
    "train acc, test acc | 0.92135, 0.9216\n",
    "train acc, test acc | 0.936016666667, 0.9337\n",
    "train acc, test acc | 0.945316666667, 0.9431\n",
    "train acc, test acc | 0.94675, 0.9427\n",
    "train acc, test acc | 0.954766666667, 0.9521\n",
    "train acc, test acc | 0.9602, 0.9551\n",
    "train acc, test acc | 0.9634, 0.9581\n",
    "train acc, test acc | 0.9656, 0.9597\n",
    "train acc, test acc | 0.9683, 0.9615\n",
    "train acc, test acc | 0.970516666667, 0.9629\n",
    "train acc, test acc | 0.97305, 0.9649\n",
    "train acc, test acc | 0.9731, 0.9661\n",
    "train acc, test acc | 0.975916666667, 0.9659\n",
    "train acc, test acc | 0.976383333333, 0.9666\n",
    "train acc, test acc | 0.977916666667, 0.969\n",
    "[Finished in 45.5s]\n",
    "\"\"\"\n",
    "\n",
    "# 5.8 정리\n",
    "\"\"\"\n",
    "계산 과정을 시각적으로 보여주는 방법인 계산 그래프에 대해 학습했다.\n",
    "계산 그래프를 통해 신경망의 동작과 오차역전파법을 설명하고, 계층이라는 단위로 구현했다.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa368d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률적 경사 하강법(Stochastic Gradient Descent)\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr # learning rate(학습률)\n",
    "    \n",
    "    def update(self, params, grads): \n",
    "        for key in params.keys(): # 가중치 매개변수와 기울기를 저장 eg: params['W1'] or grads['W1']\n",
    "            params[key] -= self.lr * grads[key] # 학습률 * 기울기를 빼면서 점차 매개변수를 갱신(학습한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ac0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in range(10000):\n",
    "    '''\n",
    "    x_batch, t_batch = get_mini_batch()\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    params = network.params\n",
    "    optimizer.update(params, grads)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b37d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None: # v는 물체의 속도\n",
    "            self.v = {}\n",
    "            for key, val in params.items(): \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69befbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = Momentum()\n",
    "momentum.update(grad_numerical, grad_backprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b002c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ec5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1))\n",
    "\n",
    "x = np.random.rand(1000, 100) # 1000개의 data\n",
    "node_num = 100 # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5\n",
    "activations = {}\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "        \n",
    "    w = np.random.rand(node_num, node_num) / np.sqrt(node_num) # 가중치 랜덤\n",
    "    a = np.dot(x, w) # 가중치, 입력값 행렬곱\n",
    "    z = sigmoid(a) # 출력값을 산출, 활성화 함수는 sigmoid 함수를 사용\n",
    "    activations[i] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef88ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZYElEQVR4nO3df3Bd9X3m8fcTKwpNIQRikU0kxcaRUSzTNMQ2yUy3FCbpCtyuoNOsI28JoUlL0spJu+lmYHcbL3GZxtkuaWfHJi3EWSA0ctykE2kBm0nYeDvNQIwILTGi2C4mSEopNj9NCAibz/5xjizJ91o6tq6519/zvGbujM4933v0vY/PeXTuPVeyIgIzMzv5va7eEzAzs9pwoZuZJcKFbmaWCBe6mVkiXOhmZolwoZuZJSKZQpf0mKQP1nsejca5VHImlSSFpI56z6PRnGy5NHShS1ojaUjSy5Jurvd8GoGkN0jaJOnHkg5I+gdJl9R7XvUm6TZJ/yLpeUm7JP1OvefUKCQtlvSSpNvqPZdGIGl7nscL+e2Res+pVhq60IGfANcBX633RKqR1FSHb9sEjAC/ApwO/DGwRdLCOsylqjrl8gVgYUS8CegBrpO0rA7zqKpOmUzYCNxXx+9flaR5dfz2ayLi1PzWWcd5VJhLLg1d6BHxtxHxbeCpY3mcpPMl3SPp2fysbYOk5nzdRknXHzF+UNJ/yr9+u6RvSdonaa+kT08Zd62kb+Zng88DV871OR6riPhpRFwbEY9FxKsRcTuwF5i1vBLP5aGIeHliMb+9c7bHpZxJPo9e4Fng7mN4zK9JeiB/tTMi6dop6+6Q9Kkjxj8o6Tfyr98l6TuSnpb0iKRVU8bdLOnLku6U9FPgojk+vdfUSZFLRDT8jews/eZZxjwGfDD/ehnwfrKz2YXAw8Af5uvOJzvzf12+PB94EXgr2Q+4+4G1QDOwCHgU6M7HXgu8AlyWj/25BsjmrcBLwLvKngtwQz7nAH4InFrmTIA3AbuAtnw+t80wNoCO/OsLgV/I5/1u4F+By/J1q4AfTHncL5KdcDUDP0/26vG38zzPA/YDXfnYm4HngF/Kt31KnfaT7cC+fG7fBy5MJZeGPkM/XhFxf0TcGxEHI+Ix4K/I3qIgInaQhfeBfHgvsD0i/hVYAbRExLqIGI+IR4Gb8jET7omIb0d2dvyz1+o5VSPp9cBfA7dExD/NNj71XCLi94HTgF8G/hZ4eeZHJJ/JnwCbImL0WB4UEdsj4kf5vB8E+skzAQaBcyQtzpc/AnwjIsaBXwcei4j/nef5APAt4D9M2fxARHw/3/ZLc3lyc3A12Q/gVuBG4P9ImvXV3MmQy0lZ6JK2Trmg8VtV1p8j6XZJT+Qvd/+U7Oxqwi3A5fnXlwNfy79eALw9f/n9rKRngf9KdkY2YaTWz+d4SHod2bzHgTX5faXPJSIORcTfk52V/l5ZM5H0HuCDwJ9XWffQlEx+ucr690n6Xv5W0nPAJ8kzycvmG8Dl+T64mumZvO+ITH4L+DdTNl/3/SQifhARByLi5Yi4hewsfWUKudTzQs1xi4jZPtXxZeABYHVEHJD0h8CHpqy/Ddgp6ReBJcC38/tHgL0RsZijq/ufp5QkYBNZeayMiFfAuRyhCXhniTO5kOwtpMez3YVTgXmSuiJi6SyP/TqwAbgkIl6S9BdU/pD7GvD3wIsRcU9+/wjw/yLiV2fYdqPtJ5DNSSnk0tBn6JKaJJ0CzCPbGU9RsU8LnAY8D7wg6V3A701dmb8EvY8s/G9NeTm8Azgg6WpJPydpnqRzJa2o2ZOqjS+Tlcu/P8aX8knmIuksSb2STs3n1k12hlTkQmCSmZC9lfBO4D357S+BO4DuAo89DXg6L63zgf84dWVeVK8C1zN5FgpwO9nbDh+R9Pr8tkLSkrk+mVqR9GZJ3RNdkr9quwDYVuDhDZ9LQxc62UfyfgZcQ/Zy92f5fbP5z2RhHyB7X/MbVcbcQnaB43DwEXGI7P2u95B9cmQ/8BWyjwc2BEkLgE+QzfGJmd5OqCLVXIKsiEeBZ4D/SXZhc7DAY5PMJCJejIgnJm7AC8BLEbGvwMN/H1gn6QDZRd8tVcbcSpbJ4c+2R8QB4N+RXUf4CfAE8EXgDXN6MrX1erIPWUxcFP0U2YXNXQUe2/C5KKIRXwGdeJIuIAt9QZQ1hCqcSyVnUknSFcBVEfFv6z2XRlLvXBr9DP2EUPbpkD8AvuIDdJJzqeRMKkl6I9nZ6o31nksjaYRcSlfo+ftWzwJvA/6irpNpIM6lkjOplF+f2Ef2Geyv13k6DaNRcintWy5mZqkp3Rm6mVmq6vY59Pnz58fChQvr9e1fM/fff//+iGgpMtaZVFeGXJxJdT5+Ks2USd0KfeHChQwNDdXr279mJP246FhnUl0ZcnEm1fn4qTRTJn7LxcwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEzFrokr4q6UlJO4+yXpL+l6Q9yv4PvffWfpqN52Mf+xhnnXUW5557btX1EcGnP/1pgHPLkoszqeRMKhXNpKOjA6CrDJnUSpEz9JuBi2dYfwmwOL9dRfa3upN35ZVXsm3b0f+E8tatW9m9ezfATkqSizOp5EwqFc0kz+XHlCCTWpm10CPi74CnZxhyKXBrZO4F3izpbbWaYKO64IILOPPMM4+6fmBggCuuuAKAsuTiTCo5k0pFM8n/p6WfUoJMaqUW76G3Mv3/wxvN76sg6SpJQ5KG9u0r8nf262PhNXew8Jo75rSNsbEx2tvbp95VNZeTJRPgNcsETp5c5rqvOJNKKWYCcz9+inhNL4pGxI0RsTwilre0FP6zFUlzJtU5l0rOpJIzma4WhT4GTP1x2pbfV2qtra2MjEz7j7xLn4szqeRMKjmT41eLQh8Ersg/7fJ+4LmI+JcabPek1tPTw6233gqAc8k4k0rOpNJEJvn/1fDzOJPCZv1ri5L6gQuB+ZJGgf9O9h+tEhF/CdwJrAT2AC8Cv32iJttIVq9ezfbt29m/fz9tbW18/vOf55VXXgHgk5/8JCtXruTOO+8EOJfsPx9OPhdnUsmZVCqaSf6xxQXABfWc78lk1kKPiNWzrA+gr2YzOkn09/fPuF4SGzdu5IYbbtgZEctfo2nVlTOp5EwqFc0k/3o4ItL/m7g14t8UNTNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MElGo0CVdLOkRSXskXVNl/TskfU/SA5IelLSy9lNtLNu2baOzs5OOjg7Wr19fsf7xxx/noosuAuhyJpkyZgLFcgHO8fEzqaz7ylzNWuiS5gEbgUuALmC1pK4jhv0xsCUizgN6gRtqPdFGcujQIfr6+ti6dSvDw8P09/czPDw8bcx1113HqlWrAIZxJkD5MoHiuQDP+PiZVMZ9pRaKnKGfD+yJiEcjYhzYDFx6xJgA3pR/fTrwk9pNsfHs2LGDjo4OFi1aRHNzM729vQwMDEwbI4nnn39+YtGZUL5MoHguwLx8MflcvK+cOEUKvRUYmbI8mt831bXA5ZJGgTuBT1XbkKSrJA1JGtq3b99xTLcxjI2N0d7efni5ra2NsbGxaWOuvfZabrvtNoB340yA4plA+XIBzvTxM6lsx0+t1Oqi6Grg5ohoA1YCX5NUse2IuDEilkfE8paWlhp968bU39/PlVdeCfAgzgQongmULxfgKR8/k3z8HJ8ihT4GtE9Zbsvvm+rjwBaAiLgHOAWYX4sJNqLW1lZGRiZftIyOjtLaOv1Fy6ZNmybeA3QmubJlAsVzAZ6GcuTifeXEKVLo9wGLJZ0tqZnsAsXgEWMeBz4AIGkJWfjJvv5ZsWIFu3fvZu/evYyPj7N582Z6enqmjXnHO97B3XffDTiTCWXLBIrnQn4Nqgy5eF85cWYt9Ig4CKwB7gIeJvs0y0OS1kma+Ff4I+B3Jf0j0A9cGRFxoiZdb01NTWzYsIHu7m6WLFnCqlWrWLp0KWvXrmVwMPtZd/3113PTTTdB9skgZ0L5MoHiuQAtPn7Kva/URETU5bZs2bJoVAuuvj0WXH17TbYFDEUCmUREXTKJBs+lVvuKM6nOx0+lmTLxb4qamSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJKFToki6W9IikPZKuOcqYVZKGJT0k6eu1nWbj2bZtG52dnXR0dLB+/fqqY7Zs2QKw1JlMKlsmUCwX4AwfP9OVcV+Zq6bZBkiaB2wEfhUYBe6TNBgRw1PGLAb+C/BLEfGMpLNO1IQbwaFDh+jr6+M73/kObW1trFixgp6eHrq6ug6P2b17N1/4whcA/ikiznMm5csEiucCvA1o9fGTKeO+UgtFztDPB/ZExKMRMQ5sBi49YszvAhsj4hmAiHiyttNsLDt27KCjo4NFixbR3NxMb28vAwMD08bcdNNN9PX1ARwCZwLlywSK5wI86eNnUhn3lVooUuitwMiU5dH8vqnOAc6R9H1J90q6uFYTbERjY2O0t7cfXm5ra2NsbGzamF27drFr1y6AdzmTTNkygeK5AKf4+JlUxn2lFmp1UbQJWAxcCKwGbpL05iMHSbpK0pCkoX379tXoWzemgwcPTryUfgRnAhTPBMqXC/AGfPwc5uPn+BQp9DGgfcpyW37fVKPAYES8EhF7gV1kBT9NRNwYEcsjYnlLS8vxzrnuWltbGRmZfNEyOjpKa+v0Fy1tbW309PQAhDPJFM0kH1CqXIBnffxMKtvxUytFCv0+YLGksyU1A73A4BFjvk12doGk+WRvwTxau2k2lhUrVrB792727t3L+Pg4mzdvntj5DrvsssvYvn074EwmlC0TKJ4LcBqUIxfvKyfOrIUeEQeBNcBdwMPAloh4SNI6SRP/CncBT0kaBr4HfDYinjpRk663pqYmNmzYQHd3N0uWLGHVqlUsXbqUtWvXMjiY/azr7u7mLW95C8BSnAlQvkygeC7AQR8/5d5XaiIi6nJbtmxZNKoFV98eC66+vSbbAoYigUwioi6ZRIPnUqt9xZlU5+On0kyZ+DdFzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwSUajQJV0s6RFJeyRdM8O435QUkpbXboqNadu2bXR2dtLR0cH69etnGvpmZ1KhNJlA8Vx8/FRVqn1lrmYtdEnzgI3AJUAXsFpSV5VxpwF/APyg1pNsNIcOHaKvr4+tW7cyPDxMf38/w8PDFeMOHDgA8FacyWFlygSK50J2LPr4maJs+0otFDlDPx/YExGPRsQ4sBm4tMq4PwG+CLxUw/k1pB07dtDR0cGiRYtobm6mt7eXgYGBinGf+9znAJ7AmRxWpkygeC5AKz5+pinbvlILRQq9FRiZsjya33eYpPcC7RFxx0wbknSVpCFJQ/v27TvmyTaKsbEx2tvbDy+3tbUxNjY2bcwPf/hDRkZGAJ6baVvOpLqy5QI0+/iZVLbjp1bmfFFU0uuALwF/NNvYiLgxIpZHxPKWlpa5fuuG9eqrr/KZz3yG66+/ftaxzqS6suXC9JOmqsqWiY+fY1ek0MeA9inLbfl9E04DzgW2S3oMeD8wmPJFjNbW1omzBwBGR0dpbZ180XLgwAF27tzJhRdeCPALOJNSZgLFcwE6ffxkyrqv1EKRQr8PWCzpbEnNQC8wOLEyIp6LiPkRsTAiFgL3Aj0RMXRCZtwAVqxYwe7du9m7dy/j4+Ns3ryZnp6ew+tPP/109u/fz2OPPQbwI5xJKTOB4rkAP/LxkynrvlILsxZ6RBwE1gB3AQ8DWyLiIUnrJPXM/Og0NTU1sWHDBrq7u1myZAmrVq1i6dKlrF27lsHBwdk3kCBnUp1zqeRMTpymIoMi4k7gziPuW3uUsRfOfVqNb+XKlaxcuXLafevWras61plUKksm4FyqcSYnhn9T1MwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS0ShQpd0saRHJO2RdE2V9Z+RNCzpQUl3S1pQ+6k2lm3bttHZ2UlHRwfr16+vWP+lL32Jrq4ugC5nkiljJlAsF2Cpj59JZd1X5mrWQpc0D9gIXAJ0AasldR0x7AFgeUS8G/gm8D9qPdFGcujQIfr6+ti6dSvDw8P09/czPDw8bcx5553H0NAQwDDOBChfJlA8F+BhHz+Tyriv1EKRM/TzgT0R8WhEjAObgUunDoiI70XEi/nivUBbbafZWHbs2EFHRweLFi2iubmZ3t5eBgYGpo256KKLeOMb3zix6EwoXyZQPBfg1Xwx+Vy8r5w4RQq9FRiZsjya33c0Hwe2Vlsh6SpJQ5KG9u3bV3yWDWZsbIz29vbDy21tbYyNjc30EGdS6aiZgHOptsKZVEolk1qp6UVRSZcDy4E/q7Y+Im6MiOURsbylpaWW37qRnYkzOdKMmUA5c/HxU5WPn2PQVGDMGNA+Zbktv28aSR8E/hvwKxHxcm2m15haW1sZGZl80TI6Okpra+WLlu9+97sAbwPe70wyZcoEiucCnIaPn2nKtq/UQpEz9PuAxZLOltQM9AKDUwdIOg/4K6AnIp6s/TQby4oVK9i9ezd79+5lfHyczZs309PTM23MAw88wCc+8QnIrj84E8qXCRTPBViAj5/Dyriv1MKshR4RB4E1wF3Aw8CWiHhI0jpJE/8KfwacCvyNpH+QNHiUzSWhqamJDRs20N3dzZIlS1i1ahVLly5l7dq1DA5mT/2zn/0sL7zwAsA7nUk5M4HiuQDz8PFT6n2lJiKiLrdly5ZFo1pw9e2x4Orba7ItYCgSyCQi6pJJNHgutdpXnEl1Pn4qzZSJf1PUzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRhQpd0sWSHpG0R9I1Vda/QdI38vU/kLSw5jNtMNu2baOzs5OOjg7Wr19fsf7ll1/mwx/+MMC5ziRTxkygWC7AIh8/k8q6r8zVrIUuaR6wEbgE6AJWS+o6YtjHgWciogP4c+CLtZ5oIzl06BB9fX1s3bqV4eFh+vv7GR4enjZm06ZNnHHGGQA7cSZA+TKB4rkAB338TCrjvlILRc7Qzwf2RMSjETEObAYuPWLMpcAt+dffBD4gSbWbZmPZsWMHHR0dLFq0iObmZnp7exkYGJg2ZmBggI9+9KMTi86E8mUCxXMBnsoXk8/F+8qJo4iYeYD0IeDiiPidfPkjwPsiYs2UMTvzMaP58j/nY/Yfsa2rgKvyxU7gkfzr+cC0sQ3uDOBNwI/z5TOBU4HH8+X5wFuBXcDbI6LFmRTPBI6ay8mWCRTP5cWIeAv4+KGcx89spj6fBRHRUnVURMx4Az4EfGXK8keADUeM2Qm0TVn+Z2D+bNueMn6o6NhGuM2WCTDkTJyJc3EmNcys0PMp8pbLGNA+Zbktv6/qGElNwOlMvoRMkTOp5Eyqcy6VnMkJUqTQ7wMWSzpbUjPQCwweMWYQmHjD60PA/438x0qinEklZ1Kdc6nkTE6QptkGRMRBSWuAu4B5wFcj4iFJ68heBgwCm4CvSdoDPE32D3QsbjzG8XU1WyZkz+dWnEmpMwHnUo0zOS6Fns+sF0XNzOzk4N8UNTNLhAvdzCwRdS302f6kwMlG0lclPZl/Lv94t+FMqm/HuVRuw5lUbqPcmdTxc5XzyD5bughoBv4R6Kr35z3n+JwuAN4L7HQmtcnEuTgTZ1I8k3qeoRf5kwInlYj4O7Ir8sfLmVTnXCo5k0qlz6Sehd4KjExZHs3vKzNnUp1zqeRMKpU+E18UNTNLRD0Lvciv/5aNM6nOuVRyJpVKn0k9C73Ir/+WjTOpzrlUciaVSp9J3Qo9Ig4CE7/++zCwJSIeqtd8akFSP3AP0ClpVNLHj+XxzqQ651LJmVRyJv7VfzOzZPiiqJlZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXi/wPQkWitYvXQhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fc00f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = 100\n",
    "w = np.random.rand(node_num, node_num) / np.sqrt(node_num)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae64d6c",
   "metadata": {},
   "source": [
    "# 배치 정규화\n",
    "배치 정규화 알고리즘은 2015년에 제안된 알고리즘으로 여러 장점이 있다.\n",
    "- 학습을 빨리 진행할 수 있다(학습 속도 개선)\n",
    "- 초깃값에 크게 의존하지 않는다(골치 아픈 초깃값 선택 장애 불필요)\n",
    "- 오버피팅을 억제한다(드롭아웃 등의 필요성 감소)\n",
    "## 배치 정규화의 기본 아이디어\n",
    "- 각 층에서의 활성화 값이 적당히 분포되도록 조정하는 것이다. \n",
    "- 학습 시 미니배치를 단위로 정규화한다. 구체적으로는 데이터 분포의 평균이 0, 분산이 1이 되도록 정규화한다.\n",
    "- 수식은 다음과 같다\n",
    "    - $$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m} x_i$$\n",
    "    - $$\\alpha^2_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^m(x_i-\\mu_B)^2$$\n",
    "    - $$\\hat{x}_i \\leftarrow \\frac{x_i-\\mu_B}{\\sqrt{\\alpha^2_B+\\epsilon}} $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10e735bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08160967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
