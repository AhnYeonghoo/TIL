{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b47bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f815ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2935973693278533\n",
      "=== epoch:1, train acc:0.121, test acc:0.108 ===\n",
      "train loss:2.2921287813192617\n",
      "train loss:2.2765483610202075\n",
      "train loss:2.2934699043294553\n",
      "train loss:2.294995093089073\n",
      "train loss:2.253465435053731\n",
      "train loss:2.2975309355413485\n",
      "train loss:2.2937368493124946\n",
      "train loss:2.2670947225481934\n",
      "train loss:2.2621722951851924\n",
      "train loss:2.2440485023349894\n",
      "train loss:2.2208055169277308\n",
      "train loss:2.2466226918840677\n",
      "train loss:2.2203284878437173\n",
      "train loss:2.217710079496211\n",
      "train loss:2.1954819000538914\n",
      "train loss:2.254555767650513\n",
      "train loss:2.192551038286831\n",
      "train loss:2.22958311711636\n",
      "train loss:2.1478657413125517\n",
      "train loss:2.1953477474061263\n",
      "train loss:2.1303943512476113\n",
      "train loss:2.1266079879281863\n",
      "train loss:2.1096416016107997\n",
      "train loss:2.096335909362765\n",
      "train loss:2.1678232106132467\n",
      "train loss:2.129768390925228\n",
      "train loss:2.144999943181999\n",
      "train loss:2.063915182380604\n",
      "train loss:1.927388214653635\n",
      "train loss:2.079129869057731\n",
      "train loss:2.0850727259229314\n",
      "train loss:2.087980063978553\n",
      "train loss:1.8359825173253745\n",
      "train loss:1.9747255075828056\n",
      "train loss:2.0637917279812146\n",
      "train loss:1.9983136473461056\n",
      "train loss:2.015382221742815\n",
      "train loss:1.9798608519030274\n",
      "train loss:2.010596449352195\n",
      "train loss:2.0615627818708044\n",
      "train loss:2.020569006601662\n",
      "train loss:1.8740284415850195\n",
      "train loss:2.0531982814884975\n",
      "train loss:1.9353837216238983\n",
      "train loss:1.9212341441787535\n",
      "train loss:1.9102223598781642\n",
      "train loss:1.9123523689130806\n",
      "train loss:1.9732358585498861\n",
      "train loss:1.9968606809963365\n",
      "train loss:1.9907391516239885\n",
      "train loss:1.817013290760733\n",
      "train loss:1.8491155950010327\n",
      "train loss:1.8736793563404623\n",
      "train loss:1.828221457280019\n",
      "train loss:1.9172975091547435\n",
      "train loss:1.7589252892025724\n",
      "train loss:1.821239785738991\n",
      "train loss:1.9187111416901794\n",
      "train loss:1.795104387900323\n",
      "train loss:1.8784142604405045\n",
      "train loss:1.9820163231118928\n",
      "train loss:1.8579737706943293\n",
      "train loss:1.685363497781241\n",
      "train loss:1.7801784016486368\n",
      "train loss:1.8491044440101263\n",
      "train loss:1.893870222215348\n",
      "train loss:1.9191395291587687\n",
      "train loss:1.8365098262880672\n",
      "train loss:1.877499490604279\n",
      "train loss:1.6828648251794933\n",
      "train loss:1.7006026300658457\n",
      "train loss:1.8282339664454945\n",
      "train loss:1.6201594967217634\n",
      "train loss:1.7904847264545147\n",
      "train loss:1.7089171502297917\n",
      "train loss:1.5744990529807346\n",
      "train loss:1.7058584671957966\n",
      "train loss:1.7946722720392452\n",
      "train loss:1.8382458453632389\n",
      "train loss:1.675784643774514\n",
      "train loss:1.6762627636955205\n",
      "train loss:1.800070147768146\n",
      "train loss:1.7245551518179993\n",
      "train loss:1.618670039072089\n",
      "train loss:1.7358835045287455\n",
      "train loss:1.7545516089025137\n",
      "train loss:1.7599787463697578\n",
      "train loss:1.6241466907839546\n",
      "train loss:1.5863621655892857\n",
      "train loss:1.6766462205563588\n",
      "train loss:1.7329178722960137\n",
      "train loss:1.7153563532163716\n",
      "train loss:1.640367756058546\n",
      "train loss:1.720092774910981\n",
      "train loss:1.6461534396813946\n",
      "train loss:1.5383802538041127\n",
      "train loss:1.6481110823799006\n",
      "train loss:1.5102997575940247\n",
      "train loss:1.6446481398277746\n",
      "train loss:1.6271724066035682\n",
      "train loss:1.6159867141373832\n",
      "train loss:1.4471744227526133\n",
      "train loss:1.5735597926004603\n",
      "train loss:1.6179616470021352\n",
      "train loss:1.6588319862517873\n",
      "train loss:1.6774216571275706\n",
      "train loss:1.5945640043935108\n",
      "train loss:1.5699775414963084\n",
      "train loss:1.5223878194612175\n",
      "train loss:1.5316531874046009\n",
      "train loss:1.7332307991440064\n",
      "train loss:1.685206864080525\n",
      "train loss:1.6625990497789815\n",
      "train loss:1.6030222501785532\n",
      "train loss:1.6952565343319534\n",
      "train loss:1.448501160634037\n",
      "train loss:1.754787550603502\n",
      "train loss:1.4744142412235923\n",
      "train loss:1.4749941054204467\n",
      "train loss:1.7894557519869203\n",
      "train loss:1.5862171107849745\n",
      "train loss:1.5212500513315\n",
      "train loss:1.739180964188916\n",
      "train loss:1.5111875072561154\n",
      "train loss:1.497146157039873\n",
      "train loss:1.704205853248888\n",
      "train loss:1.5985793957787382\n",
      "train loss:1.5706030308214403\n",
      "train loss:1.5981271746088856\n",
      "train loss:1.3735941315200475\n",
      "train loss:1.8572721925984985\n",
      "train loss:1.7059071527848853\n",
      "train loss:1.606589194265052\n",
      "train loss:1.5907780172116517\n",
      "train loss:1.531804935558584\n",
      "train loss:1.5718950958424736\n",
      "train loss:1.520797719753092\n",
      "train loss:1.4661849174543178\n",
      "train loss:1.7529180553561803\n",
      "train loss:1.3957883265074706\n",
      "train loss:1.3559379221836136\n",
      "train loss:1.6997306419059757\n",
      "train loss:1.632109971784467\n",
      "train loss:1.4438458519367854\n",
      "train loss:1.6276083186625636\n",
      "train loss:1.5630501041666056\n",
      "train loss:1.6546966791428757\n",
      "train loss:1.4055273317634387\n",
      "train loss:1.5812132522060958\n",
      "train loss:1.5611796886274407\n",
      "train loss:1.6191087794344659\n",
      "train loss:1.5204954296751056\n",
      "train loss:1.4754629800721677\n",
      "train loss:1.538573469807208\n",
      "train loss:1.624808878296269\n",
      "train loss:1.4290500358096563\n",
      "train loss:1.418075718743608\n",
      "train loss:1.4515636370408134\n",
      "train loss:1.7010393322182387\n",
      "train loss:1.405849840147991\n",
      "train loss:1.5961125935803029\n",
      "train loss:1.6052652879144969\n",
      "train loss:1.5624253698299424\n",
      "train loss:1.3937119615738316\n",
      "train loss:1.3822613566516495\n",
      "train loss:1.5001819128255565\n",
      "train loss:1.6823723697085398\n",
      "train loss:1.3549779201996537\n",
      "train loss:1.3958441840421847\n",
      "train loss:1.5076658749463379\n",
      "train loss:1.362567083024008\n",
      "train loss:1.529953511166919\n",
      "train loss:1.4693624681737805\n",
      "train loss:1.150207214317684\n",
      "train loss:1.3152136654271567\n",
      "train loss:1.4751967589879704\n",
      "train loss:1.4423628798393096\n",
      "train loss:1.482723655295152\n",
      "train loss:1.511936073380191\n",
      "train loss:1.2144402022283334\n",
      "train loss:1.4252328879248624\n",
      "train loss:1.366576018412571\n",
      "train loss:1.4430202208148872\n",
      "train loss:1.585592178113277\n",
      "train loss:1.4306259443566374\n",
      "train loss:1.6220714187702563\n",
      "train loss:1.4542208201529139\n",
      "train loss:1.3731367285977532\n",
      "train loss:1.383740485245197\n",
      "train loss:1.357587086848284\n",
      "train loss:1.4571930856502815\n",
      "train loss:1.2936183229158684\n",
      "train loss:1.4097809546719906\n",
      "train loss:1.3382881940916382\n",
      "train loss:1.5668874263984762\n",
      "train loss:1.3625137989928673\n",
      "train loss:1.4389123755756017\n",
      "train loss:1.3353681391496661\n",
      "train loss:1.4480219602044113\n",
      "train loss:1.5282579989902785\n",
      "train loss:1.5228167912396904\n",
      "train loss:1.3972139709648101\n",
      "train loss:1.3025175104116835\n",
      "train loss:1.441502689407943\n",
      "train loss:1.3637035695382096\n",
      "train loss:1.4471564028793928\n",
      "train loss:1.593124139839683\n",
      "train loss:1.567112883661307\n",
      "train loss:1.3354025770202016\n",
      "train loss:1.3124417102574222\n",
      "train loss:1.4433507636944003\n",
      "train loss:1.4255591998388693\n",
      "train loss:1.4414830265609961\n",
      "train loss:1.3427761089982786\n",
      "train loss:1.580941008719134\n",
      "train loss:1.4664532150861405\n",
      "train loss:1.4980870431683388\n",
      "train loss:1.3422939816733293\n",
      "train loss:1.339660742437154\n",
      "train loss:1.4320466162201244\n",
      "train loss:1.3439212017098447\n",
      "train loss:1.46268986386277\n",
      "train loss:1.3364508635959544\n",
      "train loss:1.3397846666408624\n",
      "train loss:1.5312273908990446\n",
      "train loss:1.449277036826944\n",
      "train loss:1.5290831847692983\n",
      "train loss:1.3637443954437167\n",
      "train loss:1.6656821246722864\n",
      "train loss:1.2604147253553348\n",
      "train loss:1.3896801203068163\n",
      "train loss:1.3055924297314767\n",
      "train loss:1.4946986012438837\n",
      "train loss:1.2873749750276133\n",
      "train loss:1.466550656354393\n",
      "train loss:1.4273373497118675\n",
      "train loss:1.3102698746048111\n",
      "train loss:1.2780578881850604\n",
      "train loss:1.4794144449466669\n",
      "train loss:1.3338454683481877\n",
      "train loss:1.284790780335446\n",
      "train loss:1.4628843458280938\n",
      "train loss:1.1754478805699542\n",
      "train loss:1.43362185012701\n",
      "train loss:1.2763569876929983\n",
      "train loss:1.340989221531315\n",
      "train loss:1.386815138638637\n",
      "train loss:1.1937924646169307\n",
      "train loss:1.3368596308513134\n",
      "train loss:1.2155028317570409\n",
      "train loss:1.3166520786805245\n",
      "train loss:1.4410471345456748\n",
      "train loss:1.4090136250444112\n",
      "train loss:1.5033519800021224\n",
      "train loss:1.4987896619187515\n",
      "train loss:1.2825736763237707\n",
      "train loss:1.2729216956520515\n",
      "train loss:1.33315125438096\n",
      "train loss:1.1860491496483945\n",
      "train loss:1.5059556584526859\n",
      "train loss:1.4331530437374786\n",
      "train loss:1.3874224856307051\n",
      "train loss:1.1931455368352835\n",
      "train loss:1.2246993132970152\n",
      "train loss:1.2741141737450408\n",
      "train loss:1.2073768424476183\n",
      "train loss:1.558833786630896\n",
      "train loss:1.496508649725906\n",
      "train loss:1.3769830331163673\n",
      "train loss:1.3031543898603228\n",
      "train loss:1.3709608838501344\n",
      "train loss:1.460801490628836\n",
      "train loss:1.500166705533705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.4914415367286755\n",
      "train loss:1.5203845275028756\n",
      "train loss:1.3609230238522003\n",
      "train loss:1.252672512400224\n",
      "train loss:1.317024931521861\n",
      "train loss:1.2998432837774812\n",
      "train loss:1.172534493810737\n",
      "train loss:1.183160190958145\n",
      "train loss:1.2374101496118777\n",
      "train loss:1.3456159500303975\n",
      "train loss:1.386438672851817\n",
      "train loss:1.438939855354978\n",
      "train loss:1.357239477223739\n",
      "train loss:1.249863904094446\n",
      "train loss:1.2707960307438062\n",
      "train loss:1.2634392446572067\n",
      "train loss:1.2254754542562418\n",
      "train loss:1.2066902107521542\n",
      "train loss:1.3255634727118655\n",
      "train loss:1.1123776724337444\n",
      "train loss:1.2096256637915699\n",
      "train loss:1.0557378763027128\n",
      "train loss:1.3412137627496272\n",
      "train loss:1.4332436360787582\n",
      "train loss:1.5898150715514117\n",
      "train loss:1.3791361450589505\n",
      "train loss:1.2385137717881094\n",
      "train loss:1.3821419233261156\n",
      "train loss:1.2280765595903962\n",
      "train loss:1.5994158240733696\n",
      "train loss:1.4019749518528908\n",
      "train loss:1.3868506525948758\n",
      "train loss:1.1221835755024376\n",
      "train loss:1.268019421655115\n",
      "train loss:1.4870033577232733\n",
      "train loss:1.296165837996507\n",
      "train loss:1.2594736469048733\n",
      "train loss:1.2072423978726983\n",
      "train loss:1.3760954589748107\n",
      "train loss:1.4558185287853518\n",
      "train loss:1.4312250417316756\n",
      "train loss:1.2878504413890337\n",
      "train loss:1.3430046480537228\n",
      "train loss:1.1798709382224541\n",
      "train loss:1.35854689511853\n",
      "train loss:1.3664780746837997\n",
      "train loss:1.2656244596901598\n",
      "train loss:1.150287209840998\n",
      "train loss:1.1782357687638376\n",
      "train loss:1.3445855731837968\n",
      "train loss:1.258499829163017\n",
      "train loss:1.0628064403343682\n",
      "train loss:1.3416288099335352\n",
      "train loss:1.3986321827155948\n",
      "train loss:1.2179730959000739\n",
      "train loss:1.2320502385452052\n",
      "train loss:1.3100618078922104\n",
      "train loss:1.1809229834625221\n",
      "train loss:1.295598350299384\n",
      "train loss:1.152127492591004\n",
      "train loss:1.259579048498752\n",
      "train loss:1.272072921226493\n",
      "train loss:1.3212650968215707\n",
      "train loss:1.190275471620866\n",
      "train loss:1.4052027360045336\n",
      "train loss:1.1671798996789937\n",
      "train loss:1.379464717496667\n",
      "train loss:1.3101040949842322\n",
      "train loss:1.433808885068764\n",
      "train loss:1.2606208556539777\n",
      "train loss:1.191183987571298\n",
      "train loss:1.3225890520766168\n",
      "train loss:1.2039655954877573\n",
      "train loss:1.3519413238709057\n",
      "train loss:1.3188483586062312\n",
      "train loss:1.2294143699973288\n",
      "train loss:1.1104964673101516\n",
      "train loss:1.2088468904675593\n",
      "train loss:1.2777726179775493\n",
      "train loss:1.05795449787983\n",
      "train loss:1.2084715859970607\n",
      "train loss:1.3530064725769468\n",
      "train loss:1.2596436343189588\n",
      "train loss:1.3587846867718263\n",
      "train loss:1.1202595487739055\n",
      "train loss:1.0262664899118512\n",
      "train loss:1.2296098733006067\n",
      "train loss:1.3158174003561875\n",
      "train loss:1.3175610101285704\n",
      "train loss:1.27940498981078\n",
      "train loss:1.3342312888530237\n",
      "train loss:1.3413443055592802\n",
      "train loss:1.0022646858523685\n",
      "train loss:1.0968376156576873\n",
      "train loss:1.236526111981649\n",
      "train loss:1.0662494964038844\n",
      "train loss:1.118659670292486\n",
      "train loss:1.1274693616085876\n",
      "train loss:1.2645816231921991\n",
      "train loss:1.3275212793793505\n",
      "train loss:1.280783125947253\n",
      "train loss:1.3546073047411766\n",
      "train loss:1.3033539403519818\n",
      "train loss:1.319105300189162\n",
      "train loss:1.2576014058832488\n",
      "train loss:1.221316663178806\n",
      "train loss:1.3947451788769019\n",
      "train loss:1.177959949409329\n",
      "train loss:1.266441376106574\n",
      "train loss:1.2228133114227027\n",
      "train loss:1.2728630426804657\n",
      "train loss:1.2555375533207922\n",
      "train loss:1.2704540187798623\n",
      "train loss:1.1363874511878107\n",
      "train loss:1.2628097505216553\n",
      "train loss:1.1592022544443976\n",
      "train loss:1.3300819244469175\n",
      "train loss:1.1249668496446372\n",
      "train loss:1.1427135796087704\n",
      "train loss:1.0849682716003537\n",
      "train loss:1.2524196178488052\n",
      "train loss:1.2935453964003656\n",
      "train loss:1.2852040626232764\n",
      "train loss:1.2644889580520196\n",
      "train loss:1.209469713084227\n",
      "train loss:1.1983111715663182\n",
      "train loss:1.1128792082456735\n",
      "train loss:1.2634546275996912\n",
      "train loss:1.3221478584813975\n",
      "train loss:1.0911615851239065\n",
      "train loss:1.1998747106057084\n",
      "train loss:1.1937537029090706\n",
      "train loss:1.2033923653461365\n",
      "train loss:1.1242723265925634\n",
      "train loss:1.1226454019547152\n",
      "train loss:1.3114656907067803\n",
      "train loss:1.2121426147453394\n",
      "train loss:1.3834994454003153\n",
      "train loss:1.3020149337789875\n",
      "train loss:1.3525415630447188\n",
      "train loss:1.284041210913196\n",
      "train loss:1.2911951994465911\n",
      "train loss:1.3349877315975078\n",
      "train loss:1.2237026916304157\n",
      "train loss:1.061893498983983\n",
      "train loss:1.0936069048378487\n",
      "train loss:1.1901276321948528\n",
      "train loss:1.1525426796455562\n",
      "train loss:1.179877783414662\n",
      "train loss:1.1817314624263018\n",
      "train loss:1.2250904068458224\n",
      "train loss:1.2814071902998614\n",
      "train loss:1.3110977355993552\n",
      "train loss:1.2333106658005233\n",
      "train loss:1.1852242027240274\n",
      "train loss:1.191108107851937\n",
      "train loss:1.2012249270502493\n",
      "train loss:1.1167485239157358\n",
      "train loss:1.2341201421430747\n",
      "train loss:1.2740231000876763\n",
      "train loss:1.1108466418053726\n",
      "train loss:1.056235882289661\n",
      "train loss:1.3073364014366589\n",
      "train loss:1.254064524848847\n",
      "train loss:1.305594327826729\n",
      "train loss:1.1508938450285657\n",
      "train loss:1.2000411214492868\n",
      "train loss:1.2553350049949221\n",
      "train loss:1.1957729312520755\n",
      "train loss:1.1892996517365884\n",
      "train loss:1.3198069787670983\n",
      "train loss:1.2502776150921564\n",
      "train loss:1.1520410699129007\n",
      "train loss:1.1310950266911717\n",
      "train loss:1.125781505741262\n",
      "train loss:1.2335603813072538\n",
      "train loss:1.230312883852887\n",
      "train loss:1.1509739080693977\n",
      "train loss:1.171179181632959\n",
      "train loss:1.3347567573141197\n",
      "train loss:1.087362078276106\n",
      "train loss:1.2858558961816136\n",
      "train loss:1.1823501442797146\n",
      "train loss:1.2191331706173314\n",
      "train loss:1.303936682321441\n",
      "train loss:1.4264562820258966\n",
      "train loss:1.0581888779528235\n",
      "train loss:1.0063630767985992\n",
      "train loss:1.3518111322948194\n",
      "train loss:1.1353092863174128\n",
      "train loss:1.0770851348675188\n",
      "train loss:1.005999508012324\n",
      "train loss:1.3125860576675257\n",
      "train loss:1.2893771990894356\n",
      "train loss:1.1479252461386031\n",
      "train loss:1.2863028111525652\n",
      "train loss:1.2379755924411964\n",
      "train loss:1.2364511662136737\n",
      "train loss:1.1610871548788833\n",
      "train loss:1.127568148837609\n",
      "train loss:1.0309670052568058\n",
      "train loss:1.182608875082041\n",
      "train loss:1.1513978219740768\n",
      "train loss:1.0533622791383712\n",
      "train loss:1.192634787458546\n",
      "train loss:1.2003189979559206\n",
      "train loss:1.0813689816124752\n",
      "train loss:1.0617618449658384\n",
      "train loss:1.0954203363603248\n",
      "train loss:1.2312089053578616\n",
      "train loss:1.097113396665848\n",
      "train loss:1.1966121936494263\n",
      "train loss:1.0647504782171273\n",
      "train loss:1.2277037115347624\n",
      "train loss:1.2496651064624225\n",
      "train loss:1.1000475045371472\n",
      "train loss:1.1828289353847596\n",
      "train loss:1.167586287744458\n",
      "train loss:1.0844429970286722\n",
      "train loss:1.2704910194630614\n",
      "train loss:1.3581139028890363\n",
      "train loss:1.2148786207110078\n",
      "train loss:1.240416502395497\n",
      "train loss:1.0749569646417065\n",
      "train loss:1.1739777054309886\n",
      "train loss:1.191042812144183\n",
      "train loss:1.2241393287788764\n",
      "train loss:1.2586115446508392\n",
      "train loss:1.0520600909881248\n",
      "train loss:1.0800815005735147\n",
      "train loss:1.3153711079852406\n",
      "train loss:1.236685113493128\n",
      "train loss:1.168827174159186\n",
      "train loss:1.1722694128353952\n",
      "train loss:1.1815620906485713\n",
      "train loss:1.066714980191975\n",
      "train loss:1.0758358734727487\n",
      "train loss:1.1430399870136059\n",
      "train loss:0.9956177980021721\n",
      "train loss:1.1118049134855166\n",
      "train loss:1.2355245693520853\n",
      "train loss:1.0376640287299268\n",
      "train loss:1.005238173632478\n",
      "train loss:1.3269839384411108\n",
      "train loss:1.1333670661661268\n",
      "train loss:1.1885869017672603\n",
      "train loss:1.217538057279345\n",
      "train loss:1.217386847050558\n",
      "train loss:1.1643769108731405\n",
      "train loss:0.9633458194359563\n",
      "train loss:1.1042276361410435\n",
      "train loss:0.9897665089450611\n",
      "train loss:1.3568311412882776\n",
      "train loss:1.049893569223581\n",
      "train loss:1.2936539885834657\n",
      "train loss:1.042869610841681\n",
      "train loss:1.0549167851500936\n",
      "train loss:1.1175817930015624\n",
      "train loss:1.0289669583946588\n",
      "train loss:1.1835486765300929\n",
      "train loss:1.0970035120091957\n",
      "train loss:1.084704127042004\n",
      "train loss:1.0647258936229205\n",
      "train loss:1.3020247899604893\n",
      "train loss:0.994300373731287\n",
      "train loss:1.2154119478714838\n",
      "train loss:1.1913328632962157\n",
      "train loss:1.2321221799181847\n",
      "train loss:1.274550845019223\n",
      "train loss:1.1969203088986782\n",
      "train loss:1.0499263757046144\n",
      "train loss:1.2884247543050638\n",
      "train loss:1.2932897767931832\n",
      "train loss:1.2658885467188485\n",
      "train loss:1.0754685171077463\n",
      "train loss:1.291015002967963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.2141502466971845\n",
      "train loss:1.1364399283406996\n",
      "train loss:1.2063501614429075\n",
      "train loss:1.1083554407876381\n",
      "train loss:1.205663721388112\n",
      "train loss:1.1811788811202184\n",
      "train loss:1.24934765178749\n",
      "train loss:1.060238102335024\n",
      "train loss:1.0931806550957335\n",
      "train loss:1.0100133803415519\n",
      "train loss:1.2080310651692654\n",
      "train loss:1.1401429898283317\n",
      "train loss:0.9841611019622996\n",
      "train loss:1.034093752482911\n",
      "train loss:1.1123599755169689\n",
      "train loss:1.1248941819576908\n",
      "train loss:0.9126229145473165\n",
      "train loss:1.1126457806537375\n",
      "train loss:1.2377852012697481\n",
      "train loss:1.1490515244140398\n",
      "train loss:0.9244701830586165\n",
      "train loss:1.1738343162163476\n",
      "train loss:0.9482298364033581\n",
      "train loss:1.2683375294659376\n",
      "train loss:1.0795463629795952\n",
      "train loss:1.1923577278372377\n",
      "train loss:0.9406548652335524\n",
      "train loss:1.0942924301250077\n",
      "train loss:1.2512101450209614\n",
      "train loss:1.1223609192954793\n",
      "train loss:1.057535332640685\n",
      "train loss:1.1479425998664659\n",
      "train loss:1.0802458025671084\n",
      "train loss:1.041531291067521\n",
      "train loss:1.1977216692749109\n",
      "train loss:1.1187477656888252\n",
      "train loss:1.1509471229292156\n",
      "train loss:1.1338364360802893\n",
      "train loss:1.0921683582270019\n",
      "train loss:1.2198941459702646\n",
      "train loss:0.9597962171911999\n",
      "train loss:0.9367690534999713\n",
      "train loss:0.9906989476491141\n",
      "train loss:1.0826267693490859\n",
      "train loss:1.1271338511308335\n",
      "train loss:1.1915992828668036\n",
      "train loss:1.1341318299118235\n",
      "train loss:1.193334112736181\n",
      "train loss:1.0022687447159564\n",
      "train loss:1.022189013332181\n",
      "train loss:1.1026864493834743\n",
      "=== epoch:2, train acc:0.978, test acc:0.97 ===\n",
      "train loss:1.011794748495538\n",
      "train loss:1.1364597721625145\n",
      "train loss:1.0030211016663868\n",
      "train loss:1.123737908111741\n",
      "train loss:1.0993032662653546\n",
      "train loss:1.0020008371939\n",
      "train loss:1.1382350687671217\n",
      "train loss:1.0132318907840376\n",
      "train loss:1.1831685505219578\n",
      "train loss:1.1670123263509715\n",
      "train loss:1.2936326153964037\n",
      "train loss:1.149685521153004\n",
      "train loss:1.1881333825966418\n",
      "train loss:1.1427113553339812\n",
      "train loss:1.1446221181933447\n",
      "train loss:1.30976309248012\n",
      "train loss:1.1651260196002422\n",
      "train loss:1.1565900165129586\n",
      "train loss:1.1889066594605824\n",
      "train loss:1.1683023547426088\n",
      "train loss:1.1040503880137538\n",
      "train loss:1.3155878533669112\n",
      "train loss:1.3150090928650835\n",
      "train loss:1.0886532665812714\n",
      "train loss:1.0239350739955722\n",
      "train loss:1.011396365460598\n",
      "train loss:1.1891349567021903\n",
      "train loss:1.0810691403702817\n",
      "train loss:1.0658174362791437\n",
      "train loss:1.0259302282580764\n",
      "train loss:1.0060224533960467\n",
      "train loss:0.9476845084384701\n",
      "train loss:1.1081106012257207\n",
      "train loss:0.9764996290802282\n",
      "train loss:1.2893776595410984\n",
      "train loss:1.1526367859489575\n",
      "train loss:1.2564690067713584\n",
      "train loss:1.183334651470669\n",
      "train loss:1.1968405696745656\n",
      "train loss:1.095071305419692\n",
      "train loss:1.1070276805962564\n",
      "train loss:1.1958437202269268\n",
      "train loss:1.0913265419202771\n",
      "train loss:1.3553228984972907\n",
      "train loss:1.2270393993957662\n",
      "train loss:1.1669140452624847\n",
      "train loss:1.2161442815547978\n",
      "train loss:1.030927934753259\n",
      "train loss:1.0422156363281876\n",
      "train loss:1.2036233733806738\n",
      "train loss:1.0517755999067149\n",
      "train loss:1.2804834458710475\n",
      "train loss:0.9992014872091649\n",
      "train loss:1.0918954144607596\n",
      "train loss:1.2438877616386643\n",
      "train loss:1.07943896650844\n",
      "train loss:1.1340570309733324\n",
      "train loss:1.100797826738697\n",
      "train loss:1.2303011505748116\n",
      "train loss:1.041360647737197\n",
      "train loss:1.0515568747978536\n",
      "train loss:1.0420570392210793\n",
      "train loss:0.9601638492857159\n",
      "train loss:0.9896495990148406\n",
      "train loss:1.036824197801469\n",
      "train loss:1.0860735604226783\n",
      "train loss:1.1228657993421118\n",
      "train loss:1.0896152703196067\n",
      "train loss:1.117682311809621\n",
      "train loss:0.9179437256155488\n",
      "train loss:1.2427892096712732\n",
      "train loss:1.0251317269942413\n",
      "train loss:1.1190723080061966\n",
      "train loss:1.2311545086295526\n",
      "train loss:1.1492248023607212\n",
      "train loss:1.0280965133146176\n",
      "train loss:0.8255034942306494\n",
      "train loss:0.9137504161818287\n",
      "train loss:1.0351121507263144\n",
      "train loss:1.1474289470400636\n",
      "train loss:1.06566025762205\n",
      "train loss:1.0752752095194453\n",
      "train loss:1.1337014164566244\n",
      "train loss:0.9826703478715401\n",
      "train loss:1.0549528421248167\n",
      "train loss:1.0158537752311356\n",
      "train loss:1.068954557904791\n",
      "train loss:1.11131899602231\n",
      "train loss:0.9835417250958711\n",
      "train loss:1.3180590199285158\n",
      "train loss:1.2724728985186602\n",
      "train loss:1.1640639271181603\n",
      "train loss:1.026211977998842\n",
      "train loss:1.1668262577027737\n",
      "train loss:1.0932488897838377\n",
      "train loss:1.1007881335672014\n",
      "train loss:1.2171937778408843\n",
      "train loss:1.0691633152723923\n",
      "train loss:1.0633312689367125\n",
      "train loss:1.1253961649016535\n",
      "train loss:1.1037004294861739\n",
      "train loss:1.2128036722136617\n",
      "train loss:1.1790106391829078\n",
      "train loss:1.0783691721444064\n",
      "train loss:1.0086851925831721\n",
      "train loss:1.102035883158433\n",
      "train loss:1.0827567610249553\n",
      "train loss:1.169933330958698\n",
      "train loss:1.0895217858169763\n",
      "train loss:1.074976895343249\n",
      "train loss:1.2218629484056778\n",
      "train loss:1.001209548070238\n",
      "train loss:0.8772120872547974\n",
      "train loss:1.0869062966520593\n",
      "train loss:1.0923392661048839\n",
      "train loss:1.0571531170274862\n",
      "train loss:0.9623040668724014\n",
      "train loss:1.0571030669429498\n",
      "train loss:1.1727603806593412\n",
      "train loss:0.962494721641332\n",
      "train loss:0.9905693787202096\n",
      "train loss:0.9213375894992956\n",
      "train loss:1.0302814291718714\n",
      "train loss:1.125107476302808\n",
      "train loss:1.187842670831626\n",
      "train loss:1.0458271490740665\n",
      "train loss:1.0037528603675738\n",
      "train loss:1.173665139265226\n",
      "train loss:1.0070288845606388\n",
      "train loss:0.9290019560536946\n",
      "train loss:1.0567186840605258\n",
      "train loss:1.0893747133662988\n",
      "train loss:1.0368834818216657\n",
      "train loss:1.4328190060713775\n",
      "train loss:0.9903740115359486\n",
      "train loss:0.9448106822276701\n",
      "train loss:0.9972681033200699\n",
      "train loss:0.9967116906575616\n",
      "train loss:0.9488351734636338\n",
      "train loss:0.9783871639359002\n",
      "train loss:1.0944688800715006\n",
      "train loss:0.9452987570561996\n",
      "train loss:1.0384981584297721\n",
      "train loss:1.0830348481919378\n",
      "train loss:1.092257299486918\n",
      "train loss:1.0210250366027673\n",
      "train loss:1.092654841015736\n",
      "train loss:1.079563655487666\n",
      "train loss:1.0310189881453926\n",
      "train loss:0.9202116771775413\n",
      "train loss:0.9113625981197215\n",
      "train loss:1.1897988967542839\n",
      "train loss:1.0719448975776038\n",
      "train loss:1.213249114250192\n",
      "train loss:0.9175081586488868\n",
      "train loss:1.0317799237352938\n",
      "train loss:1.227569367582161\n",
      "train loss:1.2657844271884868\n",
      "train loss:1.0255531447409307\n",
      "train loss:1.2498384416501451\n",
      "train loss:1.1543827363890855\n",
      "train loss:1.2671450373977462\n",
      "train loss:1.185792518177168\n",
      "train loss:1.1458787142119709\n",
      "train loss:1.0986074003042767\n",
      "train loss:1.1633856057728114\n",
      "train loss:1.054410535148351\n",
      "train loss:1.2473082431778222\n",
      "train loss:1.0869745256695582\n",
      "train loss:1.1202986077344415\n",
      "train loss:0.9977258918014813\n",
      "train loss:1.1390066776696242\n",
      "train loss:1.092243572440194\n",
      "train loss:1.007017832801459\n",
      "train loss:1.0171955885912147\n",
      "train loss:0.9198427359793251\n",
      "train loss:1.0479159200929793\n",
      "train loss:1.0340059823696572\n",
      "train loss:0.9988924383229271\n",
      "train loss:0.9956762724217073\n",
      "train loss:0.942953863528075\n",
      "train loss:0.9412398485186284\n",
      "train loss:1.3482134010888154\n",
      "train loss:1.1429844452309388\n",
      "train loss:1.0179732575182958\n",
      "train loss:1.1800207349726302\n",
      "train loss:1.0288787018523342\n",
      "train loss:0.8996403871871942\n",
      "train loss:0.9686615538691851\n",
      "train loss:1.073649318800051\n",
      "train loss:0.9060713249911712\n",
      "train loss:1.2480324904647522\n",
      "train loss:1.0166844799389372\n",
      "train loss:1.115601968857665\n",
      "train loss:1.036300271017401\n",
      "train loss:1.2016320325868037\n",
      "train loss:1.0673517818751266\n",
      "train loss:1.050102152335061\n",
      "train loss:1.0412517875180385\n",
      "train loss:0.9295778949662796\n",
      "train loss:1.208021012504353\n",
      "train loss:1.0819541987841192\n",
      "train loss:0.8865257170050915\n",
      "train loss:1.1499076287647203\n",
      "train loss:1.2396561937091615\n",
      "train loss:1.2309787827295962\n",
      "train loss:0.9669106967804809\n",
      "train loss:0.8977192508159956\n",
      "train loss:1.0632217078910349\n",
      "train loss:0.8639118535056389\n",
      "train loss:1.055951559484769\n",
      "train loss:1.1128905780759308\n",
      "train loss:1.2130100780138333\n",
      "train loss:0.9607052868027729\n",
      "train loss:0.9984729688457861\n",
      "train loss:1.0615239503261713\n",
      "train loss:0.9941404414889837\n",
      "train loss:0.9998461456107038\n",
      "train loss:1.1396830035757015\n",
      "train loss:0.9300213559728572\n",
      "train loss:1.0228090167911574\n",
      "train loss:1.1188975636001006\n",
      "train loss:1.107940006758116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0877541622617546\n",
      "train loss:0.948107000735244\n",
      "train loss:1.0438941731398643\n",
      "train loss:1.2701735906122076\n",
      "train loss:1.0850044024514482\n",
      "train loss:1.0738142327190983\n",
      "train loss:1.0385941536217036\n",
      "train loss:1.1158798931846783\n",
      "train loss:1.2701136589779327\n",
      "train loss:1.0331536183071248\n",
      "train loss:1.0863601726003562\n",
      "train loss:0.9583512685479135\n",
      "train loss:0.9643912085552881\n",
      "train loss:1.287829106315935\n",
      "train loss:0.9297076717618374\n",
      "train loss:1.0993071997371837\n",
      "train loss:1.1610180958478107\n",
      "train loss:1.0050167119969773\n",
      "train loss:0.9940926814564759\n",
      "train loss:0.9976889617703018\n",
      "train loss:1.033247209254894\n",
      "train loss:1.1714551610249515\n",
      "train loss:1.0477027680524897\n",
      "train loss:1.1735614012394457\n",
      "train loss:1.149315119328651\n",
      "train loss:1.1603757300847455\n",
      "train loss:1.2510902533429107\n",
      "train loss:0.9832801927026117\n",
      "train loss:1.0429695100755436\n",
      "train loss:1.0829547809377704\n",
      "train loss:1.2012942760475371\n",
      "train loss:1.0389441087074227\n",
      "train loss:1.030100460118141\n",
      "train loss:0.9431151703200266\n",
      "train loss:1.1943153614661968\n",
      "train loss:1.066413917509164\n",
      "train loss:1.024415118388037\n",
      "train loss:1.1164170791342078\n",
      "train loss:1.024606256418967\n",
      "train loss:1.1027285563900682\n",
      "train loss:0.8842533494197895\n",
      "train loss:1.0237303156723774\n",
      "train loss:1.0563946285622583\n",
      "train loss:1.0103250730930893\n",
      "train loss:1.1297727012749539\n",
      "train loss:0.9217137182788308\n",
      "train loss:1.1669859867721817\n",
      "train loss:1.1169776592117977\n",
      "train loss:1.0789895286742281\n",
      "train loss:1.1534783210726096\n",
      "train loss:0.9625443795675409\n",
      "train loss:0.9099890156067174\n",
      "train loss:1.2008554473888016\n",
      "train loss:0.8997234119711333\n",
      "train loss:1.1041004145157998\n",
      "train loss:0.8735713637247674\n",
      "train loss:1.16776282410033\n",
      "train loss:1.0094238403788973\n",
      "train loss:0.9761022391589387\n",
      "train loss:0.9780199760280133\n",
      "train loss:1.0940725316087418\n",
      "train loss:1.1604211576099983\n",
      "train loss:0.9179366000003648\n",
      "train loss:1.0522540589366656\n",
      "train loss:0.9953318140784799\n",
      "train loss:1.2461275196465784\n",
      "train loss:1.0833715757703557\n",
      "train loss:1.0457401069034575\n",
      "train loss:1.0098347024030914\n",
      "train loss:1.0693853927275274\n",
      "train loss:0.9453939237934945\n",
      "train loss:1.0330278974710538\n",
      "train loss:0.9722116564359733\n",
      "train loss:1.0179074913621309\n",
      "train loss:1.0004345805629504\n",
      "train loss:1.3402530743978016\n",
      "train loss:1.2010283152697239\n",
      "train loss:1.1667961267542362\n",
      "train loss:1.027158816342125\n",
      "train loss:0.9879590463578761\n",
      "train loss:1.1075566843425297\n",
      "train loss:1.0197379647313272\n",
      "train loss:0.9882042276361244\n",
      "train loss:1.057281462166069\n",
      "train loss:1.1697971211456057\n",
      "train loss:1.100652237466226\n",
      "train loss:1.1381043237005688\n",
      "train loss:1.241990340263741\n",
      "train loss:1.0733395300786628\n",
      "train loss:1.0898886940926142\n",
      "train loss:1.043574356628702\n",
      "train loss:1.0573366965271713\n",
      "train loss:1.2925688550341297\n",
      "train loss:1.0658104561863968\n",
      "train loss:0.9600742298291212\n",
      "train loss:0.8774218785244022\n",
      "train loss:1.0004139888849928\n",
      "train loss:0.9254844878994732\n",
      "train loss:0.974863065518599\n",
      "train loss:1.1130011677069174\n",
      "train loss:0.9619256269421479\n",
      "train loss:0.9070206941511384\n",
      "train loss:1.1022853465365758\n",
      "train loss:0.9834956608012856\n",
      "train loss:0.9347181439879317\n",
      "train loss:1.0000670548659472\n",
      "train loss:0.9838103076779109\n",
      "train loss:1.0602599819334197\n",
      "train loss:1.1908590263332888\n",
      "train loss:0.8138333744625998\n",
      "train loss:0.9902631495061581\n",
      "train loss:0.9575466530318613\n",
      "train loss:1.139420794084174\n",
      "train loss:1.0432466573211192\n",
      "train loss:0.9037995234993197\n",
      "train loss:1.1321863018191132\n",
      "train loss:0.8748995690977384\n",
      "train loss:0.9263292525979644\n",
      "train loss:0.9470652312480098\n",
      "train loss:0.8675225019133194\n",
      "train loss:1.1592815165849826\n",
      "train loss:1.1152938961271375\n",
      "train loss:0.8827482299821642\n",
      "train loss:0.9275023131104244\n",
      "train loss:1.1009233836522754\n",
      "train loss:1.1026612410796455\n",
      "train loss:0.9979521451974973\n",
      "train loss:0.9944454103119951\n",
      "train loss:0.9777126828646162\n",
      "train loss:0.8566535261476272\n",
      "train loss:1.0878338309632478\n",
      "train loss:0.9785880551136796\n",
      "train loss:1.072466861776438\n",
      "train loss:0.9396168331917862\n",
      "train loss:1.0465954832293731\n",
      "train loss:1.0858497992248262\n",
      "train loss:0.9548650907767106\n",
      "train loss:1.0744928855137217\n",
      "train loss:1.1598010032757768\n",
      "train loss:1.1438846857296388\n",
      "train loss:1.0709714996983595\n",
      "train loss:1.0324380121209007\n",
      "train loss:1.1382818647783155\n",
      "train loss:1.1464128918294985\n",
      "train loss:0.9933136201459081\n",
      "train loss:1.1565254178264412\n",
      "train loss:1.058890947922572\n",
      "train loss:1.1439387317523388\n",
      "train loss:0.9484951016472324\n",
      "train loss:1.178083437956821\n",
      "train loss:1.1237477894161443\n",
      "train loss:1.2318729759293638\n",
      "train loss:1.147691960777959\n",
      "train loss:1.0968460914556826\n",
      "train loss:0.9913522585886781\n",
      "train loss:1.0959962676015342\n",
      "train loss:1.0089912848112854\n",
      "train loss:1.1158226331265546\n",
      "train loss:1.2268538129235607\n",
      "train loss:0.9760823320147023\n",
      "train loss:0.9643058447935551\n",
      "train loss:0.9178238261250021\n",
      "train loss:0.9149312767724834\n",
      "train loss:0.9121525369713029\n",
      "train loss:0.9648632882348371\n",
      "train loss:1.0889877768232568\n",
      "train loss:1.1576133887623086\n",
      "train loss:0.8917687754322511\n",
      "train loss:0.9744417197768693\n",
      "train loss:1.0026825792168348\n",
      "train loss:0.8879554257885681\n",
      "train loss:0.9970650875655438\n",
      "train loss:0.9626640714713305\n",
      "train loss:0.894825524778218\n",
      "train loss:0.8327222628513234\n",
      "train loss:1.0935964466865242\n",
      "train loss:1.1705928473911384\n",
      "train loss:1.0528477115725574\n",
      "train loss:1.0495171683577305\n",
      "train loss:1.135690291341644\n",
      "train loss:1.0655672616580218\n",
      "train loss:1.1368671192251847\n",
      "train loss:1.083387186718532\n",
      "train loss:0.992673510658522\n",
      "train loss:0.840409198946841\n",
      "train loss:1.0717199817787766\n",
      "train loss:0.9972968109182591\n",
      "train loss:0.8946143223707311\n",
      "train loss:1.0903365465104176\n",
      "train loss:0.9359499989074734\n",
      "train loss:0.9813863647897085\n",
      "train loss:1.0826499930262505\n",
      "train loss:0.9373517466879798\n",
      "train loss:1.013835396089638\n",
      "train loss:1.1760749186106347\n",
      "train loss:0.9841640067307535\n",
      "train loss:1.0679974452587644\n",
      "train loss:0.9256138450596461\n",
      "train loss:1.1672258597291825\n",
      "train loss:1.0667568625266752\n",
      "train loss:0.888107744937153\n",
      "train loss:1.004756075438607\n",
      "train loss:1.0732081845896695\n",
      "train loss:0.9498749890133534\n",
      "train loss:0.9428405682001815\n",
      "train loss:0.9798507925280531\n",
      "train loss:0.9114073770125434\n",
      "train loss:0.991641044772097\n",
      "train loss:1.0759600909445926\n",
      "train loss:1.177972517617668\n",
      "train loss:1.010642819242275\n",
      "train loss:1.1569714787803402\n",
      "train loss:0.9894335009859083\n",
      "train loss:0.9870829926397642\n",
      "train loss:0.94309358401303\n",
      "train loss:0.9455325035844324\n",
      "train loss:0.8892857036485841\n",
      "train loss:1.0290751968281655\n",
      "train loss:0.8910138499875272\n",
      "train loss:0.9970409113968586\n",
      "train loss:1.02904754070708\n",
      "train loss:0.9746326304612843\n",
      "train loss:0.8840128626288688\n",
      "train loss:0.9825063292017713\n",
      "train loss:1.1089583585993783\n",
      "train loss:0.9138325055915979\n",
      "train loss:1.1226583328860225\n",
      "train loss:1.13156770396621\n",
      "train loss:1.0827637609737826\n",
      "train loss:1.0083230141007677\n",
      "train loss:0.9259608315689326\n",
      "train loss:0.8530062020950747\n",
      "train loss:0.8278290129084563\n",
      "train loss:1.0751592847916926\n",
      "train loss:1.049117126611267\n",
      "train loss:0.9968421969515152\n",
      "train loss:1.0650726796640817\n",
      "train loss:1.10460820401789\n",
      "train loss:1.142002603715859\n",
      "train loss:1.0611663643194205\n",
      "train loss:0.873329134349047\n",
      "train loss:0.9058455483524458\n",
      "train loss:1.044231645355772\n",
      "train loss:1.1137835716565478\n",
      "train loss:1.0504574888551743\n",
      "train loss:0.8112532407286172\n",
      "train loss:0.9189647579868898\n",
      "train loss:1.0964900899923204\n",
      "train loss:0.9501008294533047\n",
      "train loss:0.9236297110550222\n",
      "train loss:1.0099269332565768\n",
      "train loss:1.000313536364064\n",
      "train loss:1.0798356197772674\n",
      "train loss:0.9505681844375405\n",
      "train loss:1.0289741782638502\n",
      "train loss:0.9840387920860972\n",
      "train loss:1.0222647884624485\n",
      "train loss:0.9811553975478722\n",
      "train loss:0.8361345335832194\n",
      "train loss:1.1376496358974724\n",
      "train loss:1.1471812875852168\n",
      "train loss:1.0977115678288858\n",
      "train loss:1.0262412264895453\n",
      "train loss:0.8765587591377083\n",
      "train loss:0.9836797252336053\n",
      "train loss:1.0895872077313675\n",
      "train loss:0.9484634399745556\n",
      "train loss:1.0726486703470552\n",
      "train loss:0.9763376522822569\n",
      "train loss:0.9631190693852015\n",
      "train loss:1.112544207585718\n",
      "train loss:0.9192949845974846\n",
      "train loss:1.0527143058113966\n",
      "train loss:1.097937004542307\n",
      "train loss:0.9701912220240705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9913842672126332\n",
      "train loss:1.067975143694255\n",
      "train loss:1.025562878562088\n",
      "train loss:0.9496886381233597\n",
      "train loss:0.9018434369800937\n",
      "train loss:0.9315719938646967\n",
      "train loss:1.0174783184575107\n",
      "train loss:0.9283473637735713\n",
      "train loss:0.9973117023984319\n",
      "train loss:1.134120544628933\n",
      "train loss:1.0378766454189943\n",
      "train loss:1.0200139007496896\n",
      "train loss:1.12956728815769\n",
      "train loss:1.0666241312445102\n",
      "train loss:0.93191580562943\n",
      "train loss:0.866753744520761\n",
      "train loss:0.9994174819120919\n",
      "train loss:1.143478202751445\n",
      "train loss:0.9490651881857107\n",
      "train loss:0.9762836152880383\n",
      "train loss:0.9406268708644512\n",
      "train loss:0.9297696257276459\n",
      "train loss:1.011668555802167\n",
      "train loss:1.1612524988745825\n",
      "train loss:1.1154206637486843\n",
      "train loss:0.909738764572213\n",
      "train loss:0.9091867672968212\n",
      "train loss:1.0085260552061694\n",
      "train loss:0.9342994827331143\n",
      "train loss:0.9134782955116154\n",
      "train loss:0.9615393554973309\n",
      "train loss:0.9609853937271992\n",
      "train loss:1.1250995142295919\n",
      "train loss:1.0189063870416666\n",
      "train loss:0.8512670409280936\n",
      "train loss:1.0109731237051411\n",
      "train loss:0.783123966175678\n",
      "train loss:0.9342296716880008\n",
      "train loss:1.286997220544339\n",
      "train loss:1.1045600646076565\n",
      "train loss:0.8308475654748735\n",
      "train loss:0.9887959122042919\n",
      "train loss:1.0461634217815863\n",
      "train loss:0.9600707772792464\n",
      "train loss:0.984870372140712\n",
      "train loss:0.853493517829479\n",
      "train loss:1.1627001011118652\n",
      "train loss:0.8413515170659589\n",
      "train loss:0.9065539445783994\n",
      "train loss:1.0471486378934596\n",
      "train loss:1.1776630289626833\n",
      "train loss:0.8308007471882084\n",
      "train loss:1.0765390791455969\n",
      "train loss:1.0241178190971472\n",
      "train loss:1.0406425401542565\n",
      "train loss:1.0743359006784277\n",
      "train loss:0.9444043207212787\n",
      "train loss:1.1735030426472608\n",
      "train loss:1.0129156433698583\n",
      "train loss:1.006080527228145\n",
      "train loss:1.1192374722705243\n",
      "train loss:0.8693984717253186\n",
      "train loss:1.0554131329708845\n",
      "train loss:1.0103871774852728\n",
      "train loss:1.2488124484942\n",
      "train loss:0.8682685420895094\n",
      "train loss:0.9105100414275742\n",
      "train loss:1.0585368455513444\n",
      "train loss:1.167814323107264\n",
      "train loss:1.1114129105551889\n",
      "train loss:1.129363351250561\n",
      "train loss:0.9906839730235774\n",
      "train loss:1.183484688936808\n",
      "train loss:1.0186130407217695\n",
      "train loss:0.9118805877487445\n",
      "train loss:1.057414180616483\n",
      "train loss:0.9208365983659634\n",
      "train loss:0.8956128394660577\n",
      "train loss:0.9282786660271598\n",
      "train loss:0.8494128467464469\n",
      "train loss:0.9138769293672823\n",
      "train loss:0.9248249362580109\n",
      "train loss:1.0904951072091564\n",
      "train loss:1.0672474437897492\n",
      "train loss:0.967054129415196\n",
      "train loss:0.9738286776498722\n",
      "train loss:0.8518269023564483\n",
      "train loss:1.0301778745792647\n",
      "train loss:0.8460222448538789\n",
      "train loss:0.9919577119928122\n",
      "train loss:0.8928210250892055\n",
      "train loss:0.9520322020121048\n",
      "train loss:0.9633189180126079\n",
      "train loss:1.1707789845955145\n",
      "train loss:0.9335003424750984\n",
      "train loss:1.020023434934068\n",
      "train loss:1.0582774945816515\n",
      "train loss:0.9565796070924706\n",
      "train loss:1.086533007609902\n",
      "train loss:0.941473932785103\n",
      "train loss:1.0832781181679565\n",
      "train loss:0.9443984452732476\n",
      "=== epoch:3, train acc:0.983, test acc:0.982 ===\n",
      "train loss:0.9759106682987131\n",
      "train loss:0.8895641972047456\n",
      "train loss:1.0785964491066953\n",
      "train loss:0.9632656768242751\n",
      "train loss:1.0450450100017492\n",
      "train loss:1.0683851435610883\n",
      "train loss:0.9133440023216004\n",
      "train loss:0.9993430233143323\n",
      "train loss:0.9722395404158527\n",
      "train loss:0.8884066880284497\n",
      "train loss:1.0382041301894562\n",
      "train loss:0.8500764803735883\n",
      "train loss:1.0241336620704105\n",
      "train loss:1.0602094914311306\n",
      "train loss:0.9503962255731238\n",
      "train loss:0.9971642203048594\n",
      "train loss:1.0143928076421422\n",
      "train loss:0.880740896297195\n",
      "train loss:0.8882332043171561\n",
      "train loss:0.8651094563809231\n",
      "train loss:1.022769981804852\n",
      "train loss:0.9032143349883864\n",
      "train loss:1.014853915308956\n",
      "train loss:0.8647274192301032\n",
      "train loss:0.9581483016687282\n",
      "train loss:1.013696910941038\n",
      "train loss:1.1725236198200228\n",
      "train loss:1.041567333857566\n",
      "train loss:1.0101220697686495\n",
      "train loss:0.8876173426767532\n",
      "train loss:0.9229738278582635\n",
      "train loss:1.0534796565917812\n",
      "train loss:1.1122301175209548\n",
      "train loss:0.8907523722940579\n",
      "train loss:0.9355176076717643\n",
      "train loss:0.7393541544466806\n",
      "train loss:0.8728010694339904\n",
      "train loss:1.0410043654694874\n",
      "train loss:0.9329978848650774\n",
      "train loss:1.1134913102046748\n",
      "train loss:1.0268782080586942\n",
      "train loss:1.1522830569133462\n",
      "train loss:0.9131982578834021\n",
      "train loss:1.062040983981309\n",
      "train loss:1.060311342152769\n",
      "train loss:1.0071413706076793\n",
      "train loss:1.0692268841444852\n",
      "train loss:1.0331240867883513\n",
      "train loss:0.8556527972206999\n",
      "train loss:1.0104886115373581\n",
      "train loss:0.8453743383347994\n",
      "train loss:1.1206699751227303\n",
      "train loss:1.14133603704068\n",
      "train loss:1.0436425725751899\n",
      "train loss:0.8862246287372595\n",
      "train loss:0.925592729274186\n",
      "train loss:1.0005883406688023\n",
      "train loss:0.9379576724203598\n",
      "train loss:1.048574885322156\n",
      "train loss:0.9272198894379474\n",
      "train loss:0.9350842978716846\n",
      "train loss:1.0546160635703854\n",
      "train loss:0.8934254460843328\n",
      "train loss:1.0097563221986392\n",
      "train loss:0.9219625540788197\n",
      "train loss:0.8914769501226981\n",
      "train loss:0.8508142926558538\n",
      "train loss:0.9512438536717428\n",
      "train loss:0.9892005104444785\n",
      "train loss:1.0527849577986406\n",
      "train loss:0.8658681968308644\n",
      "train loss:0.9513491866525351\n",
      "train loss:1.0689429522149063\n",
      "train loss:0.9383796927862196\n",
      "train loss:1.000140327429166\n",
      "train loss:1.0223756364330008\n",
      "train loss:1.1360413445471391\n",
      "train loss:0.9112010905695673\n",
      "train loss:0.8929346430541202\n",
      "train loss:0.868774489056248\n",
      "train loss:0.8903509924951174\n",
      "train loss:1.0480726626399741\n",
      "train loss:0.800859665624845\n",
      "train loss:1.0101597659519028\n",
      "train loss:0.9430796205776261\n",
      "train loss:0.9720631085279358\n",
      "train loss:0.960803501912936\n",
      "train loss:0.911302340232275\n",
      "train loss:0.7553458474330184\n",
      "train loss:1.1401031167038929\n",
      "train loss:0.9608585329505562\n",
      "train loss:0.9886871830593236\n",
      "train loss:0.9555447660237015\n",
      "train loss:0.9969597853352694\n",
      "train loss:0.7939281834194216\n",
      "train loss:0.9230926127733788\n",
      "train loss:0.9808087265132963\n",
      "train loss:1.0270498180998688\n",
      "train loss:0.8729029890201536\n",
      "train loss:1.0603106380363114\n",
      "train loss:1.1100556167378164\n",
      "train loss:0.8835784743975243\n",
      "train loss:0.962349846790683\n",
      "train loss:0.9360607895102079\n",
      "train loss:0.9653667374477425\n",
      "train loss:0.8895933985004121\n",
      "train loss:0.9977070528354598\n",
      "train loss:0.9340538959202376\n",
      "train loss:0.8373637600933209\n",
      "train loss:0.8582039370648771\n",
      "train loss:0.9450445793430253\n",
      "train loss:0.9143702892102654\n",
      "train loss:0.9812715399718109\n",
      "train loss:1.021496750913556\n",
      "train loss:1.0017942274725522\n",
      "train loss:1.0363711719830628\n",
      "train loss:1.096105342861776\n",
      "train loss:1.1005870967854419\n",
      "train loss:0.9409396154972643\n",
      "train loss:0.8467584936350186\n",
      "train loss:0.9142382939302263\n",
      "train loss:1.1130963199726072\n",
      "train loss:1.0502942494519945\n",
      "train loss:1.0314340363426135\n",
      "train loss:0.9551185226082698\n",
      "train loss:0.9841435220957682\n",
      "train loss:0.8791478960639689\n",
      "train loss:0.9532388828596166\n",
      "train loss:0.9676832476788345\n",
      "train loss:1.2008289760662718\n",
      "train loss:0.9512223890676273\n",
      "train loss:1.179205332652151\n",
      "train loss:1.0835113189962498\n",
      "train loss:0.8356054095409121\n",
      "train loss:0.813121603217499\n",
      "train loss:0.6754756103922135\n",
      "train loss:1.0294619927882065\n",
      "train loss:0.9206082689460735\n",
      "train loss:1.1183386984267785\n",
      "train loss:0.8987958214081503\n",
      "train loss:1.0949111588716753\n",
      "train loss:1.134443301521997\n",
      "train loss:0.8923381953807042\n",
      "train loss:0.964154990912739\n",
      "train loss:1.0165693900846062\n",
      "train loss:1.0171292478895941\n",
      "train loss:0.9603949415554567\n",
      "train loss:0.9615054404660484\n",
      "train loss:0.8426323019095787\n",
      "train loss:1.0026097993589627\n",
      "train loss:0.9896282172378421\n",
      "train loss:0.8324214133066227\n",
      "train loss:1.0827677948582586\n",
      "train loss:0.9200904559126347\n",
      "train loss:1.0129643355489062\n",
      "train loss:0.9165094531788395\n",
      "train loss:1.0744499622183556\n",
      "train loss:0.9877762741450127\n",
      "train loss:1.0913800257939796\n",
      "train loss:0.9118574982703141\n",
      "train loss:1.0066572700524263\n",
      "train loss:0.9951388806934834\n",
      "train loss:0.839577937694242\n",
      "train loss:0.8990744498130373\n",
      "train loss:1.1060018664397193\n",
      "train loss:1.0601238359119172\n",
      "train loss:0.9843977003407481\n",
      "train loss:1.0623818884488756\n",
      "train loss:0.990301892161235\n",
      "train loss:0.8957871783980312\n",
      "train loss:0.9452180988320387\n",
      "train loss:0.9858549165460079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9034054488005141\n",
      "train loss:1.0089659581869261\n",
      "train loss:1.0663809395085533\n",
      "train loss:0.9365282490393173\n",
      "train loss:1.1682296160226087\n",
      "train loss:0.7902443228586726\n",
      "train loss:0.8241178267298891\n",
      "train loss:1.1528661713244936\n",
      "train loss:1.0387251350589073\n",
      "train loss:0.9632154965706283\n",
      "train loss:0.9818065202829618\n",
      "train loss:0.9522106507256248\n",
      "train loss:0.917956203087507\n",
      "train loss:0.9432336737154934\n",
      "train loss:1.2843923355912934\n",
      "train loss:1.1386470093644618\n",
      "train loss:0.9610624122543454\n",
      "train loss:0.9177030147823867\n",
      "train loss:1.0757131789241208\n",
      "train loss:0.8180484321061844\n",
      "train loss:0.9644415420227075\n",
      "train loss:1.03721466846238\n",
      "train loss:0.9498937475467768\n",
      "train loss:1.0369517272519686\n",
      "train loss:0.9945250658399678\n",
      "train loss:0.78632394636013\n",
      "train loss:0.929593368850105\n",
      "train loss:0.9820520426294209\n",
      "train loss:1.0239613453949203\n",
      "train loss:0.9108683463036726\n",
      "train loss:0.8257481548436018\n",
      "train loss:0.9812336733651411\n",
      "train loss:0.8668813543293084\n",
      "train loss:1.0117263246786556\n",
      "train loss:0.9614335550779808\n",
      "train loss:1.1180355813075504\n",
      "train loss:0.9435337927279314\n",
      "train loss:0.9981004465632249\n",
      "train loss:0.9076462546090113\n",
      "train loss:0.8958468698414951\n",
      "train loss:0.9845912181237608\n",
      "train loss:1.0714946842150936\n",
      "train loss:0.9241803840318515\n",
      "train loss:0.9536338807375496\n",
      "train loss:1.0037054981111146\n",
      "train loss:1.0049529742604784\n",
      "train loss:1.0225929764436497\n",
      "train loss:0.8451299792866536\n",
      "train loss:0.9021077716997561\n",
      "train loss:1.0285503883276885\n",
      "train loss:0.8045293703728666\n",
      "train loss:0.9450014827171003\n",
      "train loss:1.082094744110741\n",
      "train loss:0.9043165263050409\n",
      "train loss:1.0416378882651243\n",
      "train loss:1.0885379459130116\n",
      "train loss:0.8668178485559356\n",
      "train loss:1.0378857363955558\n",
      "train loss:0.9693434901827673\n",
      "train loss:0.9487458489057518\n",
      "train loss:0.919546053982032\n",
      "train loss:0.9746208486100081\n",
      "train loss:0.6910341682913418\n",
      "train loss:1.069182791618308\n",
      "train loss:1.2281911414583704\n",
      "train loss:0.8858546641380137\n",
      "train loss:0.8788373989007627\n",
      "train loss:0.8581753838467702\n",
      "train loss:0.8590413608491306\n",
      "train loss:1.0743621807436643\n",
      "train loss:0.9064566365600574\n",
      "train loss:0.8572173686520821\n",
      "train loss:0.8220634763674023\n",
      "train loss:0.8411761260397767\n",
      "train loss:0.9490197055425328\n",
      "train loss:0.990691030361912\n",
      "train loss:0.9885270378489444\n",
      "train loss:0.8825957202492525\n",
      "train loss:0.8343058944169295\n",
      "train loss:0.9205980426448069\n",
      "train loss:0.8429637169344093\n",
      "train loss:1.0637286309207967\n",
      "train loss:0.8456707562189737\n",
      "train loss:0.9386333600387516\n",
      "train loss:0.9243246585662649\n",
      "train loss:1.0172231021632434\n",
      "train loss:0.9795342694569836\n",
      "train loss:0.9382182976889485\n",
      "train loss:0.9155179757696732\n",
      "train loss:0.8676373463966062\n",
      "train loss:0.9964452120056848\n",
      "train loss:1.0055594520550812\n",
      "train loss:0.9894466395116609\n",
      "train loss:0.9688836064105473\n",
      "train loss:0.9367480196478136\n",
      "train loss:1.1020701277919365\n",
      "train loss:0.9070404208953411\n",
      "train loss:1.0328624445224492\n",
      "train loss:0.9807909442881767\n",
      "train loss:1.0554373229307634\n",
      "train loss:0.9554816247573303\n",
      "train loss:1.0202424998200121\n",
      "train loss:1.0080236603807338\n",
      "train loss:0.9720552968760724\n",
      "train loss:0.9863111412659633\n",
      "train loss:0.9888360559823701\n",
      "train loss:0.9516851135222061\n",
      "train loss:0.9718537158343307\n",
      "train loss:0.8993587781585319\n",
      "train loss:0.9791615779235342\n",
      "train loss:1.1027534288861125\n",
      "train loss:1.0262314549287836\n",
      "train loss:0.9337738612043123\n",
      "train loss:0.6643842309398122\n",
      "train loss:0.8244720976368743\n",
      "train loss:0.9786064492218374\n",
      "train loss:0.8630387703552114\n",
      "train loss:1.053425345926994\n",
      "train loss:0.8077023002105908\n",
      "train loss:1.1322476995401025\n",
      "train loss:0.8839426888101201\n",
      "train loss:1.0031241596426015\n",
      "train loss:1.1301046462737456\n",
      "train loss:0.8361692077512393\n",
      "train loss:0.9545322365311221\n",
      "train loss:0.9830790862836319\n",
      "train loss:0.9720474759057006\n",
      "train loss:0.9433850879956526\n",
      "train loss:0.9182612529898792\n",
      "train loss:0.8905734094142439\n",
      "train loss:1.0163906468977535\n",
      "train loss:0.9649057424075536\n",
      "train loss:0.9890665944636591\n",
      "train loss:1.0232881803614067\n",
      "train loss:0.7776630017989598\n",
      "train loss:0.9386166173648002\n",
      "train loss:1.0280540869989288\n",
      "train loss:0.9665357095458479\n",
      "train loss:0.9172570256717806\n",
      "train loss:0.929127944243998\n",
      "train loss:0.9977974119644897\n",
      "train loss:0.9633694672916399\n",
      "train loss:0.8520548082760105\n",
      "train loss:1.0960689473935958\n",
      "train loss:1.1007114064367025\n",
      "train loss:1.0685416198021835\n",
      "train loss:0.8280255822164403\n",
      "train loss:0.8821955569127522\n",
      "train loss:0.8058513421814629\n",
      "train loss:1.0091287882234672\n",
      "train loss:0.7719868337559425\n",
      "train loss:0.7783317642490151\n",
      "train loss:1.0032620158609153\n",
      "train loss:0.8790049579139976\n",
      "train loss:1.0443458630628455\n",
      "train loss:0.9287467038364814\n",
      "train loss:0.8466298770093306\n",
      "train loss:0.9228109711976024\n",
      "train loss:0.9432123456897779\n",
      "train loss:0.838783817711144\n",
      "train loss:0.9098478550440448\n",
      "train loss:0.7107050651382759\n",
      "train loss:1.0974698679391037\n",
      "train loss:0.8275660488420727\n",
      "train loss:0.9094475431834143\n",
      "train loss:0.8883548782776005\n",
      "train loss:0.9726129097521974\n",
      "train loss:0.9638381284426852\n",
      "train loss:0.9380770929833937\n",
      "train loss:1.054886232602763\n",
      "train loss:0.9035342092328613\n",
      "train loss:1.0820070293338682\n",
      "train loss:0.9486743092385851\n",
      "train loss:0.926077381643541\n",
      "train loss:1.0104376296173372\n",
      "train loss:0.909391532547218\n",
      "train loss:0.9816784997002626\n",
      "train loss:0.9192888293974241\n",
      "train loss:1.078474760664468\n",
      "train loss:1.0589827482996328\n",
      "train loss:0.9418209155436094\n",
      "train loss:0.9031045958841436\n",
      "train loss:1.1765046238016001\n",
      "train loss:1.0487923741282563\n",
      "train loss:0.87198212441978\n",
      "train loss:0.8760064173430625\n",
      "train loss:0.9224388797622676\n",
      "train loss:0.9818236275040197\n",
      "train loss:0.9296698333229374\n",
      "train loss:1.0747544386420842\n",
      "train loss:0.9880915310483596\n",
      "train loss:1.0424165323000842\n",
      "train loss:1.1020766634547017\n",
      "train loss:0.9292365057402828\n",
      "train loss:0.8834277008742076\n",
      "train loss:0.8112398829974208\n",
      "train loss:0.9070817410044556\n",
      "train loss:0.9059033835278193\n",
      "train loss:0.9529021222896694\n",
      "train loss:1.0345082982690579\n",
      "train loss:1.0444922451303509\n",
      "train loss:0.9753158569497841\n",
      "train loss:1.0479743569024356\n",
      "train loss:0.946608666394407\n",
      "train loss:0.9657454007203955\n",
      "train loss:0.9734927113528591\n",
      "train loss:0.9054146214116865\n",
      "train loss:0.9121759916724909\n",
      "train loss:0.875228194767393\n",
      "train loss:1.0060033184352932\n",
      "train loss:0.9625101970167791\n",
      "train loss:0.8640988650670377\n",
      "train loss:0.9540682928364477\n",
      "train loss:0.8648929945536221\n",
      "train loss:0.8020489489824157\n",
      "train loss:1.0005678404913045\n",
      "train loss:0.82438481725045\n",
      "train loss:0.8649109845744937\n",
      "train loss:0.7853463731412347\n",
      "train loss:0.9338878146974978\n",
      "train loss:0.9808756092165121\n",
      "train loss:1.0716951057162283\n",
      "train loss:1.0079185849657026\n",
      "train loss:1.0417183791408287\n",
      "train loss:1.0341358877074291\n",
      "train loss:0.8814211329145423\n",
      "train loss:0.9904961369926543\n",
      "train loss:1.0109464465794182\n",
      "train loss:0.9004569785122726\n",
      "train loss:0.8490435608173054\n",
      "train loss:0.7408709673098541\n",
      "train loss:1.053338808166882\n",
      "train loss:1.012437382720711\n",
      "train loss:0.9242146294121285\n",
      "train loss:0.9726472166115296\n",
      "train loss:1.1392721234190555\n",
      "train loss:0.967218862051873\n",
      "train loss:0.9616651515176655\n",
      "train loss:0.9842995555232744\n",
      "train loss:1.0755703542571284\n",
      "train loss:0.8968648161674669\n",
      "train loss:0.9937886868540002\n",
      "train loss:0.9145377072730677\n",
      "train loss:0.9596525159397363\n",
      "train loss:0.8492533256571134\n",
      "train loss:0.9649041727278548\n",
      "train loss:1.0516663064838119\n",
      "train loss:1.1628148532369058\n",
      "train loss:0.8778615047344246\n",
      "train loss:0.9925172027513263\n",
      "train loss:0.9802075443900707\n",
      "train loss:0.947583436147685\n",
      "train loss:0.883875736705153\n",
      "train loss:1.0812560858489153\n",
      "train loss:0.9320321196249427\n",
      "train loss:0.9250546886713055\n",
      "train loss:0.9651688077106035\n",
      "train loss:0.8840571994672706\n",
      "train loss:0.9279379404525829\n",
      "train loss:0.8825746270754671\n",
      "train loss:0.8516949623059967\n",
      "train loss:0.8625360690061287\n",
      "train loss:0.9223515846955428\n",
      "train loss:0.9409122133056098\n",
      "train loss:1.0780336976497393\n",
      "train loss:1.0348546821483693\n",
      "train loss:1.1689047192396995\n",
      "train loss:0.8166062023143723\n",
      "train loss:1.0016180984933991\n",
      "train loss:0.8607045514582612\n",
      "train loss:1.0042195793974953\n",
      "train loss:0.8245715872202882\n",
      "train loss:0.8103779552930616\n",
      "train loss:0.9848004855176464\n",
      "train loss:0.7760029094365437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8580268512076482\n",
      "train loss:0.80251699691097\n",
      "train loss:1.1139174545937578\n",
      "train loss:0.9897878143371213\n",
      "train loss:1.005575241782354\n",
      "train loss:0.9143789006263138\n",
      "train loss:1.0563253601508822\n",
      "train loss:0.8712085491802601\n",
      "train loss:0.9349523925909292\n",
      "train loss:0.9654540940126366\n",
      "train loss:1.1781660692214162\n",
      "train loss:0.9503070809795757\n",
      "train loss:0.8112958346917652\n",
      "train loss:0.9694095272864227\n",
      "train loss:1.1763573025304843\n",
      "train loss:0.8817869895174851\n",
      "train loss:0.8299304543170841\n",
      "train loss:0.9852449539364876\n",
      "train loss:0.9337474818993821\n",
      "train loss:1.0848916702774976\n",
      "train loss:0.7876804238003813\n",
      "train loss:0.8816792592378258\n",
      "train loss:0.9758193791455793\n",
      "train loss:0.8691575946381639\n",
      "train loss:0.9190668900089652\n",
      "train loss:0.9899417768977722\n",
      "train loss:0.9843786082120518\n",
      "train loss:1.0111863494080915\n",
      "train loss:0.9318824031692859\n",
      "train loss:0.9653472087117342\n",
      "train loss:0.8785957370633105\n",
      "train loss:0.8477276949024325\n",
      "train loss:1.02345814938614\n",
      "train loss:0.930504444149831\n",
      "train loss:0.933247578197894\n",
      "train loss:0.8101261274923555\n",
      "train loss:0.9731198960727494\n",
      "train loss:1.0051000554361158\n",
      "train loss:0.8258631954265269\n",
      "train loss:0.9225504296847222\n",
      "train loss:1.0090752130036054\n",
      "train loss:0.7857418166838376\n",
      "train loss:0.8989315212917804\n",
      "train loss:0.920356318854176\n",
      "train loss:0.7874826629917782\n",
      "train loss:0.8509675181027349\n",
      "train loss:0.9522487080826915\n",
      "train loss:0.9762345443323507\n",
      "train loss:0.8338871995814877\n",
      "train loss:1.111864296798125\n",
      "train loss:1.053900847264854\n",
      "train loss:0.9252780364557993\n",
      "train loss:0.892278157228445\n",
      "train loss:0.946372743189262\n",
      "train loss:0.9523054598351968\n",
      "train loss:0.971376076497812\n",
      "train loss:1.0289627289739185\n",
      "train loss:0.9136644121138707\n",
      "train loss:0.902720133122387\n",
      "train loss:0.9823859999170271\n",
      "train loss:0.9398247441173057\n",
      "train loss:1.038683545079154\n",
      "train loss:0.8049445597736857\n",
      "train loss:1.012820938563058\n",
      "train loss:0.957768102058811\n",
      "train loss:1.1053591029412622\n",
      "train loss:0.9778890487505175\n",
      "train loss:0.9967052977261419\n",
      "train loss:0.8709040295018602\n",
      "train loss:0.7307908908090427\n",
      "train loss:0.9270350048559954\n",
      "train loss:0.9956417076173912\n",
      "train loss:0.9611415775888742\n",
      "train loss:1.0216915218653482\n",
      "train loss:1.141498175397701\n",
      "train loss:0.8852671264758811\n",
      "train loss:0.8526205764877345\n",
      "train loss:1.0339319381625631\n",
      "train loss:0.912535150219114\n",
      "train loss:0.9967332294120793\n",
      "train loss:0.9325372314960305\n",
      "train loss:1.0846237814174529\n",
      "train loss:0.7930844785461446\n",
      "train loss:1.0440537522118136\n",
      "train loss:0.8939031318891704\n",
      "train loss:1.0121116887834112\n",
      "train loss:0.8045487491735033\n",
      "train loss:0.7938828694348884\n",
      "train loss:1.075273999524263\n",
      "train loss:0.8812061467015048\n",
      "train loss:0.9115505984467493\n",
      "train loss:0.845730879812901\n",
      "train loss:0.8775697535136426\n",
      "train loss:1.0014427729249298\n",
      "train loss:0.8886425077510239\n",
      "train loss:0.999002120049185\n",
      "train loss:1.0119767334584047\n",
      "train loss:1.0122383032526288\n",
      "train loss:0.9907691316830785\n",
      "train loss:0.892183203056939\n",
      "train loss:0.8657592445099624\n",
      "train loss:1.1063062819699183\n",
      "train loss:0.9668549876878864\n",
      "train loss:1.0128084341826924\n",
      "train loss:0.9496403686095479\n",
      "train loss:0.9748722441194998\n",
      "train loss:1.1742130843638294\n",
      "train loss:0.8727919235485082\n",
      "train loss:0.9685786744326457\n",
      "train loss:0.9139878621403577\n",
      "train loss:0.8112908167564629\n",
      "train loss:0.8559045107319493\n",
      "train loss:0.9787013585287769\n",
      "train loss:1.0325628528313435\n",
      "train loss:1.2053310769700833\n",
      "train loss:0.8920090209594771\n",
      "train loss:1.0446907679996118\n",
      "train loss:0.9183399493077015\n",
      "train loss:1.2209021819296364\n",
      "train loss:0.8345695635900209\n",
      "train loss:0.8826210502728871\n",
      "train loss:0.9932286788099056\n",
      "train loss:0.864815999712762\n",
      "train loss:1.0114564990919996\n",
      "train loss:0.9422019159942345\n",
      "train loss:0.9345995895119721\n",
      "train loss:0.8708411250477104\n",
      "train loss:0.9165558787815778\n",
      "train loss:1.0252974850986545\n",
      "train loss:0.8579958058149001\n",
      "train loss:0.931889496158113\n",
      "train loss:0.7942087517374634\n",
      "train loss:0.8404500130141791\n",
      "train loss:0.9061683012485\n",
      "train loss:0.9255091513087267\n",
      "train loss:0.8336350544697495\n",
      "train loss:1.0489384218992095\n",
      "train loss:0.9759624145350468\n",
      "train loss:0.925269774268962\n",
      "train loss:0.9845582929537301\n",
      "train loss:0.9812865419926183\n",
      "train loss:0.8768388346316442\n",
      "train loss:0.8082075030217777\n",
      "train loss:0.9617484311718502\n",
      "train loss:0.9688850434773733\n",
      "train loss:0.8555433019801805\n",
      "train loss:0.9183940521217652\n",
      "train loss:1.0293030750972887\n",
      "train loss:0.9737965918640021\n",
      "train loss:1.075498587440933\n",
      "train loss:0.8728468072292668\n",
      "train loss:0.7730560446382653\n",
      "train loss:1.0650990403182725\n",
      "=== epoch:4, train acc:0.992, test acc:0.986 ===\n",
      "train loss:0.9523706800336231\n",
      "train loss:1.0261963013763526\n",
      "train loss:1.1332221557295445\n",
      "train loss:0.9220166758077059\n",
      "train loss:0.9425224739292415\n",
      "train loss:1.117671820548097\n",
      "train loss:0.9264593587872783\n",
      "train loss:1.010004084887712\n",
      "train loss:0.8451303133743362\n",
      "train loss:0.9731457462972589\n",
      "train loss:0.8573660867258137\n",
      "train loss:1.118896213363765\n",
      "train loss:0.8191170733120989\n",
      "train loss:0.7175248812344444\n",
      "train loss:1.1872159738389447\n",
      "train loss:0.8330098213576119\n",
      "train loss:0.9780138945654504\n",
      "train loss:0.9782157453672506\n",
      "train loss:1.049263576633802\n",
      "train loss:0.9841832429091985\n",
      "train loss:0.882534205772425\n",
      "train loss:0.9528700782442436\n",
      "train loss:1.003267303163433\n",
      "train loss:0.8881838383350649\n",
      "train loss:0.8115746991014577\n",
      "train loss:1.0441860607806344\n",
      "train loss:0.948021202473221\n",
      "train loss:0.9151804833777142\n",
      "train loss:1.1016055784548875\n",
      "train loss:0.928047223407546\n",
      "train loss:0.8689000303781192\n",
      "train loss:1.0642184990312307\n",
      "train loss:0.9282591914128681\n",
      "train loss:0.8368286566550844\n",
      "train loss:0.8158326304188502\n",
      "train loss:0.9364187871645752\n",
      "train loss:0.7454520252646067\n",
      "train loss:0.9257628668749821\n",
      "train loss:0.8999410651551236\n",
      "train loss:0.9055645088659964\n",
      "train loss:0.8348020928170972\n",
      "train loss:0.8839008362886045\n",
      "train loss:0.8008730576771353\n",
      "train loss:0.8458662768985096\n",
      "train loss:0.9020307241507953\n",
      "train loss:1.303046417166737\n",
      "train loss:1.1588021841139506\n",
      "train loss:0.9412775535876641\n",
      "train loss:1.0237736070183105\n",
      "train loss:1.0613236514022675\n",
      "train loss:0.9341929215307612\n",
      "train loss:0.7842586051490825\n",
      "train loss:0.8185495720716729\n",
      "train loss:1.0705702334229843\n",
      "train loss:1.027172001137987\n",
      "train loss:0.7368046344159778\n",
      "train loss:1.0472449995306943\n",
      "train loss:0.8791969915731628\n",
      "train loss:1.0210955056894218\n",
      "train loss:1.1025185837220228\n",
      "train loss:0.7743021332689449\n",
      "train loss:0.9042686379148752\n",
      "train loss:1.0460283649252833\n",
      "train loss:0.982854783996086\n",
      "train loss:0.8660756788638788\n",
      "train loss:0.8365244193452174\n",
      "train loss:0.9415750986293605\n",
      "train loss:0.9103225594962665\n",
      "train loss:0.9173466008361585\n",
      "train loss:0.9461159861989138\n",
      "train loss:0.8979901829428053\n",
      "train loss:0.9045560536326966\n",
      "train loss:0.9681178724222654\n",
      "train loss:0.8821991848388911\n",
      "train loss:0.9461389900972113\n",
      "train loss:0.9096418150802414\n",
      "train loss:0.856194916065309\n",
      "train loss:0.8509918001736346\n",
      "train loss:0.8711382970123412\n",
      "train loss:0.8574347595006324\n",
      "train loss:1.0431907652823498\n",
      "train loss:0.7457840747562241\n",
      "train loss:0.7529145292955444\n",
      "train loss:0.7849276943305044\n",
      "train loss:0.8538381669823959\n",
      "train loss:0.9205251111887272\n",
      "train loss:0.8999547614613865\n",
      "train loss:1.0551056106453998\n",
      "train loss:0.9445575967912405\n",
      "train loss:1.0963866030666\n",
      "train loss:0.9866774678207655\n",
      "train loss:0.9861310816387355\n",
      "train loss:0.8731063539681635\n",
      "train loss:0.8455540093003847\n",
      "train loss:1.020726234426598\n",
      "train loss:0.9884399209738289\n",
      "train loss:0.8388942466075597\n",
      "train loss:0.8534709146201148\n",
      "train loss:0.9901128239572636\n",
      "train loss:0.6763028828777324\n",
      "train loss:1.0673371569470689\n",
      "train loss:0.9051148482496805\n",
      "train loss:1.05632706246841\n",
      "train loss:1.0678928880309753\n",
      "train loss:1.097556631928323\n",
      "train loss:0.9306709930130015\n",
      "train loss:1.0703371496824403\n",
      "train loss:0.8587400012283152\n",
      "train loss:0.9959590089693112\n",
      "train loss:1.1057124226171142\n",
      "train loss:1.1139919034372305\n",
      "train loss:0.9599338467483968\n",
      "train loss:0.9141113767405434\n",
      "train loss:0.8736829523370705\n",
      "train loss:1.0378116803707915\n",
      "train loss:0.9143628914440742\n",
      "train loss:0.9412612816731137\n",
      "train loss:0.9430531287376208\n",
      "train loss:0.9577776468422572\n",
      "train loss:0.9781341608071398\n",
      "train loss:1.00086404958771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8747286022841876\n",
      "train loss:0.8764877616995027\n",
      "train loss:0.809500514311618\n",
      "train loss:0.8202122997422299\n",
      "train loss:1.0448863664072638\n",
      "train loss:1.1555949854432002\n",
      "train loss:0.8322381793623036\n",
      "train loss:0.9816495057044614\n",
      "train loss:0.8507310498695125\n",
      "train loss:1.0614456025853927\n",
      "train loss:0.8565058182381159\n",
      "train loss:1.0952723319950015\n",
      "train loss:0.9479980697863319\n",
      "train loss:1.0566488048972693\n",
      "train loss:1.0987241639039604\n",
      "train loss:1.087739333529035\n",
      "train loss:0.8950241579065036\n",
      "train loss:1.073394118028887\n",
      "train loss:0.9260282938116501\n",
      "train loss:0.8734137849148997\n",
      "train loss:1.0209880328133811\n",
      "train loss:0.7667855070920614\n",
      "train loss:0.7510290036855496\n",
      "train loss:0.9745198726060065\n",
      "train loss:1.1449742141837902\n",
      "train loss:0.9607086125262186\n",
      "train loss:0.8589408205164331\n",
      "train loss:0.9401598858151766\n",
      "train loss:0.8244835046592927\n",
      "train loss:0.8342476021461891\n",
      "train loss:0.8304613295290961\n",
      "train loss:0.7671176917077271\n",
      "train loss:1.0531777016931665\n",
      "train loss:0.9494325358325848\n",
      "train loss:0.9096690556149334\n",
      "train loss:1.0243026149896912\n",
      "train loss:0.9953423076766059\n",
      "train loss:0.9987374858538887\n",
      "train loss:1.0164849475586137\n",
      "train loss:0.9142182229679635\n",
      "train loss:1.0027686819820092\n",
      "train loss:0.8606817324382984\n",
      "train loss:0.9942918996130095\n",
      "train loss:0.8741088058448918\n",
      "train loss:0.8982754596479314\n",
      "train loss:0.9087706586451334\n",
      "train loss:0.9808978204815415\n",
      "train loss:0.9849208942528473\n",
      "train loss:0.9218442097020225\n",
      "train loss:0.8823040758958582\n",
      "train loss:0.9062376830100207\n",
      "train loss:0.9783925306482838\n",
      "train loss:0.9126500659722248\n",
      "train loss:0.9661355793326198\n",
      "train loss:0.8629626108314481\n",
      "train loss:0.9473090184094051\n",
      "train loss:0.9619730670551818\n",
      "train loss:1.039962201172207\n",
      "train loss:1.1392016279245125\n",
      "train loss:0.8348835546474582\n",
      "train loss:0.8598753018907757\n",
      "train loss:0.9810418276817542\n",
      "train loss:1.1629535352044094\n",
      "train loss:0.7944307738330904\n",
      "train loss:0.7856982691680788\n",
      "train loss:1.0223223932701822\n",
      "train loss:1.0329896113239716\n",
      "train loss:0.8436711195879031\n",
      "train loss:0.7566373704894357\n",
      "train loss:0.9182016732611951\n",
      "train loss:0.892138999398971\n",
      "train loss:0.8829154926330004\n",
      "train loss:0.764437537736932\n",
      "train loss:0.9775469749410775\n",
      "train loss:0.997205087201243\n",
      "train loss:0.8329552640392737\n",
      "train loss:0.849299593795602\n",
      "train loss:1.0160083101449828\n",
      "train loss:0.8959403031600904\n",
      "train loss:0.9757190385593227\n",
      "train loss:0.8019311680885081\n",
      "train loss:1.0191430392575738\n",
      "train loss:0.9625882825747923\n",
      "train loss:0.9150179605542417\n",
      "train loss:0.8674259653187494\n",
      "train loss:1.0475189717315923\n",
      "train loss:0.8582029827384206\n",
      "train loss:0.9835813289951741\n",
      "train loss:0.9951977605004454\n",
      "train loss:0.9644918712982298\n",
      "train loss:0.9178920303833153\n",
      "train loss:0.9263958768501499\n",
      "train loss:1.0834643811563853\n",
      "train loss:1.0034750529333367\n",
      "train loss:0.818274107863322\n",
      "train loss:0.9458523581665165\n",
      "train loss:0.7384818135644303\n",
      "train loss:1.0336974322966201\n",
      "train loss:0.9101541324420194\n",
      "train loss:0.772573268013315\n",
      "train loss:0.9717973774040607\n",
      "train loss:1.1629127340637988\n",
      "train loss:0.8438738071127841\n",
      "train loss:0.8600837426984191\n",
      "train loss:1.028313051281939\n",
      "train loss:1.0403836923674108\n",
      "train loss:1.0067081527363018\n",
      "train loss:0.8904943929914207\n",
      "train loss:0.9744787751790162\n",
      "train loss:0.8599673510236204\n",
      "train loss:1.0764618242378279\n",
      "train loss:0.971643023890953\n",
      "train loss:0.8497665351797202\n",
      "train loss:0.955421996102462\n",
      "train loss:0.8710460745052728\n",
      "train loss:0.8941548334495846\n",
      "train loss:1.0071087876560691\n",
      "train loss:1.0046900353487522\n",
      "train loss:1.1118238454637568\n",
      "train loss:0.9489096653498426\n",
      "train loss:0.7956027931718385\n",
      "train loss:0.8632127235436958\n",
      "train loss:0.9703330891945336\n",
      "train loss:0.8844175712804598\n",
      "train loss:0.9668094337404584\n",
      "train loss:0.7824663391263413\n",
      "train loss:0.9221134365248654\n",
      "train loss:0.7744572325416715\n",
      "train loss:0.8229958660412718\n",
      "train loss:0.8493618353013543\n",
      "train loss:0.9066368121768132\n",
      "train loss:1.0306592865584858\n",
      "train loss:1.1030348723801033\n",
      "train loss:0.8662674765935264\n",
      "train loss:0.9000721349244968\n",
      "train loss:1.0699385454066723\n",
      "train loss:1.1218580545794012\n",
      "train loss:0.8258173051917397\n",
      "train loss:0.8698399608484355\n",
      "train loss:0.8784521193416124\n",
      "train loss:0.9861213238656475\n",
      "train loss:0.7827835715403328\n",
      "train loss:0.9549580850899104\n",
      "train loss:0.9371017336386709\n",
      "train loss:0.9462122232745316\n",
      "train loss:0.815257368154982\n",
      "train loss:1.0154582690766638\n",
      "train loss:1.0276410507329745\n",
      "train loss:0.8829469265316614\n",
      "train loss:0.9061666967607817\n",
      "train loss:0.8744184198578341\n",
      "train loss:0.9313372386913088\n",
      "train loss:0.8652142238298614\n",
      "train loss:1.179434319995059\n",
      "train loss:0.9817619862198906\n",
      "train loss:1.1694782691364467\n",
      "train loss:0.95429393108258\n",
      "train loss:0.8475236630877379\n",
      "train loss:0.9765208774281592\n",
      "train loss:0.8579125297064116\n",
      "train loss:0.8165985383681459\n",
      "train loss:0.9609928336080615\n",
      "train loss:0.914926830903558\n",
      "train loss:0.9438656114124028\n",
      "train loss:0.9053480913567705\n",
      "train loss:0.9275620791979042\n",
      "train loss:0.8283487908514026\n",
      "train loss:0.818441266904853\n",
      "train loss:0.9396137840134793\n",
      "train loss:0.8472355825805213\n",
      "train loss:1.0339845955038864\n",
      "train loss:0.9174066493159374\n",
      "train loss:0.8411450516833799\n",
      "train loss:0.7705545707195489\n",
      "train loss:1.0259129460593686\n",
      "train loss:0.7551455977846069\n",
      "train loss:0.8931480177117039\n",
      "train loss:0.8382426499752701\n",
      "train loss:1.0690353393465628\n",
      "train loss:0.954921565480881\n",
      "train loss:0.8921563791740924\n",
      "train loss:0.9696120976565009\n",
      "train loss:0.8792874126059386\n",
      "train loss:0.8194333196661263\n",
      "train loss:0.7127051512583358\n",
      "train loss:1.062869072630803\n",
      "train loss:0.8684353043428384\n",
      "train loss:0.9019456464920984\n",
      "train loss:0.9170221183152002\n",
      "train loss:0.8167698630455748\n",
      "train loss:0.9432442323915289\n",
      "train loss:0.9266099209891471\n",
      "train loss:0.9236291542018682\n",
      "train loss:1.0627134222971215\n",
      "train loss:0.9497214882844898\n",
      "train loss:0.9378729788578843\n",
      "train loss:0.7670683089402551\n",
      "train loss:0.8087077284275587\n",
      "train loss:0.8439251497461597\n",
      "train loss:0.951323778594888\n",
      "train loss:0.9301932878740297\n",
      "train loss:0.8934698167645915\n",
      "train loss:1.0468705057983356\n",
      "train loss:1.1662783700456205\n",
      "train loss:0.959389459237546\n",
      "train loss:1.054937264528006\n",
      "train loss:0.7684570046567426\n",
      "train loss:0.9280315360018431\n",
      "train loss:0.9189182193449618\n",
      "train loss:0.8165362470395824\n",
      "train loss:0.9889720614815343\n",
      "train loss:1.0983302616208146\n",
      "train loss:0.9005611077428548\n",
      "train loss:0.9107794419660152\n",
      "train loss:0.9736986001611969\n",
      "train loss:0.9053541337762389\n",
      "train loss:1.0890914821578273\n",
      "train loss:1.0241634233625987\n",
      "train loss:0.9090743497943616\n",
      "train loss:1.0305868177876647\n",
      "train loss:0.9134475810025172\n",
      "train loss:0.8155054535765427\n",
      "train loss:0.8818982591885706\n",
      "train loss:1.0367106923024134\n",
      "train loss:0.7510569408868301\n",
      "train loss:0.9454419876618467\n",
      "train loss:1.0567387764052922\n",
      "train loss:1.0770779825109789\n",
      "train loss:0.787812745186168\n",
      "train loss:0.7428537210134796\n",
      "train loss:0.976892318627332\n",
      "train loss:0.9321079590348333\n",
      "train loss:0.888881457626334\n",
      "train loss:0.9731093353762623\n",
      "train loss:0.9541478242755416\n",
      "train loss:1.0129899970649634\n",
      "train loss:1.0833663253751458\n",
      "train loss:0.9308393965550316\n",
      "train loss:1.0969559937007625\n",
      "train loss:1.0691915154762044\n",
      "train loss:1.064010283087419\n",
      "train loss:0.9581300178848309\n",
      "train loss:0.9188200856902655\n",
      "train loss:0.8396058838279531\n",
      "train loss:0.8736354040702164\n",
      "train loss:0.8275821104536887\n",
      "train loss:0.9739627157528323\n",
      "train loss:1.0135477349619326\n",
      "train loss:1.00007816598431\n",
      "train loss:1.0193915533996643\n",
      "train loss:0.7716621819612398\n",
      "train loss:1.1291725068350145\n",
      "train loss:1.0867487243244913\n",
      "train loss:0.9179832532125449\n",
      "train loss:0.8985646582580239\n",
      "train loss:0.9069539931013465\n",
      "train loss:1.1505074350720839\n",
      "train loss:1.037783903523777\n",
      "train loss:0.8054449740718936\n",
      "train loss:1.0969734665897357\n",
      "train loss:0.9712709210628274\n",
      "train loss:1.0643090774587995\n",
      "train loss:0.8355577041992059\n",
      "train loss:0.9490838446101663\n",
      "train loss:0.971861122262364\n",
      "train loss:0.9111958116570064\n",
      "train loss:0.9160012704276979\n",
      "train loss:0.775297082684379\n",
      "train loss:0.8382234599418742\n",
      "train loss:1.0659709797760042\n",
      "train loss:0.9239724943661232\n",
      "train loss:0.9472821600699775\n",
      "train loss:0.9854565884157824\n",
      "train loss:0.8184863234750622\n",
      "train loss:0.9917369606873114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8077626006603977\n",
      "train loss:0.8615710456872305\n",
      "train loss:0.9547787596395145\n",
      "train loss:0.9613870434069702\n",
      "train loss:0.8618243657104289\n",
      "train loss:1.018777334002502\n",
      "train loss:0.8786489375897486\n",
      "train loss:0.9689232123429986\n",
      "train loss:0.9779575493108006\n",
      "train loss:0.8747954556728472\n",
      "train loss:0.94123434528695\n",
      "train loss:0.7486786526778908\n",
      "train loss:0.8766534273548371\n",
      "train loss:0.9225539536514075\n",
      "train loss:1.01195338929994\n",
      "train loss:0.8741654985165921\n",
      "train loss:0.8867116448275374\n",
      "train loss:0.9053735307255123\n",
      "train loss:0.9555364309482348\n",
      "train loss:1.0187830238774278\n",
      "train loss:0.9057315350035223\n",
      "train loss:0.9414161629181035\n",
      "train loss:1.0881856221743418\n",
      "train loss:0.8278282871158403\n",
      "train loss:0.9065560448665455\n",
      "train loss:1.081391981468424\n",
      "train loss:0.911464985501475\n",
      "train loss:0.971843233528763\n",
      "train loss:1.2076155995854796\n",
      "train loss:0.9479454699643165\n",
      "train loss:0.9096518711615164\n",
      "train loss:0.8862562946014387\n",
      "train loss:0.8963959277916294\n",
      "train loss:0.9602032878044349\n",
      "train loss:1.0905632813853008\n",
      "train loss:1.0586943974975462\n",
      "train loss:0.9665268849549423\n",
      "train loss:0.9608135491946668\n",
      "train loss:0.9570793786412625\n",
      "train loss:0.7299204985182012\n",
      "train loss:0.9537190253803373\n",
      "train loss:0.8510010701048245\n",
      "train loss:0.9327801235086575\n",
      "train loss:0.837592480734441\n",
      "train loss:0.9728101352319672\n",
      "train loss:1.1044653627644225\n",
      "train loss:0.8752624288916065\n",
      "train loss:0.8973824557832059\n",
      "train loss:1.0237243892569714\n",
      "train loss:0.8301136594789724\n",
      "train loss:0.8610065922928402\n",
      "train loss:1.089366341158776\n",
      "train loss:0.8364564061857481\n",
      "train loss:1.0415137166672168\n",
      "train loss:1.018640328085042\n",
      "train loss:0.9097856763971663\n",
      "train loss:0.9156940824059463\n",
      "train loss:1.018624379625895\n",
      "train loss:0.9762659268136343\n",
      "train loss:0.9681109969092959\n",
      "train loss:0.8043391802766939\n",
      "train loss:0.9884166527433643\n",
      "train loss:0.9919206970469524\n",
      "train loss:0.9426770247218306\n",
      "train loss:0.9845392948567896\n",
      "train loss:0.8960625557927321\n",
      "train loss:0.8381745777538336\n",
      "train loss:0.8617848066281809\n",
      "train loss:0.8380203714849653\n",
      "train loss:0.856886341272\n",
      "train loss:0.9360825343482155\n",
      "train loss:0.8013235556605685\n",
      "train loss:0.9752618487259981\n",
      "train loss:0.8671199964262091\n",
      "train loss:0.7396002344248154\n",
      "train loss:0.8374201005910754\n",
      "train loss:1.0015682361396225\n",
      "train loss:0.9312035023408259\n",
      "train loss:0.9660154760621572\n",
      "train loss:0.9602605820561113\n",
      "train loss:0.7641362748291017\n",
      "train loss:0.8363045822890556\n",
      "train loss:0.8793825353459936\n",
      "train loss:0.9735498882294933\n",
      "train loss:1.1622504870908157\n",
      "train loss:0.9814232367117064\n",
      "train loss:1.104524956164324\n",
      "train loss:0.9848239284246972\n",
      "train loss:0.9812316104353564\n",
      "train loss:0.9884572969237783\n",
      "train loss:0.9585091974417367\n",
      "train loss:0.8235717020309623\n",
      "train loss:0.8898571796781347\n",
      "train loss:0.9170900806481513\n",
      "train loss:0.894177901136375\n",
      "train loss:0.9566386899082466\n",
      "train loss:0.8688742226743394\n",
      "train loss:0.7879844504189569\n",
      "train loss:0.9543967969060039\n",
      "train loss:0.8755546575981578\n",
      "train loss:0.9118557896080884\n",
      "train loss:0.9212936613177657\n",
      "train loss:0.790807479421303\n",
      "train loss:0.8850366952529951\n",
      "train loss:0.9333752266338271\n",
      "train loss:0.8659301186916012\n",
      "train loss:0.8390844042906841\n",
      "train loss:1.098951675482916\n",
      "train loss:0.9338594934530614\n",
      "train loss:0.9962751716396302\n",
      "train loss:0.9742890568221566\n",
      "train loss:0.9800772228170541\n",
      "train loss:0.9140505071145857\n",
      "train loss:0.8802488403353999\n",
      "train loss:1.2696903771744483\n",
      "train loss:0.9311626612051164\n",
      "train loss:0.9337278954956207\n",
      "train loss:0.99617509911753\n",
      "train loss:0.8444934564227927\n",
      "train loss:1.052108093465427\n",
      "train loss:0.9255509037261268\n",
      "train loss:0.9409678732114982\n",
      "train loss:0.8407849126664231\n",
      "train loss:0.8933999668313888\n",
      "train loss:0.9251791898377602\n",
      "train loss:1.1010349762671674\n",
      "train loss:1.1220326675389227\n",
      "train loss:0.9599308592960825\n",
      "train loss:0.8279809961656415\n",
      "train loss:1.2585081848765745\n",
      "train loss:0.8944600831498025\n",
      "train loss:1.0246223567342516\n",
      "train loss:0.9988339449737227\n",
      "train loss:1.009534799956207\n",
      "train loss:0.7683245844354477\n",
      "train loss:1.0010695077260572\n",
      "train loss:0.8210227250600585\n",
      "train loss:0.9942197032381586\n",
      "train loss:0.8341325932706534\n",
      "train loss:0.7201453119651832\n",
      "train loss:0.9135405124928365\n",
      "train loss:1.0060690820541263\n",
      "train loss:0.9545320175434397\n",
      "train loss:0.9467227355402078\n",
      "train loss:0.7769326253762944\n",
      "train loss:1.0380364177608203\n",
      "train loss:0.7837095156363232\n",
      "train loss:1.0421285362982626\n",
      "train loss:0.9272408299080922\n",
      "train loss:0.7056517136821338\n",
      "train loss:0.8667849362348082\n",
      "train loss:0.8670115938693672\n",
      "train loss:0.8928718878716281\n",
      "train loss:0.9733506479774049\n",
      "train loss:0.9187010055838001\n",
      "train loss:0.8222594766638213\n",
      "train loss:0.8135394472632893\n",
      "train loss:0.7251705158206555\n",
      "train loss:0.9109409560478288\n",
      "train loss:1.0522256179162093\n",
      "train loss:0.9784381243832536\n",
      "train loss:0.7467862886313097\n",
      "train loss:0.9475539807158277\n",
      "train loss:0.8100309416464745\n",
      "train loss:1.1044721431774829\n",
      "train loss:1.0407040669329977\n",
      "train loss:0.8160130068877075\n",
      "train loss:0.9840297637688891\n",
      "train loss:0.9184236295225956\n",
      "train loss:0.9926902682573783\n",
      "train loss:0.8900919497548841\n",
      "train loss:0.8643987554881167\n",
      "train loss:0.8999412955806789\n",
      "train loss:0.8491185776913349\n",
      "train loss:0.8129901690849123\n",
      "train loss:0.7865081376873411\n",
      "train loss:0.9762073210048432\n",
      "train loss:0.9504848552269729\n",
      "train loss:0.9260081120224902\n",
      "train loss:0.8300954034947072\n",
      "train loss:0.7425188800600914\n",
      "train loss:0.8575301519429525\n",
      "train loss:1.0070863751794814\n",
      "train loss:0.9213949403797049\n",
      "train loss:0.876526275206847\n",
      "train loss:1.0732324786743612\n",
      "train loss:0.9777377649606245\n",
      "train loss:0.9657812297666901\n",
      "train loss:1.0280180513896493\n",
      "train loss:0.7749490531475444\n",
      "train loss:0.9300218884795217\n",
      "train loss:0.9214979682766566\n",
      "train loss:1.0375296662451987\n",
      "train loss:0.9638300403079704\n",
      "train loss:1.045789098810817\n",
      "train loss:0.8044490598640722\n",
      "train loss:0.995607154292676\n",
      "train loss:0.846230314485011\n",
      "train loss:1.0013730511430026\n",
      "train loss:0.9056492630833536\n",
      "train loss:0.9164342029437011\n",
      "train loss:0.9150984602966341\n",
      "train loss:0.8549270786292561\n",
      "train loss:0.8587097811989465\n",
      "=== epoch:5, train acc:0.99, test acc:0.989 ===\n",
      "train loss:0.8800563639367681\n",
      "train loss:1.0572457883023811\n",
      "train loss:0.9376320750732386\n",
      "train loss:0.9748851537455333\n",
      "train loss:0.8130183440662602\n",
      "train loss:1.2291888736195686\n",
      "train loss:1.091460140491877\n",
      "train loss:0.865140578731322\n",
      "train loss:1.0300743460963206\n",
      "train loss:0.8176620164098186\n",
      "train loss:0.81197773097082\n",
      "train loss:0.8678455526220771\n",
      "train loss:0.9857186343839837\n",
      "train loss:1.1203794784544023\n",
      "train loss:0.7312860365820177\n",
      "train loss:0.9709521973218065\n",
      "train loss:0.89324685030877\n",
      "train loss:1.010870016382091\n",
      "train loss:0.9624352568175428\n",
      "train loss:0.7607520681854261\n",
      "train loss:1.0492769065843335\n",
      "train loss:0.7342596935662391\n",
      "train loss:0.9846764193034412\n",
      "train loss:0.9910037914298029\n",
      "train loss:0.8656407857451528\n",
      "train loss:0.8389174685972882\n",
      "train loss:0.866287713716149\n",
      "train loss:0.9015386623862066\n",
      "train loss:0.9409066036512037\n",
      "train loss:1.0738154151319674\n",
      "train loss:0.9553086453380846\n",
      "train loss:0.8565125500762905\n",
      "train loss:0.9053780560825208\n",
      "train loss:0.7989924619940098\n",
      "train loss:0.7632808851295059\n",
      "train loss:0.9551042096482751\n",
      "train loss:0.932599762407479\n",
      "train loss:0.9472325956504849\n",
      "train loss:0.8446451541817324\n",
      "train loss:0.9647826727333694\n",
      "train loss:0.8840137858608492\n",
      "train loss:0.9730552272244111\n",
      "train loss:0.8791493211498838\n",
      "train loss:1.0331660509924907\n",
      "train loss:0.9943647458673557\n",
      "train loss:0.8739702181478899\n",
      "train loss:0.8960246440668103\n",
      "train loss:0.882077000579625\n",
      "train loss:0.8058939426813985\n",
      "train loss:0.8121272766962061\n",
      "train loss:0.8200249233160543\n",
      "train loss:0.9381146704942332\n",
      "train loss:0.8540855868664963\n",
      "train loss:0.8324098079576963\n",
      "train loss:0.7753518187695689\n",
      "train loss:0.9060850419330303\n",
      "train loss:0.9958107735820393\n",
      "train loss:1.0007976244736685\n",
      "train loss:0.8694152795907251\n",
      "train loss:0.8491911121998527\n",
      "train loss:0.8952246726135441\n",
      "train loss:0.9251629213892208\n",
      "train loss:0.8747313206602493\n",
      "train loss:0.9887932344745426\n",
      "train loss:0.7756301813691703\n",
      "train loss:1.1127575626699546\n",
      "train loss:0.9912919161668472\n",
      "train loss:0.8406395920194895\n",
      "train loss:0.8588078642044712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9227253881774392\n",
      "train loss:1.0866681645579073\n",
      "train loss:0.8826063434581305\n",
      "train loss:0.9767982959973596\n",
      "train loss:0.9098446926036609\n",
      "train loss:0.924292523272421\n",
      "train loss:0.8177094790532901\n",
      "train loss:0.9827761572360485\n",
      "train loss:0.9759583348287327\n",
      "train loss:1.0239529909963319\n",
      "train loss:0.9377647661962514\n",
      "train loss:0.8412023012687128\n",
      "train loss:0.9046615417068381\n",
      "train loss:1.053407931914324\n",
      "train loss:0.9196817704484562\n",
      "train loss:0.9302142970427546\n",
      "train loss:0.9238037132970984\n",
      "train loss:0.826514730976764\n",
      "train loss:0.948910957653014\n",
      "train loss:0.7855191376731893\n",
      "train loss:0.9469149008545295\n",
      "train loss:0.7818383913905304\n",
      "train loss:0.8728403098525882\n",
      "train loss:1.0306742097076742\n",
      "train loss:0.8905627400962157\n",
      "train loss:0.9574926264374173\n",
      "train loss:0.9484313238633565\n",
      "train loss:0.928052113726975\n",
      "train loss:0.9855199852160799\n",
      "train loss:0.9796943241777172\n",
      "train loss:0.9063693309718058\n",
      "train loss:0.8888691078026868\n",
      "train loss:0.9790976798062989\n",
      "train loss:0.79176461487388\n",
      "train loss:0.9455309419859637\n",
      "train loss:0.8887954461346433\n",
      "train loss:0.9147108354716913\n",
      "train loss:0.9232220759065186\n",
      "train loss:0.909781971136683\n",
      "train loss:0.7405304712459785\n",
      "train loss:0.9695738327204007\n",
      "train loss:0.9418392153882474\n",
      "train loss:0.8552586855835849\n",
      "train loss:0.9146527339923602\n",
      "train loss:0.8166223712198617\n",
      "train loss:1.0085295474016511\n",
      "train loss:1.049749298610912\n",
      "train loss:0.8947130989689266\n",
      "train loss:0.9236823649333346\n",
      "train loss:0.9773600138469556\n",
      "train loss:0.7558278674059765\n",
      "train loss:0.8872761582798598\n",
      "train loss:0.7957008317357349\n",
      "train loss:0.9068968215935184\n",
      "train loss:0.8392958975403206\n",
      "train loss:0.7967216584959671\n",
      "train loss:0.7741041655058161\n",
      "train loss:0.9327579468367617\n",
      "train loss:0.9862901170821488\n",
      "train loss:1.0243190734279164\n",
      "train loss:0.9449110319263205\n",
      "train loss:0.9272338062882965\n",
      "train loss:0.9897970044404946\n",
      "train loss:0.9406879807315647\n",
      "train loss:0.8150001431034766\n",
      "train loss:0.8695436649140761\n",
      "train loss:0.8323303468616046\n",
      "train loss:0.845797945747415\n",
      "train loss:0.9415737061310554\n",
      "train loss:0.9071239347866421\n",
      "train loss:1.0391085488530674\n",
      "train loss:0.9913896601904177\n",
      "train loss:0.9570881138350493\n",
      "train loss:0.9200873368328081\n",
      "train loss:0.8008002034136724\n",
      "train loss:0.812873402159285\n",
      "train loss:0.8308705618175637\n",
      "train loss:0.9106396677480783\n",
      "train loss:0.9209132231239316\n",
      "train loss:0.9479082562516784\n",
      "train loss:1.1052111811703578\n",
      "train loss:0.9903528339028734\n",
      "train loss:0.9718286920542131\n",
      "train loss:0.7793253320694846\n",
      "train loss:1.0031405500684512\n",
      "train loss:0.9151652249133544\n",
      "train loss:0.755102035946996\n",
      "train loss:0.7885083667653899\n",
      "train loss:1.0304229682608197\n",
      "train loss:1.041353928310512\n",
      "train loss:1.2234761254935334\n",
      "train loss:0.9179749476033102\n",
      "train loss:0.8017534242953792\n",
      "train loss:0.9324654049581855\n",
      "train loss:0.9568038783671154\n",
      "train loss:0.8309106248502299\n",
      "train loss:1.0238287559095363\n",
      "train loss:0.9721839550052556\n",
      "train loss:0.9151824262127031\n",
      "train loss:0.811819874366939\n",
      "train loss:0.9412523503708065\n",
      "train loss:0.9821367262530692\n",
      "train loss:1.0585748686499876\n",
      "train loss:1.0052968324724594\n",
      "train loss:0.7964324194562997\n",
      "train loss:0.7709693659539063\n",
      "train loss:0.8118544296342773\n",
      "train loss:0.9450385946400972\n",
      "train loss:1.0960804622490712\n",
      "train loss:0.7848303860625663\n",
      "train loss:0.8291477694529878\n",
      "train loss:1.003205182414191\n",
      "train loss:1.0619946064320775\n",
      "train loss:0.9705330644276294\n",
      "train loss:0.8656234215731534\n",
      "train loss:0.9187040963375164\n",
      "train loss:0.8649327703807406\n",
      "train loss:1.0006483364463379\n",
      "train loss:0.9309169014385726\n",
      "train loss:0.8789490461688148\n",
      "train loss:1.014112316101664\n",
      "train loss:0.8608055300526258\n",
      "train loss:0.9187335663821962\n",
      "train loss:1.0531692266498383\n",
      "train loss:0.9050956036061473\n",
      "train loss:1.0707228663532564\n",
      "train loss:0.9597763136548834\n",
      "train loss:0.9382356205117075\n",
      "train loss:0.9455852179680184\n",
      "train loss:1.0612942169388413\n",
      "train loss:0.9163104498472622\n",
      "train loss:0.8619198403233793\n",
      "train loss:0.8839320742334028\n",
      "train loss:0.9666924145688942\n",
      "train loss:0.8342413285589025\n",
      "train loss:1.029525989201092\n",
      "train loss:0.9043192319940536\n",
      "train loss:0.800589182324723\n",
      "train loss:0.8885016581457612\n",
      "train loss:0.804050696543184\n",
      "train loss:0.873670326661474\n",
      "train loss:0.8064724020873443\n",
      "train loss:0.9029912648584897\n",
      "train loss:0.8205946701178742\n",
      "train loss:0.9187566012251884\n",
      "train loss:0.9697930967193824\n",
      "train loss:1.0369005543514103\n",
      "train loss:1.0081276242729809\n",
      "train loss:0.8786882512919916\n",
      "train loss:0.9257064919529927\n",
      "train loss:0.9951766903255375\n",
      "train loss:0.88501338854672\n",
      "train loss:0.980278699549629\n",
      "train loss:0.9875041455436231\n",
      "train loss:0.9712910900571495\n",
      "train loss:0.9996864999918528\n",
      "train loss:0.8440109269607544\n",
      "train loss:0.8966580255694321\n",
      "train loss:1.0454054940222204\n",
      "train loss:1.1302802199263668\n",
      "train loss:0.9554330212423325\n",
      "train loss:1.1431311005390385\n",
      "train loss:0.8525747253053345\n",
      "train loss:0.6728979068170754\n",
      "train loss:0.8185287643199999\n",
      "train loss:0.8719202294991969\n",
      "train loss:1.0353691628639878\n",
      "train loss:0.982591185771919\n",
      "train loss:0.8830124779972196\n",
      "train loss:0.807936701067283\n",
      "train loss:0.8279712981914144\n",
      "train loss:0.9504853979244274\n",
      "train loss:0.8609115894486482\n",
      "train loss:0.913259957703239\n",
      "train loss:1.0343151938767554\n",
      "train loss:1.0119116739128713\n",
      "train loss:0.8897844298418391\n",
      "train loss:1.0400303726294633\n",
      "train loss:0.9413062986204166\n",
      "train loss:0.8027075476138792\n",
      "train loss:1.0506374863805876\n",
      "train loss:0.9824315910809903\n",
      "train loss:0.8735782373073653\n",
      "train loss:0.9531843317928609\n",
      "train loss:0.8314677970587455\n",
      "train loss:0.8934742438368901\n",
      "train loss:0.999202498634133\n",
      "train loss:0.869197273001859\n",
      "train loss:1.0323619133131383\n",
      "train loss:0.8859301113377933\n",
      "train loss:0.9108223638185321\n",
      "train loss:0.9024235469463529\n",
      "train loss:0.9358546968493147\n",
      "train loss:0.8652814517427957\n",
      "train loss:0.890370720514179\n",
      "train loss:0.9432470059589321\n",
      "train loss:0.957415233343414\n",
      "train loss:1.0237074348713455\n",
      "train loss:1.0104125633063814\n",
      "train loss:0.8888360494856475\n",
      "train loss:0.9892691868581359\n",
      "train loss:0.8333806218966963\n",
      "train loss:0.9829362924724169\n",
      "train loss:0.8400332202801509\n",
      "train loss:0.8988255835509493\n",
      "train loss:0.9314731881697589\n",
      "train loss:0.8617199958795975\n",
      "train loss:0.9367946963079643\n",
      "train loss:1.0312110987809056\n",
      "train loss:1.0321787934085267\n",
      "train loss:0.7776029082490975\n",
      "train loss:0.7501907024758004\n",
      "train loss:0.7609128162540002\n",
      "train loss:0.9363866357272328\n",
      "train loss:0.9007818674056051\n",
      "train loss:0.8804430167859113\n",
      "train loss:0.806883291283539\n",
      "train loss:0.9065016497482553\n",
      "train loss:0.9109512589750811\n",
      "train loss:0.8023019324788031\n",
      "train loss:0.8173779636679899\n",
      "train loss:1.0103235041013718\n",
      "train loss:0.93557047250057\n",
      "train loss:0.9688230230321383\n",
      "train loss:0.9156367751882548\n",
      "train loss:0.8991435278268084\n",
      "train loss:0.8881135637438904\n",
      "train loss:0.8036817814357736\n",
      "train loss:0.8976846453454901\n",
      "train loss:0.8134148862989927\n",
      "train loss:0.8704912883201464\n",
      "train loss:0.7767158500488351\n",
      "train loss:0.8166127386609026\n",
      "train loss:0.8540530943796242\n",
      "train loss:0.7967389959663701\n",
      "train loss:0.8662439623457077\n",
      "train loss:0.8987119479949502\n",
      "train loss:0.9458061942821626\n",
      "train loss:0.951253035319363\n",
      "train loss:0.9791884176247159\n",
      "train loss:0.9196330031167426\n",
      "train loss:0.7690760345585442\n",
      "train loss:0.8861109687882291\n",
      "train loss:0.8440389903981419\n",
      "train loss:0.8274730453077072\n",
      "train loss:0.8835651701636771\n",
      "train loss:0.8614389489101724\n",
      "train loss:0.8708448121641841\n",
      "train loss:1.0187622481889729\n",
      "train loss:0.8849767881710275\n",
      "train loss:0.7203515084345726\n",
      "train loss:0.879339322225306\n",
      "train loss:0.9611059027880628\n",
      "train loss:0.9305154702969992\n",
      "train loss:0.8689492201157178\n",
      "train loss:1.0750602308637904\n",
      "train loss:0.9249938197308964\n",
      "train loss:0.8418261619643234\n",
      "train loss:0.8441383339567642\n",
      "train loss:0.9012599968503292\n",
      "train loss:1.0238716192948278\n",
      "train loss:0.8290117046384387\n",
      "train loss:0.860641291321892\n",
      "train loss:0.9754622612499233\n",
      "train loss:0.9220735641189468\n",
      "train loss:0.9509434068658911\n",
      "train loss:1.0185326769120884\n",
      "train loss:1.0520687258863541\n",
      "train loss:1.0458473219368027\n",
      "train loss:0.7712126608174771\n",
      "train loss:0.9949869191200834\n",
      "train loss:1.242149549019973\n",
      "train loss:0.8991747078820148\n",
      "train loss:0.9032710240276991\n",
      "train loss:1.0001078362607256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9232802877650053\n",
      "train loss:0.8886265099154635\n",
      "train loss:0.8658117450260592\n",
      "train loss:0.8096421458958788\n",
      "train loss:0.7215455416921249\n",
      "train loss:1.0520098009638936\n",
      "train loss:0.8925427705221959\n",
      "train loss:0.8307018757644689\n",
      "train loss:0.8374168227146498\n",
      "train loss:1.0040481793197344\n",
      "train loss:0.8884931861472648\n",
      "train loss:0.9399734032845696\n",
      "train loss:0.9791951412274851\n",
      "train loss:0.9915743565804489\n",
      "train loss:0.9293456268777411\n",
      "train loss:0.8520902598504598\n",
      "train loss:1.0545815742509688\n",
      "train loss:0.8862766288493984\n",
      "train loss:0.9026702982382807\n",
      "train loss:0.9822475733128723\n",
      "train loss:0.9590460685842586\n",
      "train loss:0.7948070967671677\n",
      "train loss:1.01144676882354\n",
      "train loss:0.8823725843936093\n",
      "train loss:0.9365550013408265\n",
      "train loss:0.8475757117593914\n",
      "train loss:1.1884234390918234\n",
      "train loss:1.0195152864740742\n",
      "train loss:0.9246061374243318\n",
      "train loss:0.9346093415506941\n",
      "train loss:0.8939032128345891\n",
      "train loss:1.0620651110381616\n",
      "train loss:0.8053822289596606\n",
      "train loss:1.1302277157356513\n",
      "train loss:0.8561905509035053\n",
      "train loss:0.9466608932504521\n",
      "train loss:0.9909776451672091\n",
      "train loss:0.8818645661044819\n",
      "train loss:0.8933941608641587\n",
      "train loss:0.9724560871970557\n",
      "train loss:1.095682368308752\n",
      "train loss:0.8337118085126087\n",
      "train loss:0.8814912978799146\n",
      "train loss:0.9869478272700871\n",
      "train loss:0.9330744510986021\n",
      "train loss:0.8049367678902314\n",
      "train loss:0.7545431326681477\n",
      "train loss:0.9938048118197653\n",
      "train loss:0.8199110444324293\n",
      "train loss:0.8785836639422737\n",
      "train loss:0.9390634282378759\n",
      "train loss:0.8791184531405969\n",
      "train loss:0.9741255240685007\n",
      "train loss:1.0213845428883757\n",
      "train loss:0.8409985686233226\n",
      "train loss:0.8050053362091838\n",
      "train loss:0.8881929456499569\n",
      "train loss:0.8032732022205645\n",
      "train loss:1.0005704149600576\n",
      "train loss:0.978511786289322\n",
      "train loss:0.7430010621760031\n",
      "train loss:0.9599849359098072\n",
      "train loss:0.8993309027322742\n",
      "train loss:0.922663637037412\n",
      "train loss:0.8092001086070002\n",
      "train loss:0.7036482888687118\n",
      "train loss:0.7548416811043606\n",
      "train loss:0.927871169579543\n",
      "train loss:0.9023737301098598\n",
      "train loss:0.9775393635629517\n",
      "train loss:1.0240348488216493\n",
      "train loss:0.9390960522854356\n",
      "train loss:1.0197757835314112\n",
      "train loss:0.8994495306152204\n",
      "train loss:0.9487827790414559\n",
      "train loss:0.9751518999858309\n",
      "train loss:1.0027928734870202\n",
      "train loss:1.0053611448289332\n",
      "train loss:0.9402347066605485\n",
      "train loss:0.8424113859050807\n",
      "train loss:0.8750021290154407\n",
      "train loss:0.9096114817171879\n",
      "train loss:0.8705930263061151\n",
      "train loss:0.827074149069173\n",
      "train loss:0.9651524087707825\n",
      "train loss:0.9030770475972727\n",
      "train loss:0.9456944318454069\n",
      "train loss:0.8819194138139338\n",
      "train loss:1.0231485184635367\n",
      "train loss:0.8673100518826107\n",
      "train loss:0.8858298253481522\n",
      "train loss:1.0059474785273408\n",
      "train loss:0.7403841407394475\n",
      "train loss:1.0659097543732323\n",
      "train loss:0.9092203190074677\n",
      "train loss:0.9987920431236114\n",
      "train loss:0.8432056943041956\n",
      "train loss:0.9998666483870157\n",
      "train loss:0.7704256255874408\n",
      "train loss:0.9622183880941164\n",
      "train loss:0.9493970797272108\n",
      "train loss:1.1771290651025017\n",
      "train loss:0.9309759898014041\n",
      "train loss:0.8886357196198351\n",
      "train loss:0.9229687544689119\n",
      "train loss:1.024556716249617\n",
      "train loss:0.9613408916289328\n",
      "train loss:0.8202070090033459\n",
      "train loss:0.9301558592745548\n",
      "train loss:0.942556304196363\n",
      "train loss:0.9178054560564293\n",
      "train loss:1.0389028965765277\n",
      "train loss:1.0148260390253023\n",
      "train loss:0.8822308872750324\n",
      "train loss:0.9889080689663504\n",
      "train loss:0.7094065015342248\n",
      "train loss:0.7944605972227302\n",
      "train loss:0.8036198603860758\n",
      "train loss:0.9853591200882024\n",
      "train loss:1.1588969543087932\n",
      "train loss:0.8898106506667799\n",
      "train loss:0.9194577864839973\n",
      "train loss:0.8802864035377961\n",
      "train loss:0.9844144390013638\n",
      "train loss:0.9173545834859677\n",
      "train loss:0.9074586904507163\n",
      "train loss:0.8797734990107583\n",
      "train loss:0.7623176241473034\n",
      "train loss:1.0592423458472893\n",
      "train loss:0.817722255997199\n",
      "train loss:0.8638405083041687\n",
      "train loss:1.1247090759332397\n",
      "train loss:1.0014176845652318\n",
      "train loss:1.024139837853786\n",
      "train loss:0.9735476044139318\n",
      "train loss:0.7472346330565844\n",
      "train loss:0.8902058333511704\n",
      "train loss:0.8444314607124727\n",
      "train loss:0.9929531520216812\n",
      "train loss:0.8971726413617135\n",
      "train loss:0.7770931682726914\n",
      "train loss:0.8470524234225608\n",
      "train loss:0.9615118034670097\n",
      "train loss:0.986437438361653\n",
      "train loss:0.8116044563420485\n",
      "train loss:0.969258185689834\n",
      "train loss:0.7426174702679197\n",
      "train loss:0.9622129272479448\n",
      "train loss:0.8635467646627262\n",
      "train loss:0.8856804087331751\n",
      "train loss:0.8898393707583064\n",
      "train loss:0.9974015870581318\n",
      "train loss:0.9299591797472476\n",
      "train loss:0.9849698610524104\n",
      "train loss:1.0011858217955927\n",
      "train loss:0.9962590977637159\n",
      "train loss:0.8764790158781537\n",
      "train loss:0.9010751725687548\n",
      "train loss:0.8956529838644341\n",
      "train loss:0.9209884382523664\n",
      "train loss:0.969942832326849\n",
      "train loss:0.8796345249717932\n",
      "train loss:0.9841215536384937\n",
      "train loss:0.9021814289568283\n",
      "train loss:0.8713487544212526\n",
      "train loss:0.8150512736386932\n",
      "train loss:0.9326083926062401\n",
      "train loss:1.0484091015227865\n",
      "train loss:1.0112152652778732\n",
      "train loss:1.034365761346573\n",
      "train loss:1.1483136706041386\n",
      "train loss:0.8814725174417832\n",
      "train loss:0.7934820741354675\n",
      "train loss:0.7997715428112688\n",
      "train loss:0.7938030365162108\n",
      "train loss:0.9150830944857177\n",
      "train loss:0.9988664031157196\n",
      "train loss:0.9922942864256136\n",
      "train loss:0.9620449420304448\n",
      "train loss:0.7326540675694883\n",
      "train loss:0.9165601491243427\n",
      "train loss:0.8271416204920372\n",
      "train loss:1.0312624023596553\n",
      "train loss:0.8112064344916999\n",
      "train loss:0.9366983213964885\n",
      "train loss:1.0258074643833484\n",
      "train loss:0.7787296766091795\n",
      "train loss:0.7083323094736739\n",
      "train loss:0.8776017200503897\n",
      "train loss:0.9544721117459578\n",
      "train loss:0.8662872651158879\n",
      "train loss:0.7446244216158802\n",
      "train loss:0.8640889454977286\n",
      "train loss:1.2152626972414493\n",
      "train loss:1.0049996044957048\n",
      "train loss:0.9555839238441399\n",
      "train loss:0.836298380016565\n",
      "train loss:0.9551827559555212\n",
      "train loss:0.8413162282972351\n",
      "train loss:1.0201992610049693\n",
      "train loss:0.7715383940931331\n",
      "train loss:0.9941005056045802\n",
      "train loss:1.004043669867621\n",
      "train loss:0.7557122380366276\n",
      "train loss:0.9395367059860392\n",
      "train loss:1.0310751109632426\n",
      "train loss:0.9076774026067589\n",
      "train loss:0.9172401034051895\n",
      "train loss:0.9841336667967877\n",
      "train loss:0.8717186750614846\n",
      "train loss:0.8541498847899149\n",
      "train loss:0.9029123780554295\n",
      "train loss:1.020683680420219\n",
      "train loss:0.938769775153877\n",
      "train loss:0.7824060978327756\n",
      "train loss:1.0426195175121753\n",
      "train loss:0.9142458041957413\n",
      "train loss:0.854021107969119\n",
      "train loss:0.7415539147792611\n",
      "train loss:0.8164857798217426\n",
      "train loss:0.8821636623762288\n",
      "train loss:0.7830753309972651\n",
      "train loss:0.8282242102414749\n",
      "train loss:0.8087406973063653\n",
      "train loss:1.0031241001200157\n",
      "train loss:0.8836786259129109\n",
      "train loss:0.9651567902471766\n",
      "train loss:0.942191544281392\n",
      "train loss:1.0689329430894947\n",
      "train loss:0.9145539754498643\n",
      "train loss:0.9342249839150172\n",
      "train loss:0.8355392714315875\n",
      "train loss:1.0718934381693688\n",
      "train loss:0.8411814676659602\n",
      "train loss:0.9899510515484623\n",
      "train loss:0.7518924196694862\n",
      "train loss:1.0907525214169647\n",
      "train loss:1.020572864481078\n",
      "train loss:0.9939613483870402\n",
      "train loss:1.001141438828077\n",
      "train loss:0.9357914429554467\n",
      "train loss:1.0029189850196136\n",
      "train loss:0.9788915277409028\n",
      "train loss:1.1485927163350704\n",
      "train loss:0.9200311566418036\n",
      "train loss:0.7866346242332517\n",
      "train loss:0.8051955369279562\n",
      "train loss:0.9039894729318323\n",
      "train loss:0.8958671153702769\n",
      "train loss:0.9242948615019568\n",
      "train loss:0.9824259534249979\n",
      "train loss:0.8514345405842846\n",
      "train loss:0.8840399949056181\n",
      "train loss:1.1314378500527444\n",
      "train loss:0.9020986118790962\n",
      "train loss:0.8848119438751151\n",
      "=== epoch:6, train acc:0.993, test acc:0.99 ===\n",
      "train loss:0.8400707911964992\n",
      "train loss:0.8135107381446253\n",
      "train loss:0.8826198926304744\n",
      "train loss:1.030474577579699\n",
      "train loss:0.8990131604586622\n",
      "train loss:0.9502871120136246\n",
      "train loss:0.8722471403987714\n",
      "train loss:0.9987841054232993\n",
      "train loss:0.8672734781177907\n",
      "train loss:1.0036124462429854\n",
      "train loss:0.9712703263143034\n",
      "train loss:0.8797214521436494\n",
      "train loss:0.8887024428911858\n",
      "train loss:0.9837831006895157\n",
      "train loss:0.8647442986994713\n",
      "train loss:0.9612454617918716\n",
      "train loss:0.7909377850622952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9371606459069883\n",
      "train loss:0.8894609652790814\n",
      "train loss:0.8423765907768674\n",
      "train loss:0.815081178359288\n",
      "train loss:0.9087547163806782\n",
      "train loss:1.068770548468797\n",
      "train loss:0.947167154606117\n",
      "train loss:0.8795711446604012\n",
      "train loss:0.8190892250061019\n",
      "train loss:0.9150642436458122\n",
      "train loss:1.045414158621415\n",
      "train loss:1.0413001549742034\n",
      "train loss:0.9960359675499383\n",
      "train loss:0.9998834423514499\n",
      "train loss:0.8706287251569\n",
      "train loss:0.9476147779148318\n",
      "train loss:0.9731571186228115\n",
      "train loss:0.8588913968548458\n",
      "train loss:0.8119686133985351\n",
      "train loss:0.912153427707363\n",
      "train loss:1.081667266079168\n",
      "train loss:0.8260881095497138\n",
      "train loss:0.8991354377553705\n",
      "train loss:0.884167117166142\n",
      "train loss:0.7582962292788353\n",
      "train loss:0.9796445995225388\n",
      "train loss:0.9204817593329547\n",
      "train loss:1.009443513186254\n",
      "train loss:0.8916187088643145\n",
      "train loss:0.9223282161165396\n",
      "train loss:0.7232953997570211\n",
      "train loss:1.0067326417532991\n",
      "train loss:0.8250482846192042\n",
      "train loss:0.816936796242471\n",
      "train loss:0.9486026798312309\n",
      "train loss:1.0021887717610722\n",
      "train loss:0.9310548204794583\n",
      "train loss:0.9810408255041686\n",
      "train loss:0.9902523689920393\n",
      "train loss:0.8429244255845246\n",
      "train loss:0.9263515461835345\n",
      "train loss:0.9516701168905377\n",
      "train loss:0.7314463472090473\n",
      "train loss:1.0755336685890078\n",
      "train loss:0.9618027603570068\n",
      "train loss:0.8999966796347936\n",
      "train loss:0.7826228146037502\n",
      "train loss:1.0274294821305388\n",
      "train loss:0.9715672343944491\n",
      "train loss:0.7868844745325895\n",
      "train loss:1.0493354559222041\n",
      "train loss:0.9737077878479968\n",
      "train loss:0.7310892609558904\n",
      "train loss:0.891621763208121\n",
      "train loss:0.8859572212400622\n",
      "train loss:0.827013661673754\n",
      "train loss:0.805972822203773\n",
      "train loss:0.9176716733513437\n",
      "train loss:1.1210476138215184\n",
      "train loss:0.8201362046514515\n",
      "train loss:0.88690348579231\n",
      "train loss:0.8054719889894397\n",
      "train loss:0.8795814536553435\n",
      "train loss:0.9371434601851623\n",
      "train loss:0.9022368051500793\n",
      "train loss:0.9036293509882322\n",
      "train loss:0.8768640024515304\n",
      "train loss:1.0018013165486637\n",
      "train loss:0.8924322056248457\n",
      "train loss:0.8567257735432223\n",
      "train loss:0.9254748604048599\n",
      "train loss:0.8322350112460982\n",
      "train loss:0.8442692966174755\n",
      "train loss:0.9448990920814181\n",
      "train loss:0.9117639648682021\n",
      "train loss:0.9670385666856446\n",
      "train loss:0.9299409897783744\n",
      "train loss:0.8595775352717682\n",
      "train loss:1.0067311985721803\n",
      "train loss:0.8875114066691369\n",
      "train loss:0.6946993363539165\n",
      "train loss:1.0181538414219506\n",
      "train loss:0.9738414308814682\n",
      "train loss:0.8643426243806949\n",
      "train loss:1.051088360383838\n",
      "train loss:0.80458317156641\n",
      "train loss:0.9498863942268655\n",
      "train loss:0.918316846520864\n",
      "train loss:0.9034552630154132\n",
      "train loss:0.8165086192788467\n",
      "train loss:0.9350726879414527\n",
      "train loss:0.9264951916761143\n",
      "train loss:1.02486945305291\n",
      "train loss:0.7195642291718077\n",
      "train loss:0.808148214044984\n",
      "train loss:0.904343016904334\n",
      "train loss:0.8038592307378939\n",
      "train loss:0.8479967641756798\n",
      "train loss:0.9459377090532106\n",
      "train loss:0.9400415438035009\n",
      "train loss:1.0042875739259456\n",
      "train loss:0.6914333104033769\n",
      "train loss:1.0051039202984726\n",
      "train loss:0.9020774058007975\n",
      "train loss:0.8752121837982041\n",
      "train loss:0.8379328736279337\n",
      "train loss:0.9823649993764395\n",
      "train loss:0.9356346261105165\n",
      "train loss:0.8967489934597972\n",
      "train loss:0.838183025388455\n",
      "train loss:1.0404616812632286\n",
      "train loss:0.9162006166329069\n",
      "train loss:0.8308547438036484\n",
      "train loss:0.9627701399933507\n",
      "train loss:0.8218800477715844\n",
      "train loss:0.9087644126752937\n",
      "train loss:0.9536807724507784\n",
      "train loss:1.0065949379406698\n",
      "train loss:0.7415670898417716\n",
      "train loss:0.8408651815551661\n",
      "train loss:0.9480487215540768\n",
      "train loss:0.9537856734879872\n",
      "train loss:0.9687714163482097\n",
      "train loss:0.898175621725938\n",
      "train loss:0.825813919832744\n",
      "train loss:0.8318163868322266\n",
      "train loss:1.0364651373483373\n",
      "train loss:0.7196555877006514\n",
      "train loss:0.8815379143497181\n",
      "train loss:0.9979101424522338\n",
      "train loss:0.9554484159932625\n",
      "train loss:0.8912985922209918\n",
      "train loss:0.9841803372365218\n",
      "train loss:0.8612711271856511\n",
      "train loss:1.024378068233771\n",
      "train loss:1.0189074352155734\n",
      "train loss:0.8201794520535618\n",
      "train loss:0.8982533971352179\n",
      "train loss:0.9171145634172854\n",
      "train loss:0.7603265799520774\n",
      "train loss:0.9308273308798496\n",
      "train loss:0.8625865442031249\n",
      "train loss:1.0223714446362815\n",
      "train loss:0.8332079876061238\n",
      "train loss:0.9057897577889641\n",
      "train loss:0.8179573343561977\n",
      "train loss:0.952155185811815\n",
      "train loss:0.7126056248690352\n",
      "train loss:0.959514638946126\n",
      "train loss:0.9765112007386918\n",
      "train loss:0.9807960430230962\n",
      "train loss:0.7499846503374732\n",
      "train loss:0.9505322885827299\n",
      "train loss:0.7279997170059472\n",
      "train loss:0.8894111639751137\n",
      "train loss:0.8622840293338103\n",
      "train loss:0.9717129251835958\n",
      "train loss:0.9222906076752995\n",
      "train loss:0.9636267416222966\n",
      "train loss:0.8495039814091091\n",
      "train loss:0.9626737038989752\n",
      "train loss:0.880976612395283\n",
      "train loss:0.8716403108881656\n",
      "train loss:0.9050011174509371\n",
      "train loss:0.8968358805831109\n",
      "train loss:0.809787893874075\n",
      "train loss:1.013342124090555\n",
      "train loss:0.8467535504027426\n",
      "train loss:0.849672432224271\n",
      "train loss:0.8866755251044636\n",
      "train loss:0.7677822594880785\n",
      "train loss:0.803875750297218\n",
      "train loss:0.7851215476422939\n",
      "train loss:1.0340046158351028\n",
      "train loss:0.9058323780132177\n",
      "train loss:0.9070851169405924\n",
      "train loss:0.9171240031408933\n",
      "train loss:1.0874550067278859\n",
      "train loss:0.9060474749148831\n",
      "train loss:0.9272422115560066\n",
      "train loss:0.8481597926191948\n",
      "train loss:0.9190803253403653\n",
      "train loss:0.9363992052470133\n",
      "train loss:0.8647995413845071\n",
      "train loss:0.8892129394307468\n",
      "train loss:0.9487109420011489\n",
      "train loss:0.8976508097955391\n",
      "train loss:0.8414506985849087\n",
      "train loss:0.9039056416643215\n",
      "train loss:0.8034363157944666\n",
      "train loss:0.8291541723874903\n",
      "train loss:0.8544602319734982\n",
      "train loss:0.9098919358281153\n",
      "train loss:0.8331653287616392\n",
      "train loss:0.7710071755853386\n",
      "train loss:0.8157524935358158\n",
      "train loss:0.7968294078851922\n",
      "train loss:0.8022184123119415\n",
      "train loss:0.8546849919100667\n",
      "train loss:0.8746252171087395\n",
      "train loss:0.9344102178664673\n",
      "train loss:0.9471389891978655\n",
      "train loss:0.94816467956883\n",
      "train loss:0.8241659146654214\n",
      "train loss:0.8956669177974047\n",
      "train loss:1.037282974309667\n",
      "train loss:0.8799216339325113\n",
      "train loss:0.9318386118177864\n",
      "train loss:0.8419914541892041\n",
      "train loss:0.9121558315837954\n",
      "train loss:0.9704917708619429\n",
      "train loss:0.8530227871125585\n",
      "train loss:1.040440331914896\n",
      "train loss:0.8721359568666002\n",
      "train loss:0.8730818027692631\n",
      "train loss:0.9163795405350004\n",
      "train loss:0.906231185448592\n",
      "train loss:0.7574324721584592\n",
      "train loss:0.918408030547436\n",
      "train loss:1.0066862761736513\n",
      "train loss:0.8581425008192218\n",
      "train loss:0.8715653708275426\n",
      "train loss:0.9566537380134262\n",
      "train loss:0.7561849737789451\n",
      "train loss:0.8059980755272202\n",
      "train loss:0.7921143416167462\n",
      "train loss:0.8167067058008832\n",
      "train loss:0.9215439479717802\n",
      "train loss:1.202032217813546\n",
      "train loss:0.8511631067935579\n",
      "train loss:1.0741082872785308\n",
      "train loss:0.988661094763904\n",
      "train loss:0.8779884207284363\n",
      "train loss:0.9001753043545091\n",
      "train loss:0.9719523613876919\n",
      "train loss:0.9568635754955455\n",
      "train loss:0.7885747068598669\n",
      "train loss:0.9842245794971347\n",
      "train loss:0.8662094904164944\n",
      "train loss:0.7921935000168294\n",
      "train loss:0.8253360946017207\n",
      "train loss:0.8806627309822016\n",
      "train loss:0.8401239298028246\n",
      "train loss:0.7594838245169889\n",
      "train loss:0.959629636808215\n",
      "train loss:0.9518338465583359\n",
      "train loss:0.9433591405694703\n",
      "train loss:0.7927605287660051\n",
      "train loss:0.8742104657393196\n",
      "train loss:0.923717970587489\n",
      "train loss:0.9680014647475148\n",
      "train loss:0.9704157594467766\n",
      "train loss:0.9303794480446386\n",
      "train loss:0.7892573407518053\n",
      "train loss:0.8101314273429289\n",
      "train loss:0.9215131312307041\n",
      "train loss:0.839123444079243\n",
      "train loss:0.8607397535260458\n",
      "train loss:0.9418515333608197\n",
      "train loss:0.7387376741485682\n",
      "train loss:0.9425293688847368\n",
      "train loss:0.8874927686872686\n",
      "train loss:0.9944598763945374\n",
      "train loss:0.8639399990015072\n",
      "train loss:0.8231161656771762\n",
      "train loss:0.8898740585277642\n",
      "train loss:0.9332652016971273\n",
      "train loss:0.8926189421305607\n",
      "train loss:0.9103124711836871\n",
      "train loss:0.9341669091638987\n",
      "train loss:0.8257898874019538\n",
      "train loss:0.941694817700163\n",
      "train loss:0.8913883594793037\n",
      "train loss:0.8570366294998518\n",
      "train loss:1.084361000458991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9936639408311025\n",
      "train loss:0.9675671077344912\n",
      "train loss:0.7784801997036205\n",
      "train loss:0.8520587952035811\n",
      "train loss:0.9579069851284417\n",
      "train loss:1.1113300536815118\n",
      "train loss:0.8462293369858587\n",
      "train loss:0.7719123798330565\n",
      "train loss:0.9123450116749803\n",
      "train loss:0.9080548201061245\n",
      "train loss:0.8765528785534491\n",
      "train loss:0.7603481025905797\n",
      "train loss:0.9399526133705521\n",
      "train loss:0.7773640300320577\n",
      "train loss:0.85796512059636\n",
      "train loss:0.819531526364359\n",
      "train loss:0.9315414087373918\n",
      "train loss:0.914692244177217\n",
      "train loss:0.8988705877302355\n",
      "train loss:0.8744206116329423\n",
      "train loss:0.8430766122713912\n",
      "train loss:0.8107257320408381\n",
      "train loss:0.7808776041401545\n",
      "train loss:0.7421222370634982\n",
      "train loss:1.0224989314581714\n",
      "train loss:0.8605796383353291\n",
      "train loss:0.9840899692671407\n",
      "train loss:0.8478345224787386\n",
      "train loss:1.0767190581393162\n",
      "train loss:0.9022279513470288\n",
      "train loss:0.8581929281535962\n",
      "train loss:1.0745224858411067\n",
      "train loss:0.8173227131032518\n",
      "train loss:0.9825679452563226\n",
      "train loss:0.8280451188142898\n",
      "train loss:0.9636164588214652\n",
      "train loss:0.8922905215403079\n",
      "train loss:0.9342675197858192\n",
      "train loss:0.7336830863282056\n",
      "train loss:0.9624376991097726\n",
      "train loss:0.8656699529854087\n",
      "train loss:0.9114430056026641\n",
      "train loss:1.2342753225951797\n",
      "train loss:0.6996962585900433\n",
      "train loss:1.0760693663318865\n",
      "train loss:0.8266047380532336\n",
      "train loss:0.9340742569203413\n",
      "train loss:0.9390223081856367\n",
      "train loss:1.0093261087401357\n",
      "train loss:0.8257934107045081\n",
      "train loss:0.9429325831305257\n",
      "train loss:0.9130528427425326\n",
      "train loss:0.806840171746972\n",
      "train loss:0.9133050601315139\n",
      "train loss:0.9170472140632097\n",
      "train loss:0.9717778908382811\n",
      "train loss:0.9511026239694235\n",
      "train loss:0.9200926989736954\n",
      "train loss:0.8569356113832848\n",
      "train loss:0.8660322777668115\n",
      "train loss:0.9113011195305616\n",
      "train loss:0.8948338940139606\n",
      "train loss:1.0013596682728583\n",
      "train loss:0.8926320090586678\n",
      "train loss:0.7436028479980792\n",
      "train loss:0.8377119830810308\n",
      "train loss:0.9604197443587783\n",
      "train loss:1.0785779334646421\n",
      "train loss:0.920986250856001\n",
      "train loss:0.951968273755781\n",
      "train loss:0.9175955713706895\n",
      "train loss:0.7174746111284422\n",
      "train loss:0.844790469220594\n",
      "train loss:0.8687225490264043\n",
      "train loss:0.9853719591140697\n",
      "train loss:0.923326431153919\n",
      "train loss:1.074361878696485\n",
      "train loss:0.798580443381438\n",
      "train loss:0.945682014809299\n",
      "train loss:0.8342318842120884\n",
      "train loss:0.9518395420253424\n",
      "train loss:0.9989530199293439\n",
      "train loss:0.8507605799235975\n",
      "train loss:0.9858577222499448\n",
      "train loss:0.9925019273236994\n",
      "train loss:0.8121618632046677\n",
      "train loss:0.7925535399666588\n",
      "train loss:0.9158166661830117\n",
      "train loss:0.9353369700496978\n",
      "train loss:0.8269203571546031\n",
      "train loss:0.8871515192652626\n",
      "train loss:0.8199667240454598\n",
      "train loss:0.7906469883425882\n",
      "train loss:0.8027938850610435\n",
      "train loss:0.7015658293597934\n",
      "train loss:0.8767858227717579\n",
      "train loss:0.894824092305193\n",
      "train loss:0.9829489315580306\n",
      "train loss:0.8224119682249641\n",
      "train loss:0.9439208891248847\n",
      "train loss:0.9572500572769365\n",
      "train loss:0.8096764023280535\n",
      "train loss:0.9099442918983647\n",
      "train loss:0.8274859678827237\n",
      "train loss:0.9243273726954019\n",
      "train loss:0.7829309964956566\n",
      "train loss:0.8813027411863602\n",
      "train loss:0.8878485896768461\n",
      "train loss:0.6910484563682178\n",
      "train loss:1.0303421521018254\n",
      "train loss:0.9549229636799805\n",
      "train loss:0.8923125160203995\n",
      "train loss:0.9494979449132838\n",
      "train loss:0.8737721240774681\n",
      "train loss:1.000442810746371\n",
      "train loss:0.7962351003853014\n",
      "train loss:0.8989230114889339\n",
      "train loss:1.006222600909843\n",
      "train loss:1.0430561573797026\n",
      "train loss:0.7392200310568843\n",
      "train loss:0.9228208092202356\n",
      "train loss:0.9667495460520632\n",
      "train loss:0.8109316702977375\n",
      "train loss:0.9966042350722543\n",
      "train loss:0.8896589191054922\n",
      "train loss:0.8661290983489492\n",
      "train loss:0.8560422453247578\n",
      "train loss:0.9671265261322637\n",
      "train loss:1.0068872473601345\n",
      "train loss:0.8018677415944154\n",
      "train loss:1.0835580867971375\n",
      "train loss:0.8222097325786286\n",
      "train loss:0.7720762502858753\n",
      "train loss:0.8396716273886909\n",
      "train loss:0.8471119588532461\n",
      "train loss:0.9437571673055025\n",
      "train loss:0.8905480104864257\n",
      "train loss:0.9224926672643019\n",
      "train loss:0.8214484632870943\n",
      "train loss:1.0891909057081686\n",
      "train loss:1.0308437814805016\n",
      "train loss:0.9612462523925777\n",
      "train loss:0.9231795898333109\n",
      "train loss:0.8224755829765743\n",
      "train loss:0.9713499568094132\n",
      "train loss:0.8210109128325844\n",
      "train loss:0.8891590387429349\n",
      "train loss:0.8858939546824256\n",
      "train loss:0.9557115553848544\n",
      "train loss:0.7724919570675456\n",
      "train loss:0.6535843308317673\n",
      "train loss:0.8593866334753102\n",
      "train loss:0.9675681202713285\n",
      "train loss:0.9538297445247411\n",
      "train loss:1.1543232054001304\n",
      "train loss:0.8987537332295846\n",
      "train loss:0.79306368156093\n",
      "train loss:0.9125529191619592\n",
      "train loss:1.007002167765186\n",
      "train loss:0.7833057710550553\n",
      "train loss:0.8992888886160337\n",
      "train loss:0.8144075660917689\n",
      "train loss:0.8110881209648951\n",
      "train loss:0.8230927570128453\n",
      "train loss:0.8576031229979739\n",
      "train loss:0.9933771195691246\n",
      "train loss:0.82802181769495\n",
      "train loss:0.9675587576461911\n",
      "train loss:0.8497370959812537\n",
      "train loss:1.0851254905364798\n",
      "train loss:0.8279050926761626\n",
      "train loss:1.0209625804975562\n",
      "train loss:1.01685333508362\n",
      "train loss:0.9265604173441404\n",
      "train loss:0.8738847062550639\n",
      "train loss:0.9907635397817569\n",
      "train loss:1.0107012035134733\n",
      "train loss:0.8738770582908303\n",
      "train loss:0.8156161198945704\n",
      "train loss:0.8854174822200794\n",
      "train loss:0.98348861854537\n",
      "train loss:0.8744952228008681\n",
      "train loss:0.7779858708497206\n",
      "train loss:0.767471602516591\n",
      "train loss:1.1198718495413096\n",
      "train loss:1.0011566339315483\n",
      "train loss:0.8157099502925229\n",
      "train loss:0.8309895047852135\n",
      "train loss:0.8425333050814678\n",
      "train loss:0.946134959480442\n",
      "train loss:0.8012917057686536\n",
      "train loss:0.8396381389864379\n",
      "train loss:0.855822918606431\n",
      "train loss:1.0851058929987176\n",
      "train loss:0.87886394603917\n",
      "train loss:0.9513214219436275\n",
      "train loss:1.0009440718880855\n",
      "train loss:0.7865446031973998\n",
      "train loss:0.8843524216108196\n",
      "train loss:0.9547257569180758\n",
      "train loss:1.0686698519530713\n",
      "train loss:0.8474683861331607\n",
      "train loss:0.9129827691886327\n",
      "train loss:0.8042022303757911\n",
      "train loss:0.9237257339877707\n",
      "train loss:0.7712164423130584\n",
      "train loss:0.9762547057800302\n",
      "train loss:0.8618950523828246\n",
      "train loss:0.9325997897075172\n",
      "train loss:0.9760229131361907\n",
      "train loss:0.9370567502880186\n",
      "train loss:0.8635431030228627\n",
      "train loss:0.890127062943241\n",
      "train loss:0.9230705754788382\n",
      "train loss:0.9193953946357902\n",
      "train loss:0.8494123088622945\n",
      "train loss:0.7928407154819708\n",
      "train loss:1.0065927390596798\n",
      "train loss:0.9130514830781604\n",
      "train loss:0.8931825321523164\n",
      "train loss:0.8575016562454296\n",
      "train loss:0.8510645605789382\n",
      "train loss:0.9688473200252843\n",
      "train loss:0.812074056154474\n",
      "train loss:0.9105859268133681\n",
      "train loss:0.9059309579857476\n",
      "train loss:1.0280886274477792\n",
      "train loss:0.9348323503467038\n",
      "train loss:0.8590431563263909\n",
      "train loss:0.8453142258087094\n",
      "train loss:0.8204131196675408\n",
      "train loss:0.9134543631202883\n",
      "train loss:0.8594876340798371\n",
      "train loss:0.8205926692753568\n",
      "train loss:1.0457165189479867\n",
      "train loss:0.7848515285807032\n",
      "train loss:0.9296307817714192\n",
      "train loss:0.7793798275283443\n",
      "train loss:0.8999160506400103\n",
      "train loss:0.8136185691318916\n",
      "train loss:1.0683680501658745\n",
      "train loss:0.8128204502166086\n",
      "train loss:0.9074372925405183\n",
      "train loss:0.826550367674415\n",
      "train loss:0.9885091327893434\n",
      "train loss:1.0481367366105308\n",
      "train loss:0.7255516258551115\n",
      "train loss:0.90405448198775\n",
      "train loss:1.0821353733395649\n",
      "train loss:0.8144890220742518\n",
      "train loss:0.8375229812789583\n",
      "train loss:1.0257839700298186\n",
      "train loss:0.8198146714976606\n",
      "train loss:0.9397088767337616\n",
      "train loss:0.911500116618883\n",
      "train loss:0.8948753381580598\n",
      "train loss:0.9680834750427632\n",
      "train loss:1.086765149410224\n",
      "train loss:0.7619237800661675\n",
      "train loss:1.0674666608496854\n",
      "train loss:0.9178592958622325\n",
      "train loss:0.9360993525149692\n",
      "train loss:0.88425130457242\n",
      "train loss:0.9250757154648336\n",
      "train loss:0.8470510281107309\n",
      "train loss:0.8682886289145046\n",
      "train loss:0.8773985297685997\n",
      "train loss:1.0872104629119692\n",
      "train loss:0.9119272993744191\n",
      "train loss:0.9588735921722938\n",
      "train loss:0.8736050225300143\n",
      "train loss:1.038540548600301\n",
      "train loss:0.8557316695671808\n",
      "train loss:0.8206847434656923\n",
      "train loss:1.021353399510927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8101288783667009\n",
      "train loss:0.7682405665427865\n",
      "train loss:0.8133797935923522\n",
      "train loss:0.8351270075184333\n",
      "train loss:0.9462710693225947\n",
      "train loss:0.9431802540045865\n",
      "train loss:0.9366654764155029\n",
      "train loss:0.808436821456649\n",
      "train loss:1.0306101516181236\n",
      "train loss:0.9977117701870735\n",
      "train loss:0.9997190144761984\n",
      "train loss:0.7544597077502122\n",
      "train loss:0.8385751070935044\n",
      "train loss:0.8547700618147186\n",
      "train loss:0.8920496117937582\n",
      "train loss:0.8893268050760597\n",
      "train loss:0.8938306048969682\n",
      "train loss:0.7719608148424316\n",
      "train loss:0.9075916809588815\n",
      "train loss:0.8930436121117696\n",
      "train loss:0.836957036562507\n",
      "train loss:0.8234690914981997\n",
      "train loss:1.056107830659802\n",
      "train loss:0.8225739557195386\n",
      "train loss:0.9670786435260093\n",
      "train loss:0.9104557225313065\n",
      "train loss:0.8104174502634202\n",
      "train loss:1.050566880729475\n",
      "train loss:0.884923804839475\n",
      "train loss:0.900360993290585\n",
      "train loss:0.8122864889315806\n",
      "train loss:0.7864854177566875\n",
      "train loss:0.8096766408592088\n",
      "=== epoch:7, train acc:0.99, test acc:0.988 ===\n",
      "train loss:0.855539821289241\n",
      "train loss:0.9721768731702405\n",
      "train loss:0.8754428703910392\n",
      "train loss:1.0219286457514267\n",
      "train loss:0.7539327875038704\n",
      "train loss:0.8156558901036113\n",
      "train loss:0.9227760719040161\n",
      "train loss:0.8848459721088815\n",
      "train loss:0.8058405933473726\n",
      "train loss:0.8247093056036303\n",
      "train loss:0.888168805018405\n",
      "train loss:0.9878520100507596\n",
      "train loss:0.770637882467046\n",
      "train loss:0.8900691284561375\n",
      "train loss:0.838956640461206\n",
      "train loss:0.9631124734278015\n",
      "train loss:0.922673530825326\n",
      "train loss:0.9702047857794159\n",
      "train loss:0.8310065168711066\n",
      "train loss:0.9007349440196212\n",
      "train loss:0.9551874627286882\n",
      "train loss:0.7503051987329741\n",
      "train loss:0.9878155795271547\n",
      "train loss:0.8569886518154565\n",
      "train loss:0.9250294893918481\n",
      "train loss:0.8508474743172024\n",
      "train loss:0.982551462544908\n",
      "train loss:0.9327352333929038\n",
      "train loss:0.9062524073044326\n",
      "train loss:0.9634336785180411\n",
      "train loss:0.7638475226821589\n",
      "train loss:0.8682181485785769\n",
      "train loss:0.8988897711879532\n",
      "train loss:0.8098836532874509\n",
      "train loss:0.7022917970142488\n",
      "train loss:0.929829617122451\n",
      "train loss:0.7948771604971023\n",
      "train loss:0.9425998151276379\n",
      "train loss:0.86392403201044\n",
      "train loss:0.8736653141661013\n",
      "train loss:0.9697259889123423\n",
      "train loss:0.9040566075693153\n",
      "train loss:0.9280340024969405\n",
      "train loss:1.0619936684616638\n",
      "train loss:0.9288735767630223\n",
      "train loss:1.039568573994908\n",
      "train loss:0.8159147339343609\n",
      "train loss:0.8144128998236512\n",
      "train loss:0.9327878056367348\n",
      "train loss:0.8867708150413456\n",
      "train loss:0.9790915300822608\n",
      "train loss:0.9743293224230172\n",
      "train loss:0.9332804790495254\n",
      "train loss:0.8500611333757232\n",
      "train loss:0.986518849361091\n",
      "train loss:1.0487110675262192\n",
      "train loss:0.71613463757844\n",
      "train loss:0.9447128672691599\n",
      "train loss:0.9415971166625948\n",
      "train loss:0.8929013709098137\n",
      "train loss:0.9452387150767592\n",
      "train loss:0.864023635243577\n",
      "train loss:1.0205150929610418\n",
      "train loss:0.8154002772439537\n",
      "train loss:0.9739712125121087\n",
      "train loss:0.8700051949271035\n",
      "train loss:0.9758516256841828\n",
      "train loss:0.8152919386871429\n",
      "train loss:0.98328248900175\n",
      "train loss:0.9961198801230214\n",
      "train loss:0.9851675943328446\n",
      "train loss:0.9194990673155016\n",
      "train loss:0.8239567626888877\n",
      "train loss:0.835388933640034\n",
      "train loss:0.9574154991768001\n",
      "train loss:0.904095058416616\n",
      "train loss:1.0199938031193003\n",
      "train loss:0.8988406765557703\n",
      "train loss:0.7499280384048125\n",
      "train loss:0.9404516391440028\n",
      "train loss:0.8994789190458728\n",
      "train loss:1.1008153909715717\n",
      "train loss:0.8169288406086744\n",
      "train loss:0.7743016943540014\n",
      "train loss:0.9046117136382382\n",
      "train loss:1.0571305875075891\n",
      "train loss:0.9783366636789195\n",
      "train loss:0.7199588422064329\n",
      "train loss:0.9266487948984953\n",
      "train loss:0.7705754279243137\n",
      "train loss:0.8122503848203736\n",
      "train loss:0.8494108218818508\n",
      "train loss:0.9057225261262976\n",
      "train loss:0.7627464308475276\n",
      "train loss:0.996708936660112\n",
      "train loss:0.9472416039546233\n",
      "train loss:0.987361851405499\n",
      "train loss:0.8894287202692388\n",
      "train loss:0.8505286737393957\n",
      "train loss:0.8503167006230099\n",
      "train loss:0.8621790373304877\n",
      "train loss:0.9654543461231104\n",
      "train loss:1.0523091696487776\n",
      "train loss:0.9534516755907981\n",
      "train loss:0.9002386806649995\n",
      "train loss:0.8789922225004377\n",
      "train loss:0.9416424572490395\n",
      "train loss:0.8249850897468449\n",
      "train loss:0.8163933689252971\n",
      "train loss:0.9508064029074861\n",
      "train loss:1.054703702917777\n",
      "train loss:0.72333088568564\n",
      "train loss:0.9230083337366674\n",
      "train loss:0.826175886055507\n",
      "train loss:0.7474033890724331\n",
      "train loss:0.9262396430944929\n",
      "train loss:0.8133832562819646\n",
      "train loss:0.8231254359654769\n",
      "train loss:0.6905732666174939\n",
      "train loss:0.8777541664207429\n",
      "train loss:0.9315889766217762\n",
      "train loss:0.9682492372361114\n",
      "train loss:0.9778423729301601\n",
      "train loss:0.8060883979824386\n",
      "train loss:0.9586771640707292\n",
      "train loss:0.8937095820798392\n",
      "train loss:0.7853328671251945\n",
      "train loss:0.8082447338772706\n",
      "train loss:0.9802654223206371\n",
      "train loss:0.8926758847335917\n",
      "train loss:0.9625284314502619\n",
      "train loss:0.908388951637493\n",
      "train loss:0.6625184689505984\n",
      "train loss:0.9080289851466413\n",
      "train loss:0.9975181241618647\n",
      "train loss:0.7206761492014023\n",
      "train loss:0.909337508320184\n",
      "train loss:0.7254981022750351\n",
      "train loss:0.9258875179631069\n",
      "train loss:1.0844003589822948\n",
      "train loss:1.1302021123034611\n",
      "train loss:0.9597780630326852\n",
      "train loss:0.9367848687554102\n",
      "train loss:0.7099902954068956\n",
      "train loss:0.9604800078721246\n",
      "train loss:1.0214912375720655\n",
      "train loss:0.8086258015969094\n",
      "train loss:1.0099909769804527\n",
      "train loss:0.7255973284731145\n",
      "train loss:0.9941402928653053\n",
      "train loss:0.7594399374548163\n",
      "train loss:0.8489883908486486\n",
      "train loss:0.8022573115185974\n",
      "train loss:0.9555429750322388\n",
      "train loss:0.8946790453691397\n",
      "train loss:0.9044432151425417\n",
      "train loss:0.8507110446718816\n",
      "train loss:1.030967713185835\n",
      "train loss:0.7050561664279702\n",
      "train loss:0.8032156600431479\n",
      "train loss:0.8408507790101791\n",
      "train loss:1.0412540104892232\n",
      "train loss:0.870259717593773\n",
      "train loss:1.0016487722003224\n",
      "train loss:0.9688214260753587\n",
      "train loss:0.8589631382534912\n",
      "train loss:0.9129519982643751\n",
      "train loss:1.0033418366266469\n",
      "train loss:0.8883762209241408\n",
      "train loss:0.904743727328157\n",
      "train loss:0.7836988561893206\n",
      "train loss:0.8399103532016323\n",
      "train loss:0.8967732653789414\n",
      "train loss:0.9992336947691571\n",
      "train loss:0.7668127321467929\n",
      "train loss:0.9129753188935866\n",
      "train loss:0.9953873189567594\n",
      "train loss:0.9098819676925018\n",
      "train loss:0.8728598248527482\n",
      "train loss:0.9193418479995935\n",
      "train loss:0.8517289354051998\n",
      "train loss:0.7800924327001365\n",
      "train loss:1.0580993705724104\n",
      "train loss:0.9453801441677905\n",
      "train loss:0.9219732100877842\n",
      "train loss:0.9559689060290844\n",
      "train loss:0.7997454107427718\n",
      "train loss:0.976626641483521\n",
      "train loss:0.9141452425552569\n",
      "train loss:0.8375104163331508\n",
      "train loss:0.8653507554936634\n",
      "train loss:0.8587708599109949\n",
      "train loss:0.837360372710146\n",
      "train loss:0.7998022588223536\n",
      "train loss:0.8863961980153854\n",
      "train loss:0.8047771969926926\n",
      "train loss:0.7832051135938061\n",
      "train loss:0.872035908653994\n",
      "train loss:0.8964536320972847\n",
      "train loss:0.8854924326794188\n",
      "train loss:0.8780404169986089\n",
      "train loss:0.8606837282364569\n",
      "train loss:0.9287908220336277\n",
      "train loss:0.8081486605570402\n",
      "train loss:1.0802396089665636\n",
      "train loss:1.065012831810289\n",
      "train loss:0.7804776819207575\n",
      "train loss:0.853724828666021\n",
      "train loss:0.9555447997440843\n",
      "train loss:0.9556222498962121\n",
      "train loss:0.8290413774598295\n",
      "train loss:0.8704105056648349\n",
      "train loss:0.8521823469254254\n",
      "train loss:0.8333347157624741\n",
      "train loss:0.7299435204686447\n",
      "train loss:0.7846831919171282\n",
      "train loss:0.9605126260555025\n",
      "train loss:1.021251719882067\n",
      "train loss:0.9321092666570414\n",
      "train loss:0.8906821262268518\n",
      "train loss:1.0324893911900548\n",
      "train loss:0.9832909780992197\n",
      "train loss:0.6308921299996103\n",
      "train loss:0.8002308623730369\n",
      "train loss:0.9209620314820086\n",
      "train loss:0.9083201306341383\n",
      "train loss:0.7873105154386054\n",
      "train loss:0.7717105929840647\n",
      "train loss:0.9354111451269144\n",
      "train loss:0.8802579523577815\n",
      "train loss:0.9137672440942392\n",
      "train loss:1.2021097835908519\n",
      "train loss:0.8856589826227786\n",
      "train loss:0.8260647538535182\n",
      "train loss:0.8934470042011681\n",
      "train loss:0.988600769897715\n",
      "train loss:0.9859325624212909\n",
      "train loss:1.0348728782347045\n",
      "train loss:0.9078858050633448\n",
      "train loss:0.883264365660683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8292265291984804\n",
      "train loss:0.7642799508248248\n",
      "train loss:1.0338569870961447\n",
      "train loss:0.8279094654353107\n",
      "train loss:0.7322235932913401\n",
      "train loss:0.7817670479848914\n",
      "train loss:0.945824628503277\n",
      "train loss:0.8373094824370795\n",
      "train loss:0.8438990293697153\n",
      "train loss:0.7549019838608886\n",
      "train loss:0.8348854832501417\n",
      "train loss:1.0687926186317152\n",
      "train loss:0.9367682152790571\n",
      "train loss:0.9891455538090977\n",
      "train loss:0.8746823046682518\n",
      "train loss:0.698062940827601\n",
      "train loss:1.0139936405434773\n",
      "train loss:0.7507698222562413\n",
      "train loss:0.9615571174038547\n",
      "train loss:0.8004891311306056\n",
      "train loss:0.8365982401847504\n",
      "train loss:0.8475417167256389\n",
      "train loss:0.7509142941139004\n",
      "train loss:0.870582109974356\n",
      "train loss:0.8649116985600982\n",
      "train loss:0.8648075179296972\n",
      "train loss:0.8621761498670928\n",
      "train loss:0.9249502330672774\n",
      "train loss:0.9171566845850784\n",
      "train loss:0.9402891957831244\n",
      "train loss:0.8303919916209673\n",
      "train loss:0.748280413534359\n",
      "train loss:1.1693248215604175\n",
      "train loss:0.8404473489106146\n",
      "train loss:0.824629404578826\n",
      "train loss:0.8387627853811048\n",
      "train loss:0.808882308637388\n",
      "train loss:0.8342998717447706\n",
      "train loss:0.7964386984657666\n",
      "train loss:0.8185538308160156\n",
      "train loss:0.7886075350134836\n",
      "train loss:0.8662503018093004\n",
      "train loss:0.9035051044103164\n",
      "train loss:0.7929057231791065\n",
      "train loss:0.8597609229995746\n",
      "train loss:0.8788635637798292\n",
      "train loss:0.8102994682472479\n",
      "train loss:0.9869712766794727\n",
      "train loss:1.078919052498186\n",
      "train loss:0.9136470809189217\n",
      "train loss:0.9296701354298048\n",
      "train loss:0.7778318276175393\n",
      "train loss:0.8546965460502334\n",
      "train loss:0.7175663594028562\n",
      "train loss:0.8880391905301132\n",
      "train loss:0.9219797301444156\n",
      "train loss:0.8931779583873134\n",
      "train loss:0.9510186352843545\n",
      "train loss:0.983476545093616\n",
      "train loss:0.9039953316189414\n",
      "train loss:0.8599114104981764\n",
      "train loss:0.9331927912060758\n",
      "train loss:0.9008890150550957\n",
      "train loss:0.9024383640762054\n",
      "train loss:0.9009114003851058\n",
      "train loss:0.9055320596083289\n",
      "train loss:0.8787418310268759\n",
      "train loss:0.8440846831377585\n",
      "train loss:0.7929603027536742\n",
      "train loss:0.8336224413881242\n",
      "train loss:0.9565033921454527\n",
      "train loss:0.9057924406105664\n",
      "train loss:1.0226566537611952\n",
      "train loss:0.8260111044746945\n",
      "train loss:0.831869865984756\n",
      "train loss:0.7764505228377416\n",
      "train loss:0.9461229744905101\n",
      "train loss:0.7902204084237896\n",
      "train loss:1.0350063477663487\n",
      "train loss:0.8901812750491536\n",
      "train loss:0.8183694089164689\n",
      "train loss:0.8378512317473246\n",
      "train loss:0.8135656965686114\n",
      "train loss:0.8582392937905893\n",
      "train loss:0.855814814782861\n",
      "train loss:0.8249844620613034\n",
      "train loss:0.8349238341513838\n",
      "train loss:0.8768494575143643\n",
      "train loss:0.9793009627895628\n",
      "train loss:0.846706720776565\n",
      "train loss:0.9060249075669176\n",
      "train loss:0.943797941805349\n",
      "train loss:0.9708001562032631\n",
      "train loss:0.9190952405752759\n",
      "train loss:0.8884107088645681\n",
      "train loss:1.213099419369517\n",
      "train loss:0.8324549921769362\n",
      "train loss:0.9480783334137208\n",
      "train loss:0.8319878460668968\n",
      "train loss:0.947212202475963\n",
      "train loss:0.7766740813676953\n",
      "train loss:0.8116272240878127\n",
      "train loss:0.8405030649801905\n",
      "train loss:0.8877978033245156\n",
      "train loss:0.8919361637724929\n",
      "train loss:0.7884779855370713\n",
      "train loss:0.9336249919119368\n",
      "train loss:0.9086161433706873\n",
      "train loss:0.862091525563217\n",
      "train loss:1.0305113711977594\n",
      "train loss:0.7396815733030349\n",
      "train loss:0.9905553728006317\n",
      "train loss:0.9261682191965569\n",
      "train loss:0.9844976996898502\n",
      "train loss:0.9456923177506228\n",
      "train loss:0.9104226580411975\n",
      "train loss:0.8620683571504411\n",
      "train loss:0.9777920659811942\n",
      "train loss:0.9521269373215698\n",
      "train loss:0.833844648275611\n",
      "train loss:0.8009673916742963\n",
      "train loss:0.8917193063252854\n",
      "train loss:0.8973773027333186\n",
      "train loss:0.7997766996172867\n",
      "train loss:0.8489513421942079\n",
      "train loss:0.8930565735097074\n",
      "train loss:0.9298574917617839\n",
      "train loss:0.879292722354308\n",
      "train loss:1.066823635176065\n",
      "train loss:0.7776751731988952\n",
      "train loss:0.8616353642119117\n",
      "train loss:0.8726610049188587\n",
      "train loss:0.6995445456019332\n",
      "train loss:0.8597711657078172\n",
      "train loss:0.9162907265232786\n",
      "train loss:0.968571897573502\n",
      "train loss:0.8772773458588334\n",
      "train loss:0.9241497377186638\n",
      "train loss:0.7455448719249057\n",
      "train loss:0.6933714990853715\n",
      "train loss:0.9859930854171148\n",
      "train loss:0.8666252005014543\n",
      "train loss:0.9112342265141861\n",
      "train loss:0.8625279185853304\n",
      "train loss:0.9883829369798246\n",
      "train loss:0.7358769372240467\n",
      "train loss:1.005071786712847\n",
      "train loss:0.9859654683332316\n",
      "train loss:0.9182432248175074\n",
      "train loss:0.8122858512608679\n",
      "train loss:0.7962598443635569\n",
      "train loss:0.8576812364129333\n",
      "train loss:0.7589658978036777\n",
      "train loss:0.7877223059310654\n",
      "train loss:0.7446098931690531\n",
      "train loss:0.946157106437454\n",
      "train loss:0.9015941749143016\n",
      "train loss:1.0443018240287243\n",
      "train loss:1.0408628653249787\n",
      "train loss:0.7733648223765646\n",
      "train loss:0.8062042321079352\n",
      "train loss:0.9407492087366713\n",
      "train loss:0.8206068386823735\n",
      "train loss:0.8145085438226178\n",
      "train loss:0.7503376385954132\n",
      "train loss:0.8594694768121991\n",
      "train loss:0.9278161361215987\n",
      "train loss:0.7812592109774108\n",
      "train loss:1.0747947990462658\n",
      "train loss:0.9375926710254547\n",
      "train loss:0.9795400031255309\n",
      "train loss:0.8999581058010407\n",
      "train loss:0.8581678391886406\n",
      "train loss:0.9173465838637825\n",
      "train loss:0.8763675687474197\n",
      "train loss:0.8534805405171347\n",
      "train loss:0.8664373683909392\n",
      "train loss:0.8315830480715107\n",
      "train loss:0.9188801730928092\n",
      "train loss:1.1474353444397294\n",
      "train loss:0.9471617678854787\n",
      "train loss:0.9021229566865236\n",
      "train loss:0.9497965485578165\n",
      "train loss:0.8929287171961304\n",
      "train loss:0.8794691073749893\n",
      "train loss:0.8611074559067113\n",
      "train loss:0.6552589619042896\n",
      "train loss:0.8445690505504132\n",
      "train loss:0.938475960048563\n",
      "train loss:0.9496938787973315\n",
      "train loss:0.8389316490490054\n",
      "train loss:0.8603197698055379\n",
      "train loss:0.872524062942093\n",
      "train loss:0.9676510487915461\n",
      "train loss:0.7501229132071567\n",
      "train loss:1.0887248999053307\n",
      "train loss:0.8607776151903408\n",
      "train loss:0.8455876742934706\n",
      "train loss:0.9417578306584111\n",
      "train loss:0.913068074165759\n",
      "train loss:0.906561524603039\n",
      "train loss:1.0036298489755677\n",
      "train loss:0.8119878213545237\n",
      "train loss:0.8211378286374789\n",
      "train loss:1.0512613370769484\n",
      "train loss:0.8875533165268332\n",
      "train loss:0.8908727082036978\n",
      "train loss:0.908261918646367\n",
      "train loss:0.6968739558978619\n",
      "train loss:0.8061869596753478\n",
      "train loss:0.9774662685244467\n",
      "train loss:0.9075688047729917\n",
      "train loss:0.9227024576128982\n",
      "train loss:0.8145604967188735\n",
      "train loss:0.8979564401862149\n",
      "train loss:1.0285950877671661\n",
      "train loss:0.9206758075942002\n",
      "train loss:0.8175273274949352\n",
      "train loss:0.9513030555409593\n",
      "train loss:0.9382788693810883\n",
      "train loss:0.9760105181995177\n",
      "train loss:0.9923720047464094\n",
      "train loss:0.8809159929472478\n",
      "train loss:0.9731478478773277\n",
      "train loss:0.6341922204478413\n",
      "train loss:0.874800996769521\n",
      "train loss:0.8950598333536257\n",
      "train loss:0.9067573555732816\n",
      "train loss:0.8351135421114728\n",
      "train loss:0.8499728120792954\n",
      "train loss:0.6710196484371687\n",
      "train loss:0.8838400424455344\n",
      "train loss:0.8206792708040029\n",
      "train loss:0.8563516788080072\n",
      "train loss:0.7958193922077352\n",
      "train loss:0.8820232788861692\n",
      "train loss:0.8825039470527146\n",
      "train loss:0.8711048631117374\n",
      "train loss:0.8538290635100517\n",
      "train loss:0.8928159451370762\n",
      "train loss:0.8199544032240297\n",
      "train loss:0.6580254109176006\n",
      "train loss:0.7205889899722605\n",
      "train loss:0.8899650664150701\n",
      "train loss:0.7791717460771141\n",
      "train loss:0.9241286618614011\n",
      "train loss:0.7988866638209459\n",
      "train loss:0.8876528678437875\n",
      "train loss:0.9345257233088184\n",
      "train loss:0.8608305099629758\n",
      "train loss:0.7598787810974172\n",
      "train loss:0.8468519451602221\n",
      "train loss:0.7458734159758403\n",
      "train loss:0.9307693895405815\n",
      "train loss:0.6918487086920837\n",
      "train loss:0.8387987650363242\n",
      "train loss:0.7640515547953234\n",
      "train loss:0.8082600083869015\n",
      "train loss:0.8812704437615207\n",
      "train loss:0.8260169511780708\n",
      "train loss:0.8771338756280231\n",
      "train loss:1.088419280524462\n",
      "train loss:0.7995467888763298\n",
      "train loss:0.9232439583290508\n",
      "train loss:0.7769135779558651\n",
      "train loss:0.777029129566465\n",
      "train loss:0.9546396467499434\n",
      "train loss:0.893568337998511\n",
      "train loss:0.9777498876205594\n",
      "train loss:0.8839417232227528\n",
      "train loss:0.7515161231875739\n",
      "train loss:0.8479742672827242\n",
      "train loss:0.8996783668638352\n",
      "train loss:1.010724879002033\n",
      "train loss:0.7738527913604771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8340784602251792\n",
      "train loss:1.0030865756562124\n",
      "train loss:0.8121195072795528\n",
      "train loss:0.8307701670488338\n",
      "train loss:0.7206477392280454\n",
      "train loss:0.7305966191918059\n",
      "train loss:0.9351847262289441\n",
      "train loss:0.8988365905336038\n",
      "train loss:0.9462656118792286\n",
      "train loss:0.8936841048923018\n",
      "train loss:0.7619919965558607\n",
      "train loss:0.8601483407495345\n",
      "train loss:0.8959708031132914\n",
      "train loss:0.8462978468779745\n",
      "train loss:0.8716280837002663\n",
      "train loss:0.6456386009787097\n",
      "train loss:0.8811110195549563\n",
      "train loss:0.8849280173316789\n",
      "train loss:0.821392097230899\n",
      "train loss:0.9165898664176464\n",
      "train loss:1.2419019344240607\n",
      "train loss:0.8868699502772373\n",
      "train loss:0.9013521972273862\n",
      "train loss:0.8981864061332671\n",
      "train loss:0.8775403829724528\n",
      "train loss:1.1391572789196514\n",
      "train loss:0.7637964661795438\n",
      "train loss:1.248886036189688\n",
      "train loss:0.8529923704384466\n",
      "train loss:0.927008236297148\n",
      "train loss:0.9725890316443581\n",
      "train loss:0.7755190593857413\n",
      "train loss:0.7835275144982963\n",
      "train loss:0.8460689143799109\n",
      "train loss:0.8274360584412223\n",
      "train loss:0.6095121805134556\n",
      "train loss:0.7461321214890053\n",
      "train loss:0.8343406395285894\n",
      "train loss:1.0015666515539432\n",
      "train loss:0.8741135000446755\n",
      "train loss:0.7833562724786484\n",
      "train loss:1.057381023200785\n",
      "train loss:0.8950457575769666\n",
      "train loss:0.8969480759271424\n",
      "train loss:0.7821809228974926\n",
      "train loss:1.0074260718051515\n",
      "train loss:0.9374948595151382\n",
      "train loss:0.8457143750196612\n",
      "train loss:0.8942152108352457\n",
      "train loss:0.8564786149322675\n",
      "train loss:0.7851211357555111\n",
      "train loss:0.8647918100987388\n",
      "train loss:1.0912685400321076\n",
      "train loss:0.9351220507272942\n",
      "train loss:0.7042618693125877\n",
      "train loss:0.8070640172372592\n",
      "train loss:0.7146338945414515\n",
      "train loss:0.8234242937130162\n",
      "train loss:1.123638197451444\n",
      "train loss:0.8599942423072175\n",
      "train loss:0.7938244098351739\n",
      "train loss:0.650792677711639\n",
      "train loss:0.9522410759126718\n",
      "train loss:0.8940677318014387\n",
      "train loss:0.9167318325853863\n",
      "train loss:0.8847458315911156\n",
      "train loss:0.9158303191592576\n",
      "train loss:0.9160962866408245\n",
      "train loss:0.8475769252398381\n",
      "train loss:0.8449098035994783\n",
      "train loss:1.0315463477278803\n",
      "train loss:1.0763773075070986\n",
      "train loss:0.729320999729551\n",
      "train loss:0.8000065300029396\n",
      "train loss:0.8329880005729876\n",
      "train loss:0.8747784790357004\n",
      "train loss:0.8905934041553629\n",
      "train loss:0.8489956882893762\n",
      "train loss:0.9020993067148682\n",
      "train loss:0.8712036636385534\n",
      "train loss:0.9197770778249433\n",
      "train loss:0.8903425522271129\n",
      "train loss:0.8436168821753912\n",
      "train loss:0.873286527865974\n",
      "train loss:0.9225432321359108\n",
      "=== epoch:8, train acc:0.994, test acc:0.991 ===\n",
      "train loss:0.8197287011267244\n",
      "train loss:1.026884035153053\n",
      "train loss:0.9106946517790447\n",
      "train loss:0.6981242357400075\n",
      "train loss:0.7982303467028757\n",
      "train loss:0.8977610804470554\n",
      "train loss:0.8431382967670156\n",
      "train loss:0.9601572680809609\n",
      "train loss:0.8169082673606289\n",
      "train loss:0.6552056236652735\n",
      "train loss:0.7721183819120507\n",
      "train loss:0.8805857650889997\n",
      "train loss:0.8551035823632336\n",
      "train loss:1.024572839367342\n",
      "train loss:0.9548525392610895\n",
      "train loss:0.9006303290211203\n",
      "train loss:0.7631622681099585\n",
      "train loss:0.7875912575236871\n",
      "train loss:0.9513359426222371\n",
      "train loss:0.7891725369955978\n",
      "train loss:0.7370835996490996\n",
      "train loss:0.8160836732397734\n",
      "train loss:0.6832078848218628\n",
      "train loss:1.001857042586282\n",
      "train loss:0.968805014302896\n",
      "train loss:0.9895226470334005\n",
      "train loss:0.6982282204737248\n",
      "train loss:0.9781041114367182\n",
      "train loss:0.8241344336337063\n",
      "train loss:0.9720000848887866\n",
      "train loss:0.9210215263957408\n",
      "train loss:0.9080124342411767\n",
      "train loss:0.8607189975253965\n",
      "train loss:0.8062929712767735\n",
      "train loss:0.7764402454587489\n",
      "train loss:0.7376274383565175\n",
      "train loss:0.8615828276960827\n",
      "train loss:0.8922522790026604\n",
      "train loss:0.9161808836643285\n",
      "train loss:0.87937055658454\n",
      "train loss:0.9211280134895354\n",
      "train loss:0.8305334804376092\n",
      "train loss:0.9731050116177525\n",
      "train loss:0.9445395064700728\n",
      "train loss:1.1225061629757302\n",
      "train loss:0.7941309407726809\n",
      "train loss:1.0322457079171796\n",
      "train loss:0.9996163953624483\n",
      "train loss:1.0060647497912598\n",
      "train loss:0.89935416810093\n",
      "train loss:0.9244566675062735\n",
      "train loss:0.9663967560869037\n",
      "train loss:0.9111414007676963\n",
      "train loss:1.01255020705088\n",
      "train loss:0.9439636055650912\n",
      "train loss:0.8418391917985604\n",
      "train loss:0.75972581941972\n",
      "train loss:0.7345208717358699\n",
      "train loss:0.777727750862309\n",
      "train loss:0.8792268167730533\n",
      "train loss:0.8590905761419136\n",
      "train loss:0.9761886259303195\n",
      "train loss:0.8544450534475588\n",
      "train loss:0.9107948983959763\n",
      "train loss:0.9982729485286237\n",
      "train loss:0.821205259737216\n",
      "train loss:0.9786025773191763\n",
      "train loss:0.893649174183725\n",
      "train loss:1.0852910911438776\n",
      "train loss:1.1536068424462396\n",
      "train loss:0.946850934089235\n",
      "train loss:1.002837068261139\n",
      "train loss:0.8084131422152067\n",
      "train loss:0.8940714197292259\n",
      "train loss:0.7219523803392879\n",
      "train loss:0.9430140596610778\n",
      "train loss:0.7796146091883068\n",
      "train loss:0.8603079156201615\n",
      "train loss:0.811070795834906\n",
      "train loss:1.0187774776829737\n",
      "train loss:0.9035052234145964\n",
      "train loss:1.0181394856867576\n",
      "train loss:0.8725695690423815\n",
      "train loss:0.9237735306203482\n",
      "train loss:0.8655325722829446\n",
      "train loss:0.8039063773800796\n",
      "train loss:0.9518497749516556\n",
      "train loss:0.8136172100382391\n",
      "train loss:0.7276822310652572\n",
      "train loss:0.986983597129816\n",
      "train loss:0.9283754857215234\n",
      "train loss:0.8255021472730282\n",
      "train loss:0.7691660601851692\n",
      "train loss:0.8982392147777868\n",
      "train loss:0.8495845150945667\n",
      "train loss:0.9177008115411378\n",
      "train loss:0.8257168614941027\n",
      "train loss:0.830167581427442\n",
      "train loss:0.8636333845135913\n",
      "train loss:0.855320640874322\n",
      "train loss:0.9655102171730274\n",
      "train loss:0.9640096805024134\n",
      "train loss:0.8918127839843214\n",
      "train loss:0.7240688280361292\n",
      "train loss:0.8043224697979835\n",
      "train loss:1.0587440720752694\n",
      "train loss:0.8733896795102548\n",
      "train loss:0.918757800488543\n",
      "train loss:0.8934114996597377\n",
      "train loss:0.7700234259413894\n",
      "train loss:0.7712464793213403\n",
      "train loss:0.9870872419613483\n",
      "train loss:0.7967388470018455\n",
      "train loss:1.0181637832389425\n",
      "train loss:0.816865635329611\n",
      "train loss:0.9485178809027522\n",
      "train loss:0.9755493998912037\n",
      "train loss:0.8710454065914979\n",
      "train loss:1.0065164625728353\n",
      "train loss:1.000248122829258\n",
      "train loss:0.8462782092040364\n",
      "train loss:0.9565059670059436\n",
      "train loss:1.0483144480174245\n",
      "train loss:0.8882964651452879\n",
      "train loss:0.9725363393382632\n",
      "train loss:0.8496721726302906\n",
      "train loss:0.9031943527961833\n",
      "train loss:0.8917736649364401\n",
      "train loss:0.9157407573588494\n",
      "train loss:0.8453841290063869\n",
      "train loss:0.9670054954451729\n",
      "train loss:0.7921301014368742\n"
     ]
    }
   ],
   "source": [
    "            # coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ec61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
